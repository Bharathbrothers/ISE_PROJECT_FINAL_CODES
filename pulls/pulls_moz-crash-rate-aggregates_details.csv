,pullid,pulls_number,pulltitle,pullsbody,pullsuserlogin,pullsuserid,pullauthordate,author_association,merged_status,stats_addns,stats_delns,stats_changed_files,pull_repo_desc,pull_repo_lang,pull_commit_sha,pull_commit_message
0,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/14,14,Add Mozilla Code of Conduct,"Fixes #13


As of January 1 2019, Mozilla requires that all GitHub projects include this [CODE_OF_CONDUCT.md](https://github.com/mozilla/repo-templates/blob/master/templates/CODE_OF_CONDUCT.md) file in the project root. The file has two parts:

1. Required Text - All text under the headings *Community Participation Guidelines and How to Report*, are required, and should not be altered.
2. Optional Text - The Project Specific Etiquette heading provides a space to speak more specifically about ways people can work effectively and inclusively together. Some examples of those can be found on the [Firefox Debugger](https://github.com/devtools-html/debugger.html/blob/master/CODE_OF_CONDUCT.md) project, and [Common Voice](https://github.com/mozilla/voice-web/blob/master/CODE_OF_CONDUCT.md). (The optional part is commented out in the [raw template file](https://raw.githubusercontent.com/mozilla/repo-templates/blob/master/templates/CODE_OF_CONDUCT.md), and will not be visible until you modify and uncomment that part.)

If you have any questions about this file, or Code of Conduct policies and procedures, please see [Mozilla-GitHub-Standards](https://wiki.mozilla.org/GitHub/Repository_Requirements) or email Mozilla-GitHub-Standards+CoC@mozilla.com.

_(Message COC002)_",Mozilla-GitHub-Standards,48073334,2019-03-29T21:54:43Z,NONE,False,15,0,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,a59a98380fc632d88e99dfce3ce4143f4c2f86d5,"Add Mozilla Code of Conduct file

Fixes #13.

_(Message COC002)_"
1,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/11,11,Bug 1270547 - Map SUBPROCESS_SHUTDOWN_KILL to content_shutdown_crashes in the stability aggregates,,bsmedberg,1914477,2016-05-06T16:21:11Z,CONTRIBUTOR,True,22,0,4,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,e6e85f1fa1c40f213ae1ee1b2b5a63778b104877,Bug 1270547 - Map SUBPROCESS_KILL_HARD to content_shutdown_crashes in the stability aggregates
2,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/11,11,Bug 1270547 - Map SUBPROCESS_SHUTDOWN_KILL to content_shutdown_crashes in the stability aggregates,,bsmedberg,1914477,2016-05-06T16:21:11Z,CONTRIBUTOR,True,22,0,4,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,261178204edd8840a7a68ea92bc4789881cdaa5b,"Add tests for the content shutdown crash stats, courtesy of Anthony"
3,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/10,10,Add tests for content shutdown crashes,"Based on #9 - should be merged after.
",Uberi,437196,2016-05-05T20:41:32Z,COLLABORATOR,False,20,0,4,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,2de3db386312818d030b046027df2b95a644851f,Bug 1270547 - Map SUBPROCESS_SHUTDOWN_KILL to content_shutdown_crashes in the stability aggregates
4,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/10,10,Add tests for content shutdown crashes,"Based on #9 - should be merged after.
",Uberi,437196,2016-05-05T20:41:32Z,COLLABORATOR,False,20,0,4,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,0f3905f3de0dda092ed7163acd460677ffeea500,Add tests for the content shutdown crash stats
5,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/9,9,Bug 1270547 - Map SUBPROCESS_SHUTDOWN_KILL to content_shutdown_crashes in the stability aggregates,"r? @vitillo or @Uberi  - This is _untested_ because I couldn't get vagrant to work correctly. I'd like somebody to test and deploy please.
",bsmedberg,1914477,2016-05-05T17:23:33Z,CONTRIBUTOR,False,18,0,3,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,f0246d5342f090821012026f102cf55799bd8b27,Bug 1270547 - Map SUBPROCESS_SHUTDOWN_KILL to content_shutdown_crashes in the stability aggregates
6,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/8,8,Misc. Updates,"- Previously, the watchdog notebook wasn't committed.
- Ping count shouldn't include crash pings, as per the Scala version.
- Clean up an unused Ansible rule.
",Uberi,437196,2016-04-27T19:46:37Z,COLLABORATOR,True,168,7,5,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,6757a43e884a787484c3437f298ac79d264ba1f8,Add note about the watchdog
7,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/8,8,Misc. Updates,"- Previously, the watchdog notebook wasn't committed.
- Ping count shouldn't include crash pings, as per the Scala version.
- Clean up an unused Ansible rule.
",Uberi,437196,2016-04-27T19:46:37Z,COLLABORATOR,True,168,7,5,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,9d0773d459807fa143782cbab874543dedeab853,"Various updates - change ping count values, add watchdog notebook"
8,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/8,8,Misc. Updates,"- Previously, the watchdog notebook wasn't committed.
- Ping count shouldn't include crash pings, as per the Scala version.
- Clean up an unused Ansible rule.
",Uberi,437196,2016-04-27T19:46:37Z,COLLABORATOR,True,168,7,5,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,46a4de4e5caea42e0f4f4dd40bb90025bd080b89,Fix tests
9,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/7,7,Update bucket and bootstrap notebook,"Currently we've been outputting results into telemetry-test-bucket. Eventually, we'll want to use telemetry-parquet instead, like the other parquet ones have been using.

Also, update the notebook so it's obvious what it does and where to find more information about it.

After this is merged, the cronjob on the Parquet2Hive instance needs to be updated as well.
",Uberi,437196,2016-04-14T15:57:10Z,COLLABORATOR,True,36,8,2,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,2825ff3c41e1122ee609bc6ef953c03bc47ddebd,Update the aggregator notebook with some documentation
10,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/7,7,Update bucket and bootstrap notebook,"Currently we've been outputting results into telemetry-test-bucket. Eventually, we'll want to use telemetry-parquet instead, like the other parquet ones have been using.

Also, update the notebook so it's obvious what it does and where to find more information about it.

After this is merged, the cronjob on the Parquet2Hive instance needs to be updated as well.
",Uberi,437196,2016-04-14T15:57:10Z,COLLABORATOR,True,36,8,2,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,e3d3c033ead3dab0892e7275ba07c9dd2bb7ac1a,Switch over to the telemetry-parquet bucket now that we're confident it won't break anything else
11,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/6,6,Add e10s cohort dimension,"As requested by :bsmedburg, the e10s cohort is necessary to do proper e10s vs non-e10s population comparisons starting with the beta46-apz experiment.
",Uberi,437196,2016-04-13T18:55:12Z,COLLABORATOR,True,8,0,3,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,ccadc243c7bfefe5dd39529da74b04aa408098ff,Add e10s cohort dimension
12,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/5,5,Add counters to the aggregator and clean up code a bit,"- Ignored and processed count for main and crash pings.
- Remove port bind in Vagrant so we don't take up a port we're not using - we don't need 5432 anymore since we switched away from postgres.
- Certain days have very small numbers of corrupted pings (<20 by my estimate). This causes errors on days like April 9. With these fixes in place, we either reject or handle these pings without stopping the job.
",Uberi,437196,2016-04-13T16:32:38Z,COLLABORATOR,True,58,19,4,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,ccadc243c7bfefe5dd39529da74b04aa408098ff,Add e10s cohort dimension
13,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/5,5,Add counters to the aggregator and clean up code a bit,"- Ignored and processed count for main and crash pings.
- Remove port bind in Vagrant so we don't take up a port we're not using - we don't need 5432 anymore since we switched away from postgres.
- Certain days have very small numbers of corrupted pings (<20 by my estimate). This causes errors on days like April 9. With these fixes in place, we either reject or handle these pings without stopping the job.
",Uberi,437196,2016-04-13T16:32:38Z,COLLABORATOR,True,58,19,4,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,68f1fd748c203f6c07d56c6687acbf14446a853f,"Add a few more checks and counters to the aggregator:

* Ignored and processed count for main and crash pings.
* Remove port bind in Vagrant so we don't take up a port we're not using - we don't need 5432 anymore since we switched away from postgres.
* Certain days have very small numbers of corrupted pings (<20 by my estimate). This causes errors on days like April 9. With these fixes in place, we either reject or handle these pings without erroring out the job."
14,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/5,5,Add counters to the aggregator and clean up code a bit,"- Ignored and processed count for main and crash pings.
- Remove port bind in Vagrant so we don't take up a port we're not using - we don't need 5432 anymore since we switched away from postgres.
- Certain days have very small numbers of corrupted pings (<20 by my estimate). This causes errors on days like April 9. With these fixes in place, we either reject or handle these pings without stopping the job.
",Uberi,437196,2016-04-13T16:32:38Z,COLLABORATOR,True,58,19,4,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,028bfef438397dc8ec00eb882970e33201686c6d,Improve success message to be per-day
15,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/4,4,Add a watchdog script that runs separately and warns if the job fails,"The watchdog script warns users if the analysis job fails.

It's a separate script because sometimes Spark errors (bugs?) can take down the entire IPython kernel with no exceptions. Also, it's less likely to get killed if there's an OOM error.

It runs on the same instance as the job runs on, is started by the bootstrap notebook, and then runs totally independently until it times out.

Alerts are sent to telemetry-alerts@mozilla.com, which people can subscribe to if they want these failure warnings.
",Uberi,437196,2016-04-08T19:44:54Z,COLLABORATOR,False,31,0,2,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,5b7596a093612d0c293f354ac24bde92afdbee92,Add a watchdog script that runs separately and ensures the job succeeds
16,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/4,4,Add a watchdog script that runs separately and warns if the job fails,"The watchdog script warns users if the analysis job fails.

It's a separate script because sometimes Spark errors (bugs?) can take down the entire IPython kernel with no exceptions. Also, it's less likely to get killed if there's an OOM error.

It runs on the same instance as the job runs on, is started by the bootstrap notebook, and then runs totally independently until it times out.

Alerts are sent to telemetry-alerts@mozilla.com, which people can subscribe to if they want these failure warnings.
",Uberi,437196,2016-04-08T19:44:54Z,COLLABORATOR,False,31,0,2,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,6ed62b13be41eccdb27d8bb5c484aaadc004c1b2,Nohup the process in case IPython is killed
17,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/4,4,Add a watchdog script that runs separately and warns if the job fails,"The watchdog script warns users if the analysis job fails.

It's a separate script because sometimes Spark errors (bugs?) can take down the entire IPython kernel with no exceptions. Also, it's less likely to get killed if there's an OOM error.

It runs on the same instance as the job runs on, is started by the bootstrap notebook, and then runs totally independently until it times out.

Alerts are sent to telemetry-alerts@mozilla.com, which people can subscribe to if they want these failure warnings.
",Uberi,437196,2016-04-08T19:44:54Z,COLLABORATOR,False,31,0,2,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,83373803e19d4715a84a5b99516e956bbc4a4449,Add missing watchdog script
18,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/3,3,Implement aggregator service to upload crash aggregates,"See https://github.com/mozilla/moz-crash-rate-aggregates/pull/2 for the full review history and a summary of changes.
",Uberi,437196,2016-04-08T15:40:33Z,COLLABORATOR,True,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,bd9a1929cbad512983d55bc165600f57e61d8768,"Implement aggregator service to upload crash aggregates as Parquet to S3.

This service is meant to be run daily, to aggregate crash pings and crash stats from main pings into an easily queryable format.

The resulting Parquet files are uploaded to S3, and a different machine runs parquet2hive to import them daily into Presto.

Data stored in Presto can then be queried via re:dash, which is available at [1].

Refer to https://github.com/mozilla/moz-crash-rate-aggregates/pull/2 for the full review history and a summary of changes.

[1]: https://sql.telemetry.mozilla.org/"
19,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,ec2c71bdf8cb823634d1977ea5fdabd7ac07edd6,Initial commit
20,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,92c31c2c6d0d70146031b2b5ad479dc8e6992874,Update aggregate values
21,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,6992c70ec994d6228ae3c899404237e427f71967,Update and more testing
22,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,e04babb39c25c87c6dbd8f9a6f9f5d877a68a6d8,Update deployment instructions and procedures
23,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,f9fcb1f66ab58450f8c52d764cb1a74b4e9f9448,Deployment fixes
24,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,25380e18c5b6ab2d884b3351ce21305a4bc9f09f,Remove unused telemetry_tools
25,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,808a172ef455db9dd5151ae4373674da9e45d29a,"Partitioning, use JSON columns, further setup improvements, documentation"
26,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,30f2727aeddf8109c243864e1e3f9c554fd34948,Add psycopg2 dep
27,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,856d1a7229d80a69f0b29f85b5c37a1f265f1d15,Better organization
28,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,f1ef6981ac9e7609f090988d694e502a35f49468,Small fixes to run_job
29,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,7bcbe8af83e44068af9e6dfddeb9fd566188ea8a,Pass the submission dates directly
30,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,adc3dc6b7d258070972ea40485ed19e914a66ee2,Some small tips and suggestions
31,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,f4457e9bb7d6d0fafc05105898987b872f84e8ed,"Better tests, Vagrant setup, progress indication"
32,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,32895dc02e18295aedb50a2b3e4635ebc5e8dd76,Clamp subsession lengths to a reasonable value
33,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,1303d0310a96e3671e11508e76cc0bd278cee17c,Fully working Vagrant setup
34,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,5391625f2555145a824ed7055589c80bef2e5bf6,Implement feedback from :bsmedburg
35,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,cfe3acaaf420a4d7e779b20faaef755a03fee068,TravisCI setup
36,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,d28f1f9e0ea3f7bf49906d114d04f788a6264a93,Test push
37,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,949555e90738cc114bc13a7acbcace8ca5bd8c02,Try to fix ansible local deployment
38,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,5d8e35ccd71b871c27da29cb171080e704a8620a,Fix ansible dev environment
39,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,abc96d08c884f7f11e2028509856a48fbf96aa40,Fix SPARK_HOME not being set after Ansible runs
40,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,de39d6c4eb984878400dec12056072096d5c6120,Remove extraneous line
41,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,f60273c45725cebb1f70fcf2981ecb35c475eee0,Manually specify spark directory
42,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,0803f38af430edb7c4e010bc8d1955877ea70b36,Add missing py4j import
43,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,3a7c349fd7de35ec7cf206a1ce3b9a569d091e82,Use the correct Python binary
44,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,035644877cae5b4e74994e515ac40c271364143d,Travis testing
45,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,a7689427e618bd3acf5a8bd568ed9f71dee08876,Fix travis again
46,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,02d16c3154db0ad6b28612ced0b5720572d36e79,More travis testing
47,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,d28d58518fa409ed95e3828d6c519bbf4744bd8c,See if Travis will cooperate
48,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/2,2,Parquet support,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-06T15:51:51Z,COLLABORATOR,False,893,0,11,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,6751daf0d19ea06c56ab34402e87097ac2e9c20a,Update configuration and implement new schema
49,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,0edb7789f75ae905e234a682de9326856a175deb,Initial commit
50,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,ffa1c73c783062ef8901786599b8715a5cb0535e,Update aggregate values
51,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,3b03ee4be4a27ebd67e131be1b38ca54733fb6b3,Update and more testing
52,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,e90ddaa8ff71e718275e3f7ab53f8425e4445c57,Update deployment instructions and procedures
53,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,930df44dbb7ae329974dd5281a89614d2803852c,Deployment fixes
54,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,8baaa66ec75d9976d5ca2c057fdae10d0b45baf2,Remove unused telemetry_tools
55,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,b60e18c277338f748dce9354772ff0f22d5e1f28,"Partitioning, use JSON columns, further setup improvements, documentation"
56,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,588080137b0a228f1a71e812d82f24fdd28343b5,Add psycopg2 dep
57,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,4816f27a93b5485c104e4852453b6a42050c32d0,Better organization
58,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,f7de5b9ed3813ae2e4f51dea17e104ecd251490c,Small fixes to run_job
59,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,1984b33e7e195a89104d8fd54cb217d1ae635667,Pass the submission dates directly
60,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,1b3d135a3c6df46e1ab0bf79a4497badfd471cb7,Some small tips and suggestions
61,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,54f08be09a2ad91e3befe6e417a3cb2379ac955c,"Better tests, Vagrant setup, progress indication"
62,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,fc224b581073d273b97af4f358d9750b000d737f,Clamp subsession lengths to a reasonable value
63,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,cef1940ff122c80aff2076ad511305c80a916667,Merge branch 'master' of https://github.com/Uberi/moz-crash-rate-aggregates
64,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,e8fe54d59eb5ce0545d0a4ee0032c01398bc862e,Fully working Vagrant setup
65,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,ca1d40078a1f8e94b4a746ea254ed6ba55c52783,Implement feedback from :bsmedburg
66,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,c38b39aadac035737ab1201bd19c28e3c280b832,TravisCI setup
67,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,b6db06de0cff0e759cf8d810e1eae875869e1b39,Test push
68,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,5003708253021157e9342cf29b42c94915d9e9c9,Try to fix ansible local deployment
69,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,ce9f194042b669236bf0f397e0d1f464348227f7,Fix ansible dev environment
70,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,0fcd0c77487ac6d3124e8291a50ac7bdf8e638f1,Fix SPARK_HOME not being set after Ansible runs
71,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,ab63e4ea64be38e52fbbc4f60d75fdfb86ff069c,Remove extraneous line
72,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,cdd894eaef3cee8ac5fc78a75730ffaed89289ac,Manually specify spark directory
73,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,a45a5320767ae3e3e065612f09d068bcd4c71b92,Add missing py4j import
74,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,b3c1b1909e610e93f408c726e2b8405f561217b3,Use the correct Python binary
75,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,b3e265d2da0173bceed0cc8807136284d465c064,Travis testing
76,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,8d7e98c589ec78b8a8b50b3caf787202bfb96da1,Fix travis again
77,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,7a3b2a74b050e578be660d0a50e760268e10f446,More travis testing
78,https://api.github.com/repos/mozilla/moz-crash-rate-aggregates/pulls/1,1,Use Parquet on S3 instead of Postgres,"- Postgres wouldn't scale for the amount of data we intend to store. Query performance is not fast enough for interactive use that we'd require.
- Yesterday, we decided to try storing crash aggregates as Parquet files on S3. The resulting performance is a lot better.
  - Less than 40 seconds for many queries, even those using `GROUP BY` and across the whole dataset.
  - This is on 100 days worth of sample data.
- The job still takes about 5 hours to run on an 8 node cluster, just as it did when using Postgres. The main advantage is much faster query time.
- There is a sample query here: https://sql.telemetry.mozilla.org/queries/125/source
",Uberi,437196,2016-04-05T18:16:42Z,COLLABORATOR,False,5,5,1,The repository is no longer used for the crash rate aggregates. The new code can be found at https://github.com/mozilla/telemetry-batch-view/blob/master/docs/CrashAggregateView.md.,Python,e17db3e8b3f1aac39d4bce5dfc23938a0dc15638,See if Travis will cooperate
