,pullid,pulls_number,pulltitle,pullsbody,pullsuserlogin,pullsuserid,pullauthordate,author_association,merged_status,stats_addns,stats_delns,stats_changed_files,pull_repo_desc,pull_repo_lang,pull_commit_sha,pull_commit_message
0,https://api.github.com/repos/mozilla/browserid_deployer/pulls/17,17,Add Mozilla Code of Conduct,"Fixes #16

_(Message COC002)_",Mozilla-GitHub-Standards,48073334,2019-03-28T04:06:26Z,NONE,False,15,0,1,A little server that deploys browserid,JavaScript,7c6b4a6002563b44f10958b6f2c29414c73e4116,"Add Mozilla Code of Conduct file

Fixes #16.

_(Message COC002)_"
1,https://api.github.com/repos/mozilla/browserid_deployer/pulls/15,15,Add --noemail for the newer awsbox create command,,chilts,3048,2013-10-07T20:36:12Z,NONE,False,1,0,1,A little server that deploys browserid,JavaScript,53fd23a1d90807e83b79426493edd4bcaa2acc6f,Add --noemail for the newer awsbox create command
2,https://api.github.com/repos/mozilla/browserid_deployer/pulls/14,14,updates for working with new awsbox & route53,"Some of this is overkill, but some is needed for route53. Review changes to to latest_sha.js with https://github.com/mozilla/browserid_deployer/compare/upgrade_awsbox_for_route53?w=1

Also see https://github.com/mozilla/awsbox/pull/99, which oddly is needed to pick up nice-route53@0.3.3
- made the deploy hostname, repo, branch and some others configurable via
  environment (so I could test changes without changing
  deployer.personatest.org).
- probably went too far on making this configurable ;-)
- initialize aws.createClients so vm.{list,destroy} work correctly
- improved some error messages to help log debugging
- changed lib/latest_sha.js to lookup NS (instead of hard-coded zerigo NS)
- upgraded native-dns to current to pick up a bug fix for running on osx
- added missing dep `temp@0.6.0`, and new dep nice-route53@0.3.3
- bumped the version to 0.0.2 fwiw
",jrgm,758162,2013-09-18T04:20:29Z,CONTRIBUTOR,False,85,43,4,A little server that deploys browserid,JavaScript,c59860bb85adea76ad147ca78396d5197d3e9dc9,"- made the deploy hostname, repo, branch and some others configurable via
  environment (so I could test changes without changing
  deployer.personatest.org).
- probably went too far on making this configurable ;-)
- initialize aws.createClients so vm.{list,destroy} work correctly
- improved some error messages to help log debugging
- changed lib/latest_sha.js to lookup NS (instead of hard-coded zerigo NS)
- upgraded native-dns to current to pick up a bug fix for running on osx
- added missing dep `temp@0.6.0`, and new dep nice-route53@0.3.3
- bumped the version to 0.0.2 fwiw"
3,https://api.github.com/repos/mozilla/browserid_deployer/pulls/12,12,Issue 11 dueling deployers,"We currently have the deployer check for updates when the github githook fires. We also check on a 15-minute backup loop, in case the githook is busted. Have a look at where the `deployer.checkForUpdates()` calls are made in [this part](https://github.com/mozilla/browserid_deployer/blob/de5b58f9c153f759838289587bc13139f9931f74/deploy_server.js#L255-L281) of `deploy_server`.

To avoid running more than one deployer at a time, we [set](https://github.com/mozilla/browserid_deployer/blob/de5b58f9c153f759838289587bc13139f9931f74/deploy_server.js#L156-L164) a boolean on the deployer object, `self._busy`.

Unfortunately, we aren't consistently using `self` after assigning `var self = this`. Because we are periodically calling `checkForUpdates` from inside a `setInterval` loop, the value of `this` is sometimes different. And so we see stuff in the logs where the bad checks are happening at 15 minute intervals, right in the middle of a new VM being deployed.

Here are some examples from today:

In this snippet, we're in the middle of deploying an awsbox (the lines with `progress:`), and then, `deployment_begins`. Note the time is 17:14.

``` bash
2013-08-06T17:14:43.012Z: progress: Loaded plugins: fastestmirror, priorities, security, update-motd, upgrade-helper
2013-08-06T17:14:43.116Z: progress: Loading mirror speeds from cached hostfile
2013-08-06T17:14:43.635Z: progress: * amzn-main: packages.us-east-1.amazonaws.com
2013-08-06T17:14:43.636Z: progress: * amzn-updates: packages.us-east-1.amazonaws.com
2013-08-06T17:14:43.636Z: progress: * epel: mirror.symnds.com
2013-08-06T17:14:44.303Z: progress: ... nope.  not yet.  retrying.
2013-08-06T17:14:48.822Z: info: checking for updates
2013-08-06T17:14:49.074Z: progress: Already up-to-date.
2013-08-06T17:14:49.097Z: info: latest available sha is e66df44
2013-08-06T17:14:49.178Z: resolved login.dev.anosrep.org -> 107.21.151.72
2013-08-06T17:14:49.182Z: info: can't get current running code: Error: connect ECONNREFUSED
2013-08-06T17:14:49.182Z: deployment_begins: {
  ""sha"": ""e66df44""
}
2013-08-06T17:14:49.401Z: progress: ... nope.  not yet.  retrying.
2013-08-06T17:14:49.646Z: progress: BO: remote: npm
2013-08-06T17:14:49.648Z: progress: BO: WARN engine connect@1.7.2: wanted: {""node"":"">= 0.4.1 < 0.7.0""} (current: {""node"":""v0.8
.17"",""npm"":""1.2.0""})ESC[K
```

A bit earlier in history, exactly 15 minutes earlier (16:59), same deal. Loglines with 'progress' are the awsbox deploy, loglines with 'info' are the setInterval loop:

``` bash
2013-08-06T16:59:48.691Z: progress: BO: remote: npm
2013-08-06T16:59:48.691Z: progress: BO: http
2013-08-06T16:59:48.692Z: progress: BO: 200 http://127.0.0.1:36199/iconv-lite/-/iconv-lite-0.2.7.tgzESC[K
2013-08-06T16:59:48.820Z: info: checking for updates
2013-08-06T16:59:48.864Z: progress: BO: remote: make: Entering directory `/tmp/deploy11372-5720-bpboqw/node_modules/jwcrypto/n
ode_modules/bigint/build'ESC[K
2013-08-06T16:59:48.864Z: progress: BO: remote:   CXX(target) Release/obj.target/bigint/bigint.oESC[K
2013-08-06T16:59:48.933Z: progress: ... nope.  not yet.  retrying.
2013-08-06T16:59:48.976Z: progress: BO: remote: npm
<snipped>
2013-08-06T16:59:49.131Z: progress: BO: http
2013-08-06T16:59:49.131Z: progress: BO: GET
2013-08-06T16:59:49.132Z: progress: BO: http://127.0.0.1:15797/colors/-/colors-0.3.0.tgzESC[K
2013-08-06T16:59:49.247Z: progress: Already up-to-date.
2013-08-06T16:59:49.276Z: info: latest available sha is e66df44
2013-08-06T16:59:49.352Z: resolved login.dev.anosrep.org -> 54.211.32.231
2013-08-06T16:59:49.358Z: info: current running sha is: c0bf068
2013-08-06T16:59:49.358Z: deployment_begins: {
  ""sha"": ""e66df44""
}
2013-08-06T16:59:49.733Z: progress: BO: remote: path.existsSync is now called `fs.existsSync`.ESC[K
2013-08-06T16:59:49.745Z: progress: BO: remote: npm
```

Same deal, exactly 15 minutes earlier (16:44)--the only difference in this snippet is that DNS lookup failed in the setInterval loop. I'm not sure the reason, but it shouldn't have kicked off in the first place.

``` bash
2013-08-06T16:44:53.104Z: progress: BO: http://127.0.0.1:32301/minimatch/-/minimatch-0.0.5.tgzESC[K
2013-08-06T16:44:53.105Z: progress: Verifying  : mysql55-libs-5.5.31-1.32.amzn1.x86_64                        2/6
2013-08-06T16:44:53.166Z: progress: Verifying  : mysql-server-5.5-1.3.amzn1.noarch                            3/6
2013-08-06T16:44:53.166Z: info: can't get current running code: Timeout resolving login.dev.anosrep.org
2013-08-06T16:44:53.166Z: deployment_begins: {
  ""sha"": ""e66df44""
}
2013-08-06T16:44:53.198Z: resolved login.dev.anosrep.org -> null
2013-08-06T16:44:53.200Z: info: can't get current running code: Error: connect ECONNREFUSED
2013-08-06T16:44:53.200Z: deployment_begins: {
  ""sha"": ""e66df44""
}
2013-08-06T16:44:53.259Z: progress: Verifying  : mysql55-server-5.5.31-1.32.amzn1.x86_64                      4/6
2013-08-06T16:44:53.331Z: progress: BO: remote:
2013-08-06T16:44:53.331Z: progress: BO: remote: > browserid@1.0.0-b2 preinstall /tmp/deploy11372-5713-17kg5qiESC[K
2013-08-06T16:44:53.331Z: progress: BO: remote: > node ./scripts/lockdownESC[K
```

I suppose it's possible there's some other root cause, but this seems like a pretty solid explanation to me.
",jaredhirsch,96396,2013-08-06T22:41:35Z,MEMBER,True,8,2,1,A little server that deploys browserid,JavaScript,9967e1e801e4060374ed140e6fd97fd9275333fe,"Add detailed logging each time we checkForUpdates, to isolate race conditions."
4,https://api.github.com/repos/mozilla/browserid_deployer/pulls/12,12,Issue 11 dueling deployers,"We currently have the deployer check for updates when the github githook fires. We also check on a 15-minute backup loop, in case the githook is busted. Have a look at where the `deployer.checkForUpdates()` calls are made in [this part](https://github.com/mozilla/browserid_deployer/blob/de5b58f9c153f759838289587bc13139f9931f74/deploy_server.js#L255-L281) of `deploy_server`.

To avoid running more than one deployer at a time, we [set](https://github.com/mozilla/browserid_deployer/blob/de5b58f9c153f759838289587bc13139f9931f74/deploy_server.js#L156-L164) a boolean on the deployer object, `self._busy`.

Unfortunately, we aren't consistently using `self` after assigning `var self = this`. Because we are periodically calling `checkForUpdates` from inside a `setInterval` loop, the value of `this` is sometimes different. And so we see stuff in the logs where the bad checks are happening at 15 minute intervals, right in the middle of a new VM being deployed.

Here are some examples from today:

In this snippet, we're in the middle of deploying an awsbox (the lines with `progress:`), and then, `deployment_begins`. Note the time is 17:14.

``` bash
2013-08-06T17:14:43.012Z: progress: Loaded plugins: fastestmirror, priorities, security, update-motd, upgrade-helper
2013-08-06T17:14:43.116Z: progress: Loading mirror speeds from cached hostfile
2013-08-06T17:14:43.635Z: progress: * amzn-main: packages.us-east-1.amazonaws.com
2013-08-06T17:14:43.636Z: progress: * amzn-updates: packages.us-east-1.amazonaws.com
2013-08-06T17:14:43.636Z: progress: * epel: mirror.symnds.com
2013-08-06T17:14:44.303Z: progress: ... nope.  not yet.  retrying.
2013-08-06T17:14:48.822Z: info: checking for updates
2013-08-06T17:14:49.074Z: progress: Already up-to-date.
2013-08-06T17:14:49.097Z: info: latest available sha is e66df44
2013-08-06T17:14:49.178Z: resolved login.dev.anosrep.org -> 107.21.151.72
2013-08-06T17:14:49.182Z: info: can't get current running code: Error: connect ECONNREFUSED
2013-08-06T17:14:49.182Z: deployment_begins: {
  ""sha"": ""e66df44""
}
2013-08-06T17:14:49.401Z: progress: ... nope.  not yet.  retrying.
2013-08-06T17:14:49.646Z: progress: BO: remote: npm
2013-08-06T17:14:49.648Z: progress: BO: WARN engine connect@1.7.2: wanted: {""node"":"">= 0.4.1 < 0.7.0""} (current: {""node"":""v0.8
.17"",""npm"":""1.2.0""})ESC[K
```

A bit earlier in history, exactly 15 minutes earlier (16:59), same deal. Loglines with 'progress' are the awsbox deploy, loglines with 'info' are the setInterval loop:

``` bash
2013-08-06T16:59:48.691Z: progress: BO: remote: npm
2013-08-06T16:59:48.691Z: progress: BO: http
2013-08-06T16:59:48.692Z: progress: BO: 200 http://127.0.0.1:36199/iconv-lite/-/iconv-lite-0.2.7.tgzESC[K
2013-08-06T16:59:48.820Z: info: checking for updates
2013-08-06T16:59:48.864Z: progress: BO: remote: make: Entering directory `/tmp/deploy11372-5720-bpboqw/node_modules/jwcrypto/n
ode_modules/bigint/build'ESC[K
2013-08-06T16:59:48.864Z: progress: BO: remote:   CXX(target) Release/obj.target/bigint/bigint.oESC[K
2013-08-06T16:59:48.933Z: progress: ... nope.  not yet.  retrying.
2013-08-06T16:59:48.976Z: progress: BO: remote: npm
<snipped>
2013-08-06T16:59:49.131Z: progress: BO: http
2013-08-06T16:59:49.131Z: progress: BO: GET
2013-08-06T16:59:49.132Z: progress: BO: http://127.0.0.1:15797/colors/-/colors-0.3.0.tgzESC[K
2013-08-06T16:59:49.247Z: progress: Already up-to-date.
2013-08-06T16:59:49.276Z: info: latest available sha is e66df44
2013-08-06T16:59:49.352Z: resolved login.dev.anosrep.org -> 54.211.32.231
2013-08-06T16:59:49.358Z: info: current running sha is: c0bf068
2013-08-06T16:59:49.358Z: deployment_begins: {
  ""sha"": ""e66df44""
}
2013-08-06T16:59:49.733Z: progress: BO: remote: path.existsSync is now called `fs.existsSync`.ESC[K
2013-08-06T16:59:49.745Z: progress: BO: remote: npm
```

Same deal, exactly 15 minutes earlier (16:44)--the only difference in this snippet is that DNS lookup failed in the setInterval loop. I'm not sure the reason, but it shouldn't have kicked off in the first place.

``` bash
2013-08-06T16:44:53.104Z: progress: BO: http://127.0.0.1:32301/minimatch/-/minimatch-0.0.5.tgzESC[K
2013-08-06T16:44:53.105Z: progress: Verifying  : mysql55-libs-5.5.31-1.32.amzn1.x86_64                        2/6
2013-08-06T16:44:53.166Z: progress: Verifying  : mysql-server-5.5-1.3.amzn1.noarch                            3/6
2013-08-06T16:44:53.166Z: info: can't get current running code: Timeout resolving login.dev.anosrep.org
2013-08-06T16:44:53.166Z: deployment_begins: {
  ""sha"": ""e66df44""
}
2013-08-06T16:44:53.198Z: resolved login.dev.anosrep.org -> null
2013-08-06T16:44:53.200Z: info: can't get current running code: Error: connect ECONNREFUSED
2013-08-06T16:44:53.200Z: deployment_begins: {
  ""sha"": ""e66df44""
}
2013-08-06T16:44:53.259Z: progress: Verifying  : mysql55-server-5.5.31-1.32.amzn1.x86_64                      4/6
2013-08-06T16:44:53.331Z: progress: BO: remote:
2013-08-06T16:44:53.331Z: progress: BO: remote: > browserid@1.0.0-b2 preinstall /tmp/deploy11372-5713-17kg5qiESC[K
2013-08-06T16:44:53.331Z: progress: BO: remote: > node ./scripts/lockdownESC[K
```

I suppose it's possible there's some other root cause, but this seems like a pretty solid explanation to me.
",jaredhirsch,96396,2013-08-06T22:41:35Z,MEMBER,True,8,2,1,A little server that deploys browserid,JavaScript,abe9fe3dcb46ee5ec1acc02e55842c58a872edd8,"Don't use both 'self' and 'this'. Fixes #11.

    I think the cause of #11 is that checkForUpdates is running inside a
    setInterval loop, where this._busy isn't getting set to the same thing
    as self._busy, causing parallel threads of execution.

    If we see more 'dueling deployers', this theory will be proven wrong :-P"
5,https://api.github.com/repos/mozilla/browserid_deployer/pulls/6,6,Prepend ISO date timestamp to deployer events. Fixes #2.,"meh. verbose, dumb, hopefully uncontroversial. I've already put more time into this PR than I ever wanted to put into deployer maintenance. @seanmonstar thoughts?
",jaredhirsch,96396,2013-07-02T00:18:04Z,MEMBER,True,8,8,2,A little server that deploys browserid,JavaScript,de5b58f9c153f759838289587bc13139f9931f74,Prepend ISO date timestamp to deployer events. Fixes #2.
6,https://api.github.com/repos/mozilla/browserid_deployer/pulls/5,5,Monkeypatch console.log to force timestamps into all log calls. Fixes #2 again.,"@seanmonstar thoughts? Is this dirty/lazy or brilliant? Does what it should, logs onboard deployer go from

``` bash
deploy server starting up
code dir is: /home/app/browserid_repo
deployment log dir is: /home/app/deploy_logs
ready
info: checking for updates
deploy server bound
progress: Already up-to-date.
info: latest available sha is f5218dc
resolved login.dev.anosrep.org -> 54.242.215.50
info: current running sha is: f5218dc
info: up to date
warn: Forever detected script exited with code: null
```

to

``` bash
2013-07-01T22:43:50.896Z: deploy server starting up
2013-07-01T22:43:50.898Z: code dir is:
2013-07-01T22:43:50.907Z: deployment log dir is:
2013-07-01T22:43:50.917Z: ready
2013-07-01T22:43:50.918Z: info: checking for updates
2013-07-01T22:43:50.934Z: deploy server bound
2013-07-01T22:43:51.182Z: progress: Already up-to-date.
2013-07-01T22:43:51.195Z: info: latest available sha is f5218dc
2013-07-01T22:43:51.276Z: resolved login.dev.anosrep.org -> 54.242.215.50
2013-07-01T22:43:51.284Z: info: current running sha is: f5218dc
2013-07-01T22:43:51.284Z: info: up to date
```
",jaredhirsch,96396,2013-07-01T22:47:52Z,MEMBER,False,4,0,1,A little server that deploys browserid,JavaScript,383383abbdc280da66c716d10f9f44397aaf0ec2,Monkeypatch console.log to force timestamps into all log calls. Fixes #2.
7,https://api.github.com/repos/mozilla/browserid_deployer/pulls/4,4,Prepend ISO date to log lines. Fixes #2.,"@rfk mind having a look? This pull request prepends the ISO date to loglines, so that deployer's logs are less insane to try to read in a pinch.

I couldn't quite figure out how to test this locally, but the code looks ok to me. Thoughts?

After the Zerigo outage ends, I can push the changes onboard deployer and verify nothing's borked.
",jaredhirsch,96396,2013-06-29T01:18:07Z,MEMBER,True,3,3,2,A little server that deploys browserid,JavaScript,039d2a1a713d5bcfd1548d45078e0053cd6c4472,Prepend ISO date to log lines. Fixes #2.
