,pullid,pulls_number,pulltitle,pullsbody,pullsuserlogin,pullsuserid,pullauthordate,author_association,merged_status,stats_addns,stats_delns,stats_changed_files,pull_repo_desc,pull_repo_lang,pull_commit_sha,pull_commit_message
0,https://api.github.com/repos/mozilla/chronicle/pulls/368,368,bug(db): specify TEMPLATE when creating db.,"  By specifying the template, we avoid collation errors when running on clients
  that do not have the en_US locale.

  Fixes #367.
",jaredhirsch,96396,2015-04-16T16:44:34Z,MEMBER,True,1,1,1,find everything you've ever found,JavaScript,616bd5f27e8cae169657c61fbdfd2607bcb73be7,"bug(db): specify TEMPLATE when creating db.

  By specifying the template, we avoid collation errors when running on clients
  that do not have the en_US locale.

  Fixes #367."
1,https://api.github.com/repos/mozilla/chronicle/pulls/365,365,refactor(server): replace node-resque with kue,"Fixes #363.
",jaredhirsch,96396,2015-02-28T17:05:02Z,MEMBER,True,888,3550,6,find everything you've ever found,JavaScript,54f681c0dac5cbc7d0315ecd013003877cb0b967,"refactor(server): replace node-resque with kue

Fixes #363."
2,https://api.github.com/repos/mozilla/chronicle/pulls/359,359,WIP - Issue 330 store all embedly data,"@pdehaan mind taking a look? This is not ready to land; I need to update the controller/view layer to fix the API, but there are so many fiddly little changes here, and I'd really appreciate another set of eyes on this.

Previously, I was making things unnecessarily complex at the DB layer:

We were only storing a subset of the embedly response, doing unnecessary transforms on that response. Now, we store the whole thing using the postgres native json type. Queries get so much simpler, and embedly.js gets so much simpler.

All the keys in each record were underscored in postgres, but camelCased in elasticsearch. Also, we were structuring the data slightly differently in both, by nesting certain fields one level deep in elasticsearch, but not in postgres. Now, we keep the same structure in elasticsearch and postgres, where the embedly data is under an extracted_data key, and everything else is top level. This makes the pg -> es mapping super super simple, which is good, since we do it in several places (visit.create, user-page.update). There is one added bit of complexity: where the postgres db object used to always camelize its results, now we only sometimes camelize them. I think this can be fixed, but not right now. Another added bit of complexity is that postgres.camelize now has to separately camelize the contents of the embedly response; I opted for dumb nested `Object.keys().forEach` calls, rather than fancy recursion. I think it's more readable; we'll reassess if we go another level deeper.

Other random notes:

Once AWS adds 9.4 support, we can consider moving to the jsonb type. This wouldn't bring us any significant improvements right now anyway, since we don't do any querying on the contents of the embedly response, but jsonb auto-indexes all the keys, and converts json to an internal binary representation on insert - so it would presumably make it easy and fast to include those keys in queries. Seems like it would give us really cool flexibility with our data.

I haven't added anything into a user-page record to indicate which scraper scraped the page, because we only have one scraper right now. Keeping it simple, minimizing the amount of code.

I've added the error check inside embedly.js, so we can avoid storing responses with the error type (#257).
",jaredhirsch,96396,2015-02-24T20:07:48Z,MEMBER,True,240,233,18,find everything you've ever found,JavaScript,0a64f405c24246d7b064934a18262d437aa0ce61,"feat(server): store embedly response as json

Fixes #330.

Finally add crude views to go along with primitive controllers and models.
The views temporarily include a transform to match the structure expected
by the front-end code. For example, the visits front-end expects
  visit.userPage.extractedDescription,
while the new code by default returns
  visit.userPage.extracted_data.description.

Don't bother attempting to migrate data until we're in production.
We know the migration library works, let's just go fast while we can.

Simplify the pg-es relationship by sending the exact postgres record
to elasticsearch. Previously, the es records had camelCased keys, and
they had different keys at top-level vs postgres.
 - This means that postgres.query should not always camelize underscored keys,
   since we want underscored keys, not camelized keys, in elasticsearch. Add
   an optional flag to disable camelizing in such cases.
 - We also wound up extracting a camelize util, since we now need elasticsearch
   to camelize its keys when returning search results.

Among other embedly.js changes, add a `type == 'error'` check that fixes #257."
3,https://api.github.com/repos/mozilla/chronicle/pulls/358,358,docs(signoff): create Chronicle Prerelease Deployment Checklist,"Converted from a wiki page to a markdown file in /docs/.
",pdehaan,557895,2015-02-23T22:24:13Z,CONTRIBUTOR,True,41,0,1,find everything you've ever found,JavaScript,6289cf9a1da4649cd2c81fa4229d6ce572693aa2,docs(signoff): create Chronicle Prerelease Deployment Checklist
4,https://api.github.com/repos/mozilla/chronicle/pulls/357,357,feat(server): restrict access to whitelisted users,"Currently returning a 401 with a nice error string, we could do more.

Fixes #128.
",jaredhirsch,96396,2015-02-23T17:17:06Z,MEMBER,True,34,2,4,find everything you've ever found,JavaScript,0390024f435e4754242270c9af2766074059e13d,"feat(server): restrict access to whitelisted users

Currently returning a 401 with a nice error string, we could do more.

Fixes #128."
5,https://api.github.com/repos/mozilla/chronicle/pulls/355,355,feat(docs): add a /docs route using Lout,"Adds a http://localhost:8080/docs route which shows some auto-generated route documentation using [**Lout**](https://www.npmjs.com/package/lout).

It ain't pretty, but _meh_.

Closes #108 
",pdehaan,557895,2015-02-19T01:38:43Z,CONTRIBUTOR,True,160,55,12,find everything you've ever found,JavaScript,3a28f0bc70ddee2f3518955a2ec87cbb47b36d2e,feat(docs): add a /docs route using Lout
6,https://api.github.com/repos/mozilla/chronicle/pulls/354,354,chore(build): make linting quicker when committing code,"Speeds up the build process by using **grunt-newer** to only lint _modified_ files during the **pre-push** git hook (via `$ grunt quicklint`).

Fixes #322 
",pdehaan,557895,2015-02-19T00:40:58Z,CONTRIBUTOR,True,59,36,3,find everything you've ever found,JavaScript,491bebb56edc0036e5da3583624c9152526c1e2e,chore(build): make linting quicker when committing code
7,https://api.github.com/repos/mozilla/chronicle/pulls/349,349,fix(visit): Change date heading to use day of month with ordinal,"Per http://momentjs.com/docs/#/parsing/string-format/

| Input | Example | Description |
| --: | --- | --- |
| `Do` | `1st..31st` | Day of month with ordinal |

Fixes #348 
",pdehaan,557895,2015-02-17T18:12:50Z,CONTRIBUTOR,True,1,1,1,find everything you've ever found,JavaScript,49ada139533acf0a1e743de8cf2cd7e668efb824,fix(visit): Change date heading to use day of month with ordinal
8,https://api.github.com/repos/mozilla/chronicle/pulls/346,346,refactor(db): Change the test data time interval so we see more date headings,"Each successive test data visit should be +5 hours later than the previous one, giving us 5-6 entries per date heading.

FWIW, 5 [hours] is a magically random number that I made up with no significance, apart from using 2, 3, 4, 6, or 8 were all divisible by 24, giving us the exact same numbers of entries per day. Or we could increase this to an equally arbitrary 7 or 9 hours difference, if that floats your collective boats.

Fixes #345
",pdehaan,557895,2015-02-17T01:21:25Z,CONTRIBUTOR,True,4,2,1,find everything you've ever found,JavaScript,820b30d9a9787cd33d13ce93df867911c4e6f10c,refactor(db): Change the test data time interval so we see more date headings
9,https://api.github.com/repos/mozilla/chronicle/pulls/344,344,chore(sass): Add a convict flag to specify Sass output style,"Added a useless knob to control your Sass `outputStyle`. Possibly only useful to me and @johngruen (or anybody else dabbling in CSS who doesn't want to read minimized code).

The output is still compressed/minimized by default, but you can easily override locally by setting the `""sass_outputStyle"": ""nested""` in your config/local.json file.

Fixes #301 
",pdehaan,557895,2015-02-17T00:20:23Z,CONTRIBUTOR,True,10,2,3,find everything you've ever found,JavaScript,b39d1748639e06e8003abf115cc3c432969bfbba,chore(sass): Add a convict flag to specify Sass output style
10,https://api.github.com/repos/mozilla/chronicle/pulls/343,343,docs(readme): update README to use current bin scripts,"Updated README.md to mention the new location of the database scripts in ./bin/.
Also noticed that we weren't linting any of the JavaScript scripts in /bin/ since there weren't any previously.

Fixes #316 
",pdehaan,557895,2015-02-16T23:19:02Z,CONTRIBUTOR,True,25,23,6,find everything you've ever found,JavaScript,54b4c23f7286aa919df01255021a336f72f966e9,docs(readme): update README to use current bin scripts
11,https://api.github.com/repos/mozilla/chronicle/pulls/342,342,chore(deps): Update grunt-sass dependency,"Updating [**grunt-sass**](https://github.com/sindresorhus/grunt-sass/) ([CHANGELOG](https://github.com/sindresorhus/grunt-sass/compare/v0.17.0...v0.18.0)) and [**wreck**](https://github.com/hapijs/wreck/) ([CHANGELOG](https://github.com/hapijs/wreck/compare/v5.1.0...v5.2.0)) dependencies.

Chronicle should now run on iojs and Node 0.12! (tested w/ iojs-v1.2.0 and v0.12.0)!

Fixes #321
",pdehaan,557895,2015-02-16T22:40:52Z,CONTRIBUTOR,True,234,112,2,find everything you've ever found,JavaScript,770680534f221df6678a28b72bf945a9a14141ac,chore(deps): Update grunt-sass and wreck dependencies
12,https://api.github.com/repos/mozilla/chronicle/pulls/342,342,chore(deps): Update grunt-sass dependency,"Updating [**grunt-sass**](https://github.com/sindresorhus/grunt-sass/) ([CHANGELOG](https://github.com/sindresorhus/grunt-sass/compare/v0.17.0...v0.18.0)) and [**wreck**](https://github.com/hapijs/wreck/) ([CHANGELOG](https://github.com/hapijs/wreck/compare/v5.1.0...v5.2.0)) dependencies.

Chronicle should now run on iojs and Node 0.12! (tested w/ iojs-v1.2.0 and v0.12.0)!

Fixes #321
",pdehaan,557895,2015-02-16T22:40:52Z,CONTRIBUTOR,True,234,112,2,find everything you've ever found,JavaScript,d362cf581acca313b67f5146f7630a7ab35dadb4,chore(deps): removing unused wreck dependency
13,https://api.github.com/repos/mozilla/chronicle/pulls/341,341, style(dates): make date markers pretty,"This should get merged after #340 
",johngruen,3323249,2015-02-14T18:31:50Z,CONTRIBUTOR,True,60,12,10,find everything you've ever found,JavaScript,3a195d6b102c42a2540ca677624d413b1f83f26d,fix(style): single row items are too tall
14,https://api.github.com/repos/mozilla/chronicle/pulls/341,341, style(dates): make date markers pretty,"This should get merged after #340 
",johngruen,3323249,2015-02-14T18:31:50Z,CONTRIBUTOR,True,60,12,10,find everything you've ever found,JavaScript,7ae05f95538efe8f7799c6c92ae099ed08583106,style(dates): make date markers pretty
15,https://api.github.com/repos/mozilla/chronicle/pulls/340,340,fix(style): single row items are too tall,"Fixes #297 
",johngruen,3323249,2015-02-14T16:36:47Z,CONTRIBUTOR,True,23,11,5,find everything you've ever found,JavaScript,3a195d6b102c42a2540ca677624d413b1f83f26d,fix(style): single row items are too tall
16,https://api.github.com/repos/mozilla/chronicle/pulls/339,339,chore(build): replace precommit-hook with husky for auto linting,"Replacing **precommit-hook** module with [**husky**](https://www.npmjs.com/package/husky) so we can remove some sketchy automated JSHint logic included in **precommit-hook** and convert the hook from ""pre-commit"" to ""pre-push"". See https://github.com/mozilla/chronicle/issues/322 for some more context.

This doesn't address the core issue of #322 (_""git pre-commit linter should only run on files in the git index""_), but should give a slight performance bump (if you like to `git commit` a lot and `git push` a little).
### How do I even?

To install/test this, I recommend cleaning out any existing git hooks in your ./.git/hooks/ directory that may have been set up by **precommit-hook** _before_ running `npm install`. The **husky** npm install will recreate any hooks that it watches (notably ""precommit"", ""prepush"", and ""postmerge"").
",pdehaan,557895,2015-02-14T01:20:44Z,CONTRIBUTOR,True,25,183,2,find everything you've ever found,JavaScript,ba5c91272fa6f72d382b177f6664a48e6446cd5e,chore(build): replace precommit-hook with husky for auto linting
17,https://api.github.com/repos/mozilla/chronicle/pulls/338,338,delete redis dump.rdb file,,pdehaan,557895,2015-02-14T00:13:05Z,CONTRIBUTOR,True,0,0,1,find everything you've ever found,JavaScript,542a72b9761716db5c7b64facf058b06c2e068b8,delete redis dump.rdb file
18,https://api.github.com/repos/mozilla/chronicle/pulls/337,337,"bug(search): include title, extractedLead, extractedUrl, and extractedProviderUrl when searching","Closes #334 
",pdehaan,557895,2015-02-13T19:03:56Z,CONTRIBUTOR,True,3,0,1,find everything you've ever found,JavaScript,96698d9c16d3637dc865edea4d3a5108bbfca196,"bug(search): include title, extractedLead, extractedUrl, and extractedProviderUrl when searching"
19,https://api.github.com/repos/mozilla/chronicle/pulls/337,337,"bug(search): include title, extractedLead, extractedUrl, and extractedProviderUrl when searching","Closes #334 
",pdehaan,557895,2015-02-13T19:03:56Z,CONTRIBUTOR,True,3,0,1,find everything you've ever found,JavaScript,908e9dbb6717b3612c0f884a7519db2d1857204d,bug(search): removing extractedProviderUrl from search fields
20,https://api.github.com/repos/mozilla/chronicle/pulls/332,332,feat(front-end): Adds welcome with add-on installation,"This adds the authenticated portion of the onboarding flow at `/#welcome`. If you're a new user to Chronicle, you'll be directed here after your account is created. This is purposely unstyled and therefore ready for @johngruen to turn it into something much better. It doesn't do a whole lot for now, but will at least help you get the add-on installed in Firefox.

Fixes #247.

@6a68 @johngruen r?
",nchapman,3095,2015-02-13T05:11:47Z,CONTRIBUTOR,True,309,27,17,find everything you've ever found,JavaScript,2dcc24aa40941b2fc445d438a290aad214df034a,feat(front-end): Adds welcome with add-on installation
21,https://api.github.com/repos/mozilla/chronicle/pulls/329,329,bug(search): include description when searching,"Fixes #324 

@nchapman mind taking a look? Seems to be working for me:

![screenshot 2015-02-12 14 27 32](https://cloud.githubusercontent.com/assets/96396/6178503/661f02a2-b2c4-11e4-9b9a-1fc1fc613323.png)
",jaredhirsch,96396,2015-02-12T22:36:32Z,MEMBER,True,1,0,1,find everything you've ever found,JavaScript,360bffad171375016fbdee976de859490d75437d,"bug(search): include description when searching

Fixes #324"
22,https://api.github.com/repos/mozilla/chronicle/pulls/328,328,chore(deps): update dependencies and devDependencies,"Updated all dependencies and devDependencies. Everything seems to still run and I can signout/signin, search, browse, etc.

``` sh
$ npm run outdated

> mozilla-chronicle@0.1.0 outdated /Users/pdehaan/dev/github/chronicle
> npm outdated --depth 0

$ david
All dependencies up to date
```

And now down to only 1 warning from `npm run validate-shrinkwrap` task (which is out of our control):

``` sh
$ npm run validate-shrinkwrap

Running ""validate-shrinkwrap"" task
/Users/pdehaan/dev/github/chronicle/npm-shrinkwrap.json
>> Name  Installed  Patched  Vulnerable Dependency
>> qs      0.5.6     >= 1.x  mozilla-chronicle > grunt-contrib-watch > tiny-lr-fork
>> qs      0.5.6     >= 1.x  mozilla-chronicle > grunt-contrib-watch > tiny-lr-fork
Warning: known vulnerable modules found Used --force, continuing.

Done, but with warnings.
```
",pdehaan,557895,2015-02-12T20:06:34Z,CONTRIBUTOR,True,3491,449,2,find everything you've ever found,JavaScript,e1a232483b34bc3296f8c51a095685eafb319269,chore(deps): update dependencies and devDependencies
23,https://api.github.com/repos/mozilla/chronicle/pulls/325,325,feat(server): adding a /ver.json route,"Fixes #214 
",pdehaan,557895,2015-02-11T20:50:18Z,CONTRIBUTOR,True,118,0,2,find everything you've ever found,JavaScript,e5405d92b41ecfc55b8cd2dafbed1c63712eeeb5,feat(server): adding a /ver.json route
24,https://api.github.com/repos/mozilla/chronicle/pulls/323,323,Issue 291 reindex on migrate,"@nchapman @pdehaan r?

Since we're only using one worker for now, I didn't bother integrating this into the work queue. However, I did make sure all the job variables were passed into the ""worker"" as the params object. Should be pretty straightforward to move this into the queue later.

Also made a couple of minor adjustments to the DB, should be self-explanatory, except the rgb -> hex conversion was added because I saw elasticsearch complaining, and might have some impact on the front-end code (if we're using the favicon colors). Note that they're stored without the leading '#'.
",jaredhirsch,96396,2015-02-11T00:31:19Z,MEMBER,True,227,19,9,find everything you've ever found,JavaScript,0dd7af3f93771dd1c382335d24bd6269f83506ba,refactor(db): include table name at start of all indexes
25,https://api.github.com/repos/mozilla/chronicle/pulls/323,323,Issue 291 reindex on migrate,"@nchapman @pdehaan r?

Since we're only using one worker for now, I didn't bother integrating this into the work queue. However, I did make sure all the job variables were passed into the ""worker"" as the params object. Should be pretty straightforward to move this into the queue later.

Also made a couple of minor adjustments to the DB, should be self-explanatory, except the rgb -> hex conversion was added because I saw elasticsearch complaining, and might have some impact on the front-end code (if we're using the favicon colors). Note that they're stored without the leading '#'.
",jaredhirsch,96396,2015-02-11T00:31:19Z,MEMBER,True,227,19,9,find everything you've ever found,JavaScript,890c35bf21eeb06321bac11769cb8a9c436130ba,feat(db): add creation timestamp index to user_pages table to support elasticsearch reindexing
26,https://api.github.com/repos/mozilla/chronicle/pulls/323,323,Issue 291 reindex on migrate,"@nchapman @pdehaan r?

Since we're only using one worker for now, I didn't bother integrating this into the work queue. However, I did make sure all the job variables were passed into the ""worker"" as the params object. Should be pretty straightforward to move this into the queue later.

Also made a couple of minor adjustments to the DB, should be self-explanatory, except the rgb -> hex conversion was added because I saw elasticsearch complaining, and might have some impact on the front-end code (if we're using the favicon colors). Note that they're stored without the leading '#'.
",jaredhirsch,96396,2015-02-11T00:31:19Z,MEMBER,True,227,19,9,find everything you've ever found,JavaScript,4454d90c7b6ac69854fb3fb1ece116f7f6f8840f,bug(scraper): convert rgb arrays to hex before storing
27,https://api.github.com/repos/mozilla/chronicle/pulls/323,323,Issue 291 reindex on migrate,"@nchapman @pdehaan r?

Since we're only using one worker for now, I didn't bother integrating this into the work queue. However, I did make sure all the job variables were passed into the ""worker"" as the params object. Should be pretty straightforward to move this into the queue later.

Also made a couple of minor adjustments to the DB, should be self-explanatory, except the rgb -> hex conversion was added because I saw elasticsearch complaining, and might have some impact on the front-end code (if we're using the favicon colors). Note that they're stored without the leading '#'.
",jaredhirsch,96396,2015-02-11T00:31:19Z,MEMBER,True,227,19,9,find everything you've ever found,JavaScript,fcf4d20077e62f26794f4b1ad1fcceb46a8c2d12,"bug(db): reindex elasticsearch as part of migration process

Fixes #291."
28,https://api.github.com/repos/mozilla/chronicle/pulls/317,317,feature(welcome): add first time experience to chronicle,"@nchapman 
",johngruen,3323249,2015-02-10T18:32:00Z,CONTRIBUTOR,True,826,23,24,find everything you've ever found,JavaScript,44bb0776c225f9f6ca49d4b918f3d44af227dbc4,feature(welcome): add first time experience to chronicle
29,https://api.github.com/repos/mozilla/chronicle/pulls/315,315,bug(server): change default session to one month,"Fixes #248

@nchapman @pdehaan r?
",jaredhirsch,96396,2015-02-10T16:20:55Z,MEMBER,True,9,3,3,find everything you've ever found,JavaScript,ab3759f5fa22b38a9fa315773db5c6b859dad20a,"bug(server): enable long-lasting sessions

Set a default cookie ttl of a month, use keepalives to refresh that ttl.

Fixes #248"
30,https://api.github.com/repos/mozilla/chronicle/pulls/314,314,chore(front-end): Enables js on the home page,"This enables js on the home page. I'm not sure we'll keep this boot sequence for long, but it should work for now until our needs are more complicated.

Fixes #310.

@6a68 r?
",nchapman,3095,2015-02-10T04:34:05Z,CONTRIBUTOR,True,44,7,5,find everything you've ever found,JavaScript,66d0ea099e6a88d00204e054028a7c1bf6a38f58,chore(front-end): Enables js on the home page.
31,https://api.github.com/repos/mozilla/chronicle/pulls/313,313,fix(styles): fixing invalid width in scss,"Fixes #311 
",pdehaan,557895,2015-02-09T22:34:29Z,CONTRIBUTOR,True,11,1,2,find everything you've ever found,JavaScript,f3d2654372bf386a2d6d6d5a546bb5ae41eaafc9,fix(styles): fixing invalid width in scss
32,https://api.github.com/repos/mozilla/chronicle/pulls/313,313,fix(styles): fixing invalid width in scss,"Fixes #311 
",pdehaan,557895,2015-02-09T22:34:29Z,CONTRIBUTOR,True,11,1,2,find everything you've ever found,JavaScript,51d9048abaf1660fe296e1ba07173054d6261ff9,fix(styles): Updating .scss-lint.yml file to work better with scss-lint 0.33.0+
33,https://api.github.com/repos/mozilla/chronicle/pulls/307,307,fix(server): Removes caching for root path,"Fixes #219.

So this feels like a hack, but it dawned on me that if we just reply with a string then hapi doesn't know the modification date and our caching problems go away. I'm ok with trashing this if there's a better way :smiley:.

@6a68 @pdehaan r?
",nchapman,3095,2015-02-09T17:09:25Z,CONTRIBUTOR,True,9,1,1,find everything you've ever found,JavaScript,c3ce1e64b3bf26986f3ae06e331322b70e6d55db,fix(server): Removes caching for root path.
34,https://api.github.com/repos/mozilla/chronicle/pulls/306,306,Issue 280,"@nchapman: sorry about that! give this a try
",johngruen,3323249,2015-02-09T04:34:29Z,CONTRIBUTOR,True,23,9,2,find everything you've ever found,JavaScript,09fe1c22fa9f1d036aa5dfbd0390d372c37fdd90,styles(destroy): iconify destroy button
35,https://api.github.com/repos/mozilla/chronicle/pulls/306,306,Issue 280,"@nchapman: sorry about that! give this a try
",johngruen,3323249,2015-02-09T04:34:29Z,CONTRIBUTOR,True,23,9,2,find everything you've ever found,JavaScript,018beec7188c3f3a912b0f70d743f330b2ea882e,style(actions): fix hit target for destroy button
36,https://api.github.com/repos/mozilla/chronicle/pulls/304,304,chore(build): Adding lint task to Travis,"Fixes #303 
",pdehaan,557895,2015-02-07T01:58:40Z,CONTRIBUTOR,True,1,0,1,find everything you've ever found,JavaScript,03bac2fd4bdf7f21766cbd3a1d5daf27cf05c072,chore(build): Adding lint task to Travis
37,https://api.github.com/repos/mozilla/chronicle/pulls/293,293,style(breakpoints): add basic breakpoint logic,"@nchapman: This seems like a good solution...what do you think?
",johngruen,3323249,2015-02-06T15:54:38Z,CONTRIBUTOR,True,47,28,8,find everything you've ever found,JavaScript,8778d86c6ce8b25ec6fb67a5ef5cf243a44f5511,style(breakpoints): add basic breakpoint logic
38,https://api.github.com/repos/mozilla/chronicle/pulls/292,292,styles(destroy): iconify destroy button,"@nchapman @pdehaan: this directly addresses #280, but it's also setting up formally styling the .actions area. This PR isn't intended to totally match @bryanbell's mockups, just an incremental improvement.
",johngruen,3323249,2015-02-06T00:36:11Z,CONTRIBUTOR,True,40,3,12,find everything you've ever found,JavaScript,d7b1857919547a45dd1c324410bca189e9c48746,styles(destroy): iconify destroy button
39,https://api.github.com/repos/mozilla/chronicle/pulls/290,290,feat(db): add migrations support,"Fixes #92

@nchapman @pdehaan r?

Seems to be working locally for me. If you want to play around with migrations to different levels, we currently have 0, 1, and 2; just do `./bin/migrate.js 0` or whichever int you'd like.

This does not solve the reindexing problem for elasticsearch, that will be in followup issue #291.
",jaredhirsch,96396,2015-02-06T00:07:42Z,MEMBER,True,484,3073,12,find everything you've ever found,JavaScript,82043847baa5c4f08db7c8025218f9c44d3b8694,"feat(db): add migrations support

Fixes #92"
40,https://api.github.com/repos/mozilla/chronicle/pulls/289,289,fix(front-end): Fixes endless scroll in Firefox,"Fixes #285.

@pdehaan r?
",nchapman,3095,2015-02-06T00:06:50Z,CONTRIBUTOR,True,1,1,1,find everything you've ever found,JavaScript,b7a13ac01b247439dba48d9574c9caf6188af455,fix(front-end): Fixes endless scroll in Firefox.
41,https://api.github.com/repos/mozilla/chronicle/pulls/286,286,style(user-menu): make user menu stylish,,johngruen,3323249,2015-02-05T17:25:58Z,CONTRIBUTOR,True,216,53,12,find everything you've ever found,JavaScript,0bee73ad8a160957877f152068da47c9e05c71f9,style(user-menu): make user menu stylish
42,https://api.github.com/repos/mozilla/chronicle/pulls/286,286,style(user-menu): make user menu stylish,,johngruen,3323249,2015-02-05T17:25:58Z,CONTRIBUTOR,True,216,53,12,find everything you've ever found,JavaScript,8b1035a0b03a87696b5f4c9e5a653b950691e8d9,fix(front-end): Fixes sign out link
43,https://api.github.com/repos/mozilla/chronicle/pulls/283,283,Properly delete visits,"@nchapman @pdehaan r?

TL;DR: visit deletion was busted because of the bug fixed by this patch.

The visit.delete model code was expecting an integer count to be returned by postgres, but `{count: <int>}` was getting returned instead. By the laws of javascript insanity, the code then declared shenanigans, told itself that `count > 0` was definitely not true, and, thus, decided that it was time to delete the user_page, when in fact visits still existed that corresponded to said user_page. That led to a postgres error being thrown and unhandled (because the visits table references the user_pages table as a foreign key, and we do _not_ cascade deletes, in order to avoid accidentally nuking data thanks to exactly this kind of bug!). I suspect/hope the DB connection leakage was happening when that uncaught error nuked the process, since systemctl merrily spun up a new one in response.

I've filed #282 to figure out wtf is going on with our error handling cleanup in the models; the problem may simply be that no graceful shutdown happened, so the db connection was leaked. Still seems highly suboptimal handle uncaught exceptions in this way, since, uh, I expect this won't be the last one that leaks :-\

To be continued, I guess.

I've also fixed a typo that was causing jshint to barf.

The bug fix commit fixes the problems we're seeing in the logs on dev, so (fingers crossed) dev should be solid again after landing this.
",jaredhirsch,96396,2015-02-05T00:19:25Z,MEMBER,True,17,11,2,find everything you've ever found,JavaScript,13ca8ce516e84f1c02bf239010a8acb90ae36e0f,fix typo
44,https://api.github.com/repos/mozilla/chronicle/pulls/283,283,Properly delete visits,"@nchapman @pdehaan r?

TL;DR: visit deletion was busted because of the bug fixed by this patch.

The visit.delete model code was expecting an integer count to be returned by postgres, but `{count: <int>}` was getting returned instead. By the laws of javascript insanity, the code then declared shenanigans, told itself that `count > 0` was definitely not true, and, thus, decided that it was time to delete the user_page, when in fact visits still existed that corresponded to said user_page. That led to a postgres error being thrown and unhandled (because the visits table references the user_pages table as a foreign key, and we do _not_ cascade deletes, in order to avoid accidentally nuking data thanks to exactly this kind of bug!). I suspect/hope the DB connection leakage was happening when that uncaught error nuked the process, since systemctl merrily spun up a new one in response.

I've filed #282 to figure out wtf is going on with our error handling cleanup in the models; the problem may simply be that no graceful shutdown happened, so the db connection was leaked. Still seems highly suboptimal handle uncaught exceptions in this way, since, uh, I expect this won't be the last one that leaks :-\

To be continued, I guess.

I've also fixed a typo that was causing jshint to barf.

The bug fix commit fixes the problems we're seeing in the logs on dev, so (fingers crossed) dev should be solid again after landing this.
",jaredhirsch,96396,2015-02-05T00:19:25Z,MEMBER,True,17,11,2,find everything you've ever found,JavaScript,a91ceb7cc59925a43ed009ccf379ff324ad37211,"bug(db): properly get and use count of remaining visits sharing a user_page

I am hoping the DB connection leakage was due to deletion throwing without
cleaning up properly...which is a problem to fix properly later (#282)"
45,https://api.github.com/repos/mozilla/chronicle/pulls/281,281,Digging into db problems,"Adding a bit of logging to try to figure out what's up with dev.
",jaredhirsch,96396,2015-02-04T23:43:35Z,MEMBER,False,7,3,2,find everything you've ever found,JavaScript,13ca8ce516e84f1c02bf239010a8acb90ae36e0f,fix typo
46,https://api.github.com/repos/mozilla/chronicle/pulls/281,281,Digging into db problems,"Adding a bit of logging to try to figure out what's up with dev.
",jaredhirsch,96396,2015-02-04T23:43:35Z,MEMBER,False,7,3,2,find everything you've ever found,JavaScript,b347eb8ae0d2f11f3c5510b844a4e05551a40e62,digging into the server bug
47,https://api.github.com/repos/mozilla/chronicle/pulls/277,277,style(footer): dejank the footer,,johngruen,3323249,2015-02-04T20:27:33Z,CONTRIBUTOR,True,42,8,5,find everything you've ever found,JavaScript,fa0fb0fd401590c0da9e5a6f895e8cc9282b6856,style(footer): dejank the footer
48,https://api.github.com/repos/mozilla/chronicle/pulls/272,272,fix(front-end): Fixed regressions and tweaked small things,"Fixes are here!

@6a68 @johngruen @pdehaan r?
",nchapman,3095,2015-02-04T18:47:02Z,CONTRIBUTOR,True,13,13,6,find everything you've ever found,JavaScript,772bb2eb4c1cdf6e63e6ed8036518810694fdc27,fix(front-end): Fixed regressions and tweaked small things
49,https://api.github.com/repos/mozilla/chronicle/pulls/269,269,"refactor(styles): start making things sassy, flexy, responsive","@nchapman: here you go
",johngruen,3323249,2015-02-04T16:12:32Z,CONTRIBUTOR,True,175,122,11,find everything you've ever found,JavaScript,302bc18b567127e1916f84657c5ff4546364f8b1,"refactor(styles): start making things sassy, flexy, responsive"
50,https://api.github.com/repos/mozilla/chronicle/pulls/268,268,feat(front-end): Adds date dividers to visits,"This should be merged after #265, #266, and #267.

---

Date dividers for visits with really basic relative dates (Today and Yesterday).

@6a68 @pdehaan r?
",nchapman,3095,2015-02-04T06:58:59Z,CONTRIBUTOR,True,74,7,6,find everything you've ever found,JavaScript,743522647aa770239808b83ed394a1cbab66a324,feat(front-end): Adds date dividers to visits
51,https://api.github.com/repos/mozilla/chronicle/pulls/267,267,feat(front-end): Endless scrolling of visits,"This should be merged after #265 and #266.

---

Here's a take on endless scrolling for visits. I wish it could be simplified further, but it at least does the job for now. It doesn't do anything to keep the back button working so we'll need to fix that in another pass (or target=blank all the things).

@6a68 let me know what you think about this... We can do something simpler if you think that's better.
",nchapman,3095,2015-02-04T05:58:14Z,CONTRIBUTOR,True,67,6,3,find everything you've ever found,JavaScript,cb688dff8093e0abbb21f32122ab3e04671c7481,feat(front-end): Endless scrolling of visits
52,https://api.github.com/repos/mozilla/chronicle/pulls/266,266,feat(front-end): Adds user info data with avatar to header,"This should be merged after #265.

---

Adds user info box to the header. Looks like this right now:

![screen shot 2015-02-03 at 8 15 24 pm](https://cloud.githubusercontent.com/assets/3095/6035087/7195b1ec-abe1-11e4-891b-8052e5e72c40.png)

Obviously not awesome yet, but ready for @johngruen to style up. We can have that happen in this branch or just merge this and clean up later. Either works for me.

@6a68 @pdehaan r?
",nchapman,3095,2015-02-04T04:20:27Z,CONTRIBUTOR,True,67,5,4,find everything you've ever found,JavaScript,f35374fcd3cb0cd2cf5b24fc0731deb070ddb345,feat(front-end): Adds user info data with avatar to header
53,https://api.github.com/repos/mozilla/chronicle/pulls/265,265,feat(front-end): Working search box and results,"Working search results are here! Lots of new views. The header is now a little stack of specialized views. The visit presenter has been reworked as a user page presenter. Tried to share a reasonable amount of code between visits and search. I avoided making many scss changes since @johngruen is working on that in a separate branch.

@6a68 @pdehaan r?
",nchapman,3095,2015-02-04T03:14:31Z,CONTRIBUTOR,True,341,47,22,find everything you've ever found,JavaScript,4c9d7bec9fefea64ef4e7adea5e872d1ca1835e3,feat(front-end): Working search box and results
54,https://api.github.com/repos/mozilla/chronicle/pulls/254,254,bug(ops): flatten convict configs per ops request,"Fixes #220 

Seems to be working for me locally. @pdehaan mind taking a look?
",jaredhirsch,96396,2015-02-02T21:48:05Z,MEMBER,True,382,443,25,find everything you've ever found,JavaScript,fc4a6a287f30702f063bd7f0cd78d711be69dd01,"bug(ops): flatten convict configs per ops request

Fixes #220"
55,https://api.github.com/repos/mozilla/chronicle/pulls/254,254,bug(ops): flatten convict configs per ops request,"Fixes #220 

Seems to be working for me locally. @pdehaan mind taking a look?
",jaredhirsch,96396,2015-02-02T21:48:05Z,MEMBER,True,382,443,25,find everything you've ever found,JavaScript,c1e99481e584f5b18a39bd0f5c2541d75f0cb386,bug(server): reduce test user creation timeout from 60 to 30 seconds
56,https://api.github.com/repos/mozilla/chronicle/pulls/242,242,feat(front-end): Adds image proxy via embedly,"This adds Embedly Display as an image proxy on the front-end. This should fix any mixed content warnings over https and does a little work to serve retina images when appropriate (could be brittle). This also replaces our use of CSS to ""crop"" images using background cover. This makes images actual img tags instead of scumbag background images.

@6a68 @pdehaan r?
",nchapman,3095,2015-02-01T00:12:00Z,CONTRIBUTOR,True,86,17,6,find everything you've ever found,JavaScript,da85b8ed9c239309f3832f760643b117860128a4,feat(front-end): Adds image proxy via embedly
57,https://api.github.com/repos/mozilla/chronicle/pulls/241,241,feat(front-end): Displaying screenshots instead of low entropy images,"Screenshots are in the mix! Also fixes #240 and #237.

@6a68 @pdehaan r?
",nchapman,3095,2015-01-31T04:56:43Z,CONTRIBUTOR,True,8,7,5,find everything you've ever found,JavaScript,47aae0378e75344a584a0ce3f86390206593c031,feat(front-end): Displaying screenshots instead of low entropy images
58,https://api.github.com/repos/mozilla/chronicle/pulls/239,239,fix(server): Fixes FxA sign in,"Seems like the fix for FxA sign in was just renaming the fetched user object. I'm going to merge this right away so I can fix the dev server unless someone is really quick to object :smile:.
",nchapman,3095,2015-01-31T04:06:08Z,CONTRIBUTOR,True,4,3,1,find everything you've ever found,JavaScript,9167f278809b7bf99591205eb73c30d7e5d92706,fix(server): Fixes FxA sign in.
59,https://api.github.com/repos/mozilla/chronicle/pulls/236,236,chore(sass): tweaking some styles per scss-lint feedback,"For more info on **scss-lint**, see https://github.com/causes/scss-lint.
It requires Ruby, so it doesn't make sense to add this to any ""lint"" workflows, but it is available as a npm script convenience method (`npm run scss-lint`) if you already have **scss-lint** installed locally.

Current output is this (but I don't agree with merging):

```
app/styles/modules/_visit.scss:115 [W] Merge rule `&.small + &.small` with rule on line 94
```
",pdehaan,557895,2015-01-31T01:37:48Z,CONTRIBUTOR,True,44,11,5,find everything you've ever found,JavaScript,a602d23f581dbfb6cd180934c2b8aa9e3081e46e,chore(sass): tweaking some styles per scss-lint
60,https://api.github.com/repos/mozilla/chronicle/pulls/235,235,fix(db): improve elasticsearch query,"Fixes #227 

Stole @nchapman's v0 search query, let's rock!
",jaredhirsch,96396,2015-01-31T00:15:06Z,MEMBER,True,15,19,1,find everything you've ever found,JavaScript,5a791f81190df9b76c2cd22990eb8b595a0bcd89,"fix(db): improve elasticsearch query

Fixes #227"
61,https://api.github.com/repos/mozilla/chronicle/pulls/234,234,feat(front-end): Real styles and real data displayed for visits,"Here's the first pass at getting real data from the API displayed and styled on the front-end. There's still a lot more polish to do, color variables to extract, etc, but I wanted to get this landed ASAP. Look forward for a follow on PR that will hook up search, user's avatar, site screenshots, delete button, and soo _much_ **more**!

@6a68 r?
",nchapman,3095,2015-01-30T23:53:49Z,CONTRIBUTOR,True,381,75,20,find everything you've ever found,JavaScript,faf906466bb6658f98a542836347466cddbe4531,feat(front-end): Real styles and real data displayed for visits.
62,https://api.github.com/repos/mozilla/chronicle/pulls/233,233,feat(api): add url2png to provide screenshots,"@nchapman r?
- add url2png package, update npm shrinkwrap
- add url2png keys to config
- add screenshotUrl to visit, visits, and search items
  - implemented as a view-level transform (not in the DB)
  - this patch introduces some duplication, but I want to separate the ""get it working"" task from the ""make it pretty"" refactoring task
- also fix a stray SQL error in `server/models/visits` that I missed with yesterday's giant PR
",jaredhirsch,96396,2015-01-30T21:09:13Z,MEMBER,True,160,12,8,find everything you've ever found,JavaScript,9380c0bf0d5a9f7a4bc83e5b6aa0ee08becd530b,feat(api): add url2png to provide screenshots
63,https://api.github.com/repos/mozilla/chronicle/pulls/232,232,WIP: send welcome email,"Questions:
- [**sendmail**](https://www.npmjs.com/package/sendmail) vs [**nodemailer**](https://www.npmjs.com/package/nodemailer) -- It looks like nodemailer has support for different transport options (such as Amazon SES via [**nodemailer-ses-transport**](https://github.com/andris9/nodemailer-ses-transport)), plugins (DKIM), HTML+plaintext...
- When should we use SES vs just SMTP? Currently just has a big `if (config.get('env') === 'local') {...}` block, but not sure if we should use SMTP when `env` is ""test"".
- There has to be a better way of doing templates than my hacky thing of using `_.template` (since we were already requiring underscore in package.json). Not sure if we should be using Handlebars or something else. I need to dig into what content/auth server is doing to see if there is a more official way.
- Should we have different queue tasks for welcome emails vs other emails, or just switch statements in the server/work-queue/jobs/send-email.js to say `if (o.type === 'welcome') {...}` versus just having specific jobs like ""send-welcome-email.js"".
",pdehaan,557895,2015-01-30T21:01:46Z,CONTRIBUTOR,True,3156,193,7,find everything you've ever found,JavaScript,95fd9a8dd6c17c13d8d296659d29f389bd78fa99,"feat(server): add welcome email job

Fixes #22"
64,https://api.github.com/repos/mozilla/chronicle/pulls/231,231,Issue 229 add profile endpoint,"@nchapman pour vous
",jaredhirsch,96396,2015-01-30T19:08:14Z,MEMBER,True,63,3,4,find everything you've ever found,JavaScript,bdeb2b5bf0815ea84bfbd032687d7a1227e582fe,bug(docs): correct install process in README
65,https://api.github.com/repos/mozilla/chronicle/pulls/231,231,Issue 229 add profile endpoint,"@nchapman pour vous
",jaredhirsch,96396,2015-01-30T19:08:14Z,MEMBER,True,63,3,4,find everything you've ever found,JavaScript,2e462b544021be0592c5cec5c81014cae6b11913,"feat(api): add /v1/profile endpoint

Fixes #229"
66,https://api.github.com/repos/mozilla/chronicle/pulls/226,226,feat(server): add scraper and lotsa other stuff,"Fixes #21 

For PR discussion see: https://github.com/mozilla/chronicle/pull/224

Merging as a crazy gigantic monolithic commit

![](http://media.giphy.com/media/Yb0A0j6yJFTVe/giphy.gif)

**VICTORY IS OURS**
",jaredhirsch,96396,2015-01-30T00:44:02Z,MEMBER,True,1675,3678,37,find everything you've ever found,JavaScript,ae2f293fbaab110010cd6f7b2ebbfc05f04c63ee,"feat(server): add scraper and lotsa other stuff

Fixes #21"
67,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,dff1ce8507d4e7f7b976a65f2ca7f6dc3fa2dbc4,add user_pages table
68,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,9ddd31a89933f7062fe23b7f69c934db16a2201a,"the inevitable db refactor commit, part 1 of ???

- extract postgres, elasticsearch noise to separate DBO files
- split models
- convert postgres DBO + models to use promises
- update callers to source deps correctly"
69,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,50df05d26c4b46d233771eecf5091e21ae8021bc,"stupid lil commit, use dashes in all filenames"
70,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,1b65bfa5fa9daa406e245b120a37da78429ec8a5,create a spot to store all the conventions and decisions as I make them
71,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,50c7a4e2ae2718c9ad6638d93e379ead81ab5f63,replace normalize func with db convention wut wut
72,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,46caacfd39adfad26ab44ff2752a39329114be98,ULTRA WIP server code docs
73,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,4cda0b34a832e7a7d725987a2fed6207844a112e,lazily create user_page in postgres when creating a visit
74,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,9be6098e57af0598f0c0002350b28a4cbc98cb76,minor updates to docs
75,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,a683f0edeb88f98741867dc7283e0eb2b9e0b6ac,"add embedly package, update shrinkwrap file"
76,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,d1f75023e21d97488507248d654f39f8eb7360d5,embedly queue job bits
77,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,a8a1c4e7691d90144d1c00b99d2fd1bbfe8e70ad,"module.exports is a singleton, so just use it as such"
78,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,6f308afe15981a05669536bb7a32391cd516e0f1,"stopping for the night.

- alpahbetize all those keys in user_pages just for sanity
- create user-pages before visits
- filter the embedly results
- update postgres queries to join across visits, user_pages + pull back goodness
- update the job to use user-pages model (makes the most sense, sucks though)
  - we no longer have clear boundary between tables and domain objects
  - this really sucks
  - but it works, so, oh well"
79,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,bd8786949abce84ad02be2548024ee217e6233a3,many tiny things to get visit + user-page creation + embedly working
80,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,19e696d307f6443d6ce82c89a8445e5b5a7717c3,"more WIPs, definitely not guaranteed to work"
81,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,2be8f9b3b5cd6723a87c5bfc2dd3a3e8cd124c41,disable lint step when running locally: let me hack on my server in peace
82,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,417935c1b13b69ee3c03e82b06774acb50f1d6ae,somehow committed a vim swapfile; update gitignore to prevent it happening again
83,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,3bd942a999176c22588f10d9e2825479ee24fe81,minor tweaks to get addon working with scraper magic
84,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,ecac3d7363649a810e7c1fe4c26993e4870fcd60,offset can be a huge number
85,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,160a0ffda998ef4de01cdb5f1dcb2a5347ed5e5c,typo
86,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,a01687fe21839a2040089925b40d01623daa098e,remove another dotfile urgh
87,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,6297f5c351dd9017407a5f0d00af0a0ffcc682da,get multi-visits returning _all the things_
88,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,8627d7db97105ae442d51a1fa2bea3951ebe3ba7,start splitting handlers out from routes
89,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,3bef763a96150423c44cebbecd01e29740beaa94,cache_age can easily exceed INTEGER; use BIGINT
90,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,4fcaa68e193e33acd7af4290938b2ad715b27014,"store embedly published as ISO time, not millis"
91,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,c55747a673d612c52ddaa4851dc6f85a5c234466,extract controllers for easy repl access
92,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,cbfbf145545788f50cc3e1d9718a561bc2cb42b0,"also extract controller from visits

oddly, git was saying it was a rename, I guess some ""smart"" diffing algo"
93,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,84c46bd9ed0646ed7b63b58891f5120a46ee6369,be a little more defensive with embedly results
94,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,57f38e3729feb8f357f1ac727044977395528d68,"search working: match on content, filter by user"
95,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,576630b3b441dedcf1538d1051f6f7451a05051d,Store embedly's funky published+offset as one UTC timestamp
96,https://api.github.com/repos/mozilla/chronicle/pulls/224,224,Scraper db all the things etc,"@nchapman @pdehaan ok.....here we go, readyish for review (I need to do a final code cleanup pass, but you guys can help with that :-).

This is a huge diff. I'll make it a goal to do a better job of splitting out subtasks next sprint.

I'm not sure the best way to ask for a review of this code. On one hand, leaving the 30-something individual commits makes it possible to understand wtf is happening in a local sense. On the other hand, some commits undo others, and at the end of the day, the whole massive diff has to make sense together.

The good news is, this is really a lot of fun to play around with, especially if you use the addon-equipped browser to insert some new visits. Suddenly it almost feels like a real product :rainbow:
### so you want to test this patch locally
#### prepare for log spewin' volcano action

You can set the log level to `verbose` to see all the stuff that flows in and out of the work queue, plus a lot of args to the models. This is fun for convincing yourself individual pieces work, but it can also get annoying. Every error is logged at the `warn` level, so setting log config back to `info` will give you errors and the occasional informative logline, and a lot less noise.
#### about the test data and test user

Run create_db and create_test_data to set up the database. create_db now wipes and recreates the chronicle index in elasticsearch; let me know if this fails for either of you. create_test_data now just shoves a list of URLs into the production code path. This means that embedly will scrape those pages in real time; be ready for a LOT of log output if you've got it set to verbose. The create_test_data script bails after 60 seconds, rather than try to figure out some way to quit when the queue is empty (I ran out of steam there).
#### about all the refactoring...

A lot of pieces got split apart, as I've been doing a ton of repl testing to convince myself things work. The route handlers have been moved under /controllers. The db code has been split out from the models. The models have been split into pieces. It's a lot more pieces, but there is some kind of form emerging.
#### adding visits

You should be able to fire up the mozilla-chronicle-addon code, surf to localhost:8080/auth/login, and then surf around and see those visits asynchronously added, then scraped. These new visits should show up in the main page, as well as in the search results.
##### dude, I can't find anything

I know. The current search implementation is horrible. It's only searching the content of the page, not the title or url or anything else that's useful. This makes it challenging to convince yourself a new page has been indexed or has been deleted. We'll fix this soon, maybe before I land this patch.
#### deleting visits

You can still delete visits from the homepage. Again, you should see them get deleted from elasticsearch, and (if you can find the right search term from the article body), you should see them vanish from the search results pretty rapidly.
#### updating visits

I actually haven't verified that this works. Need to dig out my notes on cURLing with cookies. It's not really in the product yet, so seems low priority to me.
### Other things
#### API docs: what me worry?

I'm returning a crazy mountain of data to the front-end right now. I'm really happy to just see that data, but maybe I should return a saner subset before landing this (thoughts?).
#### not yet handling bulk inserts

Next sprint, I guess.
#### underscores and camelCase

I decided to make life easier by inventing a little convention: all database things are underscored; all JS things are camelCased. The pg layer handles the underscore to camel conversion; all of the queries already refer to underscored versions. Oh, that reminds me:
#### there is yet another doc file

I created server/README halfway through the refactoring, so it's already out of date! _le sigh_ I think it's probably best to nuke it, for now, and reinstate it soonish with actually accurate information. I wound up with the models living in a funky limbo, halfway between ""model per table"" and ""model as business logic noun which abstracts multiple tables"". I hope to fix that soon - maybe when I take a day to evaluate ORM options. Feedback welcome...I'm happy to see things getting saner, but definitely not yet proud of the code :-P

There are probably a lot of details I'm forgetting. I'll go through the code tomorrow morning and annotate all the failsauce, as well as the winsauce and break-evensauce.

![](http://media.giphy.com/media/1GTZA4flUzQI0/giphy.gif)
",jaredhirsch,96396,2015-01-29T02:34:06Z,MEMBER,False,1446,673,37,find everything you've ever found,JavaScript,23eab8e4cd996ee8ba7179e8fd3d5b9944c7cc71,"Add support for disabling embedly

This is mainly for travisCI to do stuff without barfing"
97,https://api.github.com/repos/mozilla/chronicle/pulls/215,215,fix(db): closing some db pool connections,"Closes some unclosed Postgres pool connections. A quick search shows 9 instances of `done)` arguments and an equal number of `done()` callback invocations.

> **note:** if you do not call `done()` the client will never be returned to the pool and you will leak clients. This is mega-bad so always call `done()`
> &mdash; via https://github.com/brianc/node-postgres/wiki/pg#connectfunction-callback

Fixes #213 
",pdehaan,557895,2015-01-24T00:56:57Z,CONTRIBUTOR,True,3,8,1,find everything you've ever found,JavaScript,abd84ff6c432e45f3075aa07eb9ab4fcf5265468,fix(db): closing some db pool connections
98,https://api.github.com/repos/mozilla/chronicle/pulls/211,211,feat(heartbeat): Adding a __heartbeat__ route,"Fixes #204 
",pdehaan,557895,2015-01-23T22:14:55Z,CONTRIBUTOR,True,3061,202,4,find everything you've ever found,JavaScript,3c939734943bce5e25ac1b751ab564f63f98dabb,feat(heartbeat): Adding a __heartbeat__ route
99,https://api.github.com/repos/mozilla/chronicle/pulls/211,211,feat(heartbeat): Adding a __heartbeat__ route,"Fixes #204 
",pdehaan,557895,2015-01-23T22:14:55Z,CONTRIBUTOR,True,3061,202,4,find everything you've ever found,JavaScript,64920dbc8fac74983da4683154d65cc9433241a3,feat(heartbeat): Adding redis check to __heartbeat__
100,https://api.github.com/repos/mozilla/chronicle/pulls/211,211,feat(heartbeat): Adding a __heartbeat__ route,"Fixes #204 
",pdehaan,557895,2015-01-23T22:14:55Z,CONTRIBUTOR,True,3061,202,4,find everything you've ever found,JavaScript,5956cb785d84cdbe4596baef99ac2e0480569cd3,feat(heatbeat): Updating shrinkwrap file because Travis
101,https://api.github.com/repos/mozilla/chronicle/pulls/209,209,Issue 91 elasticsearch init,"This isn't yet integrated into the create_test_data script. However, that test data is not going to search well at all in its current form, since every title is the same.

The best way to play around with this PR is to install the addon, surf around a bit, say, to some mozilla sites, then manually hit the endpoint, something like  `localhost:8080/v1/search?q=mozilla`, and see what you can make out in the returned JSON.

I have a ton of TODOs which I've added to #91, to be handled later in the current sprint. One step at a time :-)
",jaredhirsch,96396,2015-01-22T01:25:29Z,MEMBER,True,298,46,15,find everything you've ever found,JavaScript,b9d71467aaa4c81edb67b6355c96763a6d0ed30a,refactor(server): move db create scripts to /bin; rename /db to /models
102,https://api.github.com/repos/mozilla/chronicle/pulls/209,209,Issue 91 elasticsearch init,"This isn't yet integrated into the create_test_data script. However, that test data is not going to search well at all in its current form, since every title is the same.

The best way to play around with this PR is to install the addon, surf around a bit, say, to some mozilla sites, then manually hit the endpoint, something like  `localhost:8080/v1/search?q=mozilla`, and see what you can make out in the returned JSON.

I have a ton of TODOs which I've added to #91, to be handled later in the current sprint. One step at a time :-)
",jaredhirsch,96396,2015-01-22T01:25:29Z,MEMBER,True,298,46,15,find everything you've ever found,JavaScript,b1408dcc44b5fa2a83958d0e86877be7720b42c0,"feat(server): full-text search, version 1

- install elasticsearch, update shrinkwrap
- add /v1/search route to API
- update docs/API.md with basic info
- add search method to visits model which queries ES, reformats results
  - currently only matching on page title
- update elasticsearch as part of creating, updating, or deleting a visit"
103,https://api.github.com/repos/mozilla/chronicle/pulls/208,208,docs: Removing instances of MySQL from the repo,"Fixes #207 
",pdehaan,557895,2015-01-22T01:09:18Z,CONTRIBUTOR,True,5,5,3,find everything you've ever found,JavaScript,d50a12cb69ee6aec7c10743d8ac18d41104181ea,docs: Removing instances of MySQL from the repo
104,https://api.github.com/repos/mozilla/chronicle/pulls/145,145,fix(build): Added optional user param to create_db.sh,"So here's what I was thinking as a quick fix for specifying the user in the `create_db.sh` script. Good enough for now?

@6a68 @pdehaan r?
",nchapman,3095,2015-01-20T19:41:27Z,CONTRIBUTOR,True,13,8,2,find everything you've ever found,JavaScript,077a000034063796cd8d6577272507a869a1b02c,fix(build): Added optional user param to create_db.sh
105,https://api.github.com/repos/mozilla/chronicle/pulls/122,122,refactor(db): switching from  to  module,"DO NOT MERGE

Proof of concept to show switching from [**pg**](https://www.npmjs.com/package/pg) module to [**pg-db**](https://www.npmjs.com/package/pg-db) module.

This lets us remove a bunch of the `pg.connect(dbParams, function(err, client, done) {...});` blocks and take advantage of **pg-db**'s magical abilities to:

> - Automatically return connections to the pool - no need to call `done()`
> - SQL errors automatically destroy the connection - no need to call `done(err)`

This will also make it easier to make a switch from positional arguments (`$1`) to named arguments (`:fxaId`), or if we want to create a promise-based wrapper for queries instead of using callbacks w/ `cb(err, data)`.
",pdehaan,557895,2015-01-20T02:22:31Z,CONTRIBUTOR,False,2975,210,3,find everything you've ever found,JavaScript,1d27fdf97ec2c1010a67d239081b8e3f9316cf06,refactor(db): switching from  to  module
106,https://api.github.com/repos/mozilla/chronicle/pulls/121,121,"feat(server): add work queues, create visits using the queue","for the moment, the only working job is the one that creates visits.

testing the visit creation with curl:
1.  get the cookies and dump them in a file: `curl -c cookies.txt http://localhost:8080/auth/login`
2. POST using the cookie file, eg: `curl -b cookies.txt -H ""Content-Type: application/json"" -X POST http://localhost:8080/v1/visits -d '{""url"":""bar.com"",""title"":""bar"",""visitedAt"":""2015-01-16T23:41:21.354Z""}'`
",jaredhirsch,96396,2015-01-17T00:15:48Z,MEMBER,True,371,2863,12,find everything you've ever found,JavaScript,1c31abacf7f9d3ced2ac05ab26dfa090db52c465,"feat(server): add work queues, create visits using the queue"
107,https://api.github.com/repos/mozilla/chronicle/pulls/121,121,"feat(server): add work queues, create visits using the queue","for the moment, the only working job is the one that creates visits.

testing the visit creation with curl:
1.  get the cookies and dump them in a file: `curl -c cookies.txt http://localhost:8080/auth/login`
2. POST using the cookie file, eg: `curl -b cookies.txt -H ""Content-Type: application/json"" -X POST http://localhost:8080/v1/visits -d '{""url"":""bar.com"",""title"":""bar"",""visitedAt"":""2015-01-16T23:41:21.354Z""}'`
",jaredhirsch,96396,2015-01-17T00:15:48Z,MEMBER,True,371,2863,12,find everything you've ever found,JavaScript,0ce1d2613d0a338d8672e30f846b614db7eeedc3,fix(tests): Update Travis to work with the queue
108,https://api.github.com/repos/mozilla/chronicle/pulls/120,120,refactor(db): creates the server/db/create_db.sh from a template,"Not sure how I feel about it, but it seems to ""work for me"".

The `grunt template` task gets built from `grunt build` so building the server/db/create_db.sh should be fairly automatic.

Fixes #116 
",pdehaan,557895,2015-01-17T00:07:13Z,CONTRIBUTOR,False,53,35,4,find everything you've ever found,JavaScript,63d9aae5a7df734a4429854c59e3c33543a1fa40,refactor(db): creates the server/db/create_db.sh from a template
109,https://api.github.com/repos/mozilla/chronicle/pulls/117,117,feat(server): initial commit of embedly worker,,pdehaan,557895,2015-01-15T23:52:20Z,CONTRIBUTOR,True,2740,204,10,find everything you've ever found,JavaScript,e485491d166165414aec2e9336521dbbdfc28b78,feat(server): inital commit of embedly worker
110,https://api.github.com/repos/mozilla/chronicle/pulls/114,114,"bug(server): emit 401 on auth failure, not 302","Fixes #106 
",jaredhirsch,96396,2015-01-15T00:20:26Z,MEMBER,True,0,1,1,find everything you've ever found,JavaScript,fbcc90295dff2c265119b979f2b387155f1ea5b4,"bug(server): emit 401 on auth failure, not 302

Fixes #106"
111,https://api.github.com/repos/mozilla/chronicle/pulls/113,113,feat(db): replace mysql with postgres,"@pdehaan @nchapman @dannycoates r?

_edit: doh, I always think '@dcoates'. Thanks, Kit!_
",jaredhirsch,96396,2015-01-15T00:08:21Z,MEMBER,True,372,394,13,find everything you've ever found,JavaScript,604cc3ca45d9a1b753a9dc3dfb184c2a319fde33,"feat(db): replace mysql with postgres

Fixes #102

![](http://media.giphy.com/media/J8Iqja8CXfMw8/giphy.gif)"
112,https://api.github.com/repos/mozilla/chronicle/pulls/110,110,fix(requirejs): Fixing requirejs config on app.html,"Fallout from the great index.html -> app.html refactor of #107 
",pdehaan,557895,2015-01-13T20:06:23Z,CONTRIBUTOR,True,2,2,2,find everything you've ever found,JavaScript,bdfce49edfe9d3ac7af901834bc4b9974dbc0d8e,fix(requirejs): Fixing requirejs config on app.html
113,https://api.github.com/repos/mozilla/chronicle/pulls/109,109,fix(server): strip trailing slashes from server routes,"Turned out to be easier than thought, as the paths didn't have any existing trailing slashes; so just a quick config change.

FIxes #73
",pdehaan,557895,2015-01-13T19:55:57Z,CONTRIBUTOR,True,11,3,1,find everything you've ever found,JavaScript,b044253711750206c5b4b957564e1b8d82557d80,fix(server): strip trailing slashes from server routes
114,https://api.github.com/repos/mozilla/chronicle/pulls/107,107,feat(front-end): Added sign in and sign out links,"We'll have to figure out the JS story on the home page, but this is the simplest thing that works for now.

@6a68 r?

Fixes #69 
",nchapman,3095,2015-01-13T01:10:17Z,CONTRIBUTOR,True,43,26,3,find everything you've ever found,JavaScript,6e0549fd198ecfbb6b138bcac1aeb72ba95aff2c,feat(front-end): Added sign in and sign out links
115,https://api.github.com/repos/mozilla/chronicle/pulls/105,105,fix(docs): Adding note about grunt-cli requirement,"Fixes #101 
",pdehaan,557895,2015-01-13T00:02:22Z,CONTRIBUTOR,True,1,1,1,find everything you've ever found,JavaScript,8d960e2f2b430f8f320554d2912e4b24cc65c591,fix(docs): Adding note about grunt-cli requirement
116,https://api.github.com/repos/mozilla/chronicle/pulls/104,104,fix(logging): Fixing the deleteVisit logging message,"Typo: changing `updateVisit` to `deleteVisit`.
",pdehaan,557895,2015-01-12T23:50:36Z,CONTRIBUTOR,True,1,1,1,find everything you've ever found,JavaScript,9abc5b264ac190accfbdf44841e97f153938e831,fix(logging): Fixing the deleteVisit logging message
117,https://api.github.com/repos/mozilla/chronicle/pulls/99,99,bug(db): shorten email field by one char to avoid mysql drama,,jaredhirsch,96396,2015-01-10T20:19:21Z,MEMBER,True,1,1,1,find everything you've ever found,JavaScript,453f95a31d28cf5c62d948b37b78ecdc0ff9075a,bug(db): shorten email field by one char to avoid mysql drama
118,https://api.github.com/repos/mozilla/chronicle/pulls/97,97,chore(build): Cleaning up some unused vars and eslint warnings,"Fixes #94 
",pdehaan,557895,2015-01-10T01:05:54Z,CONTRIBUTOR,True,20,18,7,find everything you've ever found,JavaScript,77150d5446da6f8283a21e19eb4ca9c7662f8178,chore(build): Cleaning up some unused vars and eslint warnings
119,https://api.github.com/repos/mozilla/chronicle/pulls/97,97,chore(build): Cleaning up some unused vars and eslint warnings,"Fixes #94 
",pdehaan,557895,2015-01-10T01:05:54Z,CONTRIBUTOR,True,20,18,7,find everything you've ever found,JavaScript,e2d1628400eae0d32b7eef5f9366265ddc6f1a70,chore(build): Cleaning up some unused vars and eslint warnings
120,https://api.github.com/repos/mozilla/chronicle/pulls/96,96,chore(build): renaming npm validate task to validate-shrinkwrap,"Now the I-will-never-fail `grunt validate-shrinkwrap` task won't get run on every commit via the pre-commit hook.
I just saved you 2s per commit. You're welcome.

Fixes #95 
",pdehaan,557895,2015-01-10T00:05:45Z,CONTRIBUTOR,True,3,3,2,find everything you've ever found,JavaScript,abd74628497d49364cb7cc8277d44c7f2aee5ad5,chore(build): renaming npm validate task to validate-shrinkwrap
121,https://api.github.com/repos/mozilla/chronicle/pulls/93,93,feat(tests): Setup Intern for functional tests,,nchapman,3095,2015-01-09T22:39:39Z,CONTRIBUTOR,True,2970,45,11,find everything you've ever found,JavaScript,95eaf19aa903cd44ac8c5e5ac17c62a4cd343bfd,feat(tests): Setup Intern for functional tests
122,https://api.github.com/repos/mozilla/chronicle/pulls/86,86,Fixup auth,"I tried to be a bit more descriptive in the commit messages, as it was a little late to make this stuff a set of atomic commits, but I still wanted to capture some extra detail. I'll just paste those in as a PR description.

When doing local development with the test user, this patch lets you simply surf to /auth/login and a session cookie will be set, skipping the oauth flow completely. All other funky special casing code is gone. Dang, that's nice. (Normally, hitting /auth/login redirects to another endpoint, which redirects to the fxa oauth flow, which eventually ends with a cookie being set, if the oauth stuff worked out successfully.)

Comments welcome!

---

 chore(server): fixup auth

Correctly use Bell to handle the OAuth nonce. Correctly set and validate
cookies (both that the fxaId exists, and that any TTL hasn't expired).
Fixes #45.

Replace log.trace with log.verbose everywhere, since we never really
want a trace of the function calls.

Enable cookie-based sessions for the visits API, and modify the cookie
validation function to always allow access if the `testUser` config
option is set. Fixes #76.

Add crude placeholder API docs for auth endpoints.

---

 chore(api): simplify handling of test user

With this patch, we handle the test user by simply setting a cookie as soon as
the user hits the /auth/login endpoint. This is the only bit of special case
code that we need; the cookie is otherwise normal, is validated normally, and
none of the affected routes need any special case code. w00t!
",jaredhirsch,96396,2015-01-09T01:31:20Z,MEMBER,True,395,2696,12,find everything you've ever found,JavaScript,6d161881bb3b147f243d7ddddbbb22a0dd91002e,"feat(server): get oauth/session auth working with visits API

Correctly use Bell to handle the OAuth nonce. Correctly set and validate
cookies (both that the fxaId exists, and that any TTL hasn't expired).
Fixes #45.

Replace log.trace with log.verbose everywhere, since we never really
want a trace of the function calls.

Enable cookie-based sessions for the visits API, and modify the cookie
validation function to always allow access if the `testUser` config
option is set. Fixes #76.

Add crude placeholder API docs for auth endpoints."
123,https://api.github.com/repos/mozilla/chronicle/pulls/86,86,Fixup auth,"I tried to be a bit more descriptive in the commit messages, as it was a little late to make this stuff a set of atomic commits, but I still wanted to capture some extra detail. I'll just paste those in as a PR description.

When doing local development with the test user, this patch lets you simply surf to /auth/login and a session cookie will be set, skipping the oauth flow completely. All other funky special casing code is gone. Dang, that's nice. (Normally, hitting /auth/login redirects to another endpoint, which redirects to the fxa oauth flow, which eventually ends with a cookie being set, if the oauth stuff worked out successfully.)

Comments welcome!

---

 chore(server): fixup auth

Correctly use Bell to handle the OAuth nonce. Correctly set and validate
cookies (both that the fxaId exists, and that any TTL hasn't expired).
Fixes #45.

Replace log.trace with log.verbose everywhere, since we never really
want a trace of the function calls.

Enable cookie-based sessions for the visits API, and modify the cookie
validation function to always allow access if the `testUser` config
option is set. Fixes #76.

Add crude placeholder API docs for auth endpoints.

---

 chore(api): simplify handling of test user

With this patch, we handle the test user by simply setting a cookie as soon as
the user hits the /auth/login endpoint. This is the only bit of special case
code that we need; the cookie is otherwise normal, is validated normally, and
none of the affected routes need any special case code. w00t!
",jaredhirsch,96396,2015-01-09T01:31:20Z,MEMBER,True,395,2696,12,find everything you've ever found,JavaScript,89e646ca6a17edc84bca5443f04115dcd916bfe2,"fix(api): simplify handling of test user

With this patch, we handle the test user by simply setting a cookie as soon as
the user hits the /auth/login endpoint. This is the only bit of special case
code that we need; the cookie is otherwise normal, is validated normally, and
none of the affected routes need any special case code. w00t!"
124,https://api.github.com/repos/mozilla/chronicle/pulls/86,86,Fixup auth,"I tried to be a bit more descriptive in the commit messages, as it was a little late to make this stuff a set of atomic commits, but I still wanted to capture some extra detail. I'll just paste those in as a PR description.

When doing local development with the test user, this patch lets you simply surf to /auth/login and a session cookie will be set, skipping the oauth flow completely. All other funky special casing code is gone. Dang, that's nice. (Normally, hitting /auth/login redirects to another endpoint, which redirects to the fxa oauth flow, which eventually ends with a cookie being set, if the oauth stuff worked out successfully.)

Comments welcome!

---

 chore(server): fixup auth

Correctly use Bell to handle the OAuth nonce. Correctly set and validate
cookies (both that the fxaId exists, and that any TTL hasn't expired).
Fixes #45.

Replace log.trace with log.verbose everywhere, since we never really
want a trace of the function calls.

Enable cookie-based sessions for the visits API, and modify the cookie
validation function to always allow access if the `testUser` config
option is set. Fixes #76.

Add crude placeholder API docs for auth endpoints.

---

 chore(api): simplify handling of test user

With this patch, we handle the test user by simply setting a cookie as soon as
the user hits the /auth/login endpoint. This is the only bit of special case
code that we need; the cookie is otherwise normal, is validated normally, and
none of the affected routes need any special case code. w00t!
",jaredhirsch,96396,2015-01-09T01:31:20Z,MEMBER,True,395,2696,12,find everything you've ever found,JavaScript,63dc2f124ca5e88da1bb6c4e16cdcddc673c2ef4,refactor(server): move routes into a subdirectory
125,https://api.github.com/repos/mozilla/chronicle/pulls/85,85,chore(build): Updating mozlog and mysql dependencies,"Updating the last 2 outdated modules, as per #70 comment.
",pdehaan,557895,2015-01-08T21:45:19Z,CONTRIBUTOR,True,17,22,2,find everything you've ever found,JavaScript,a41fb57a0ee656b8959279663bfe51a90baf4828,chore(build): Updating mozlog and mysql dependencies
126,https://api.github.com/repos/mozilla/chronicle/pulls/83,83,bug(server): reduce server verbosity,"Fixes #65.

This hides the server error text when the logging level is set to anything lower than 'trace', as well as the one log call that was spitting out the fxaId in one of the visits routes.
",jaredhirsch,96396,2015-01-08T01:48:19Z,MEMBER,True,6,3,2,find everything you've ever found,JavaScript,d3337d267529a45255c8ae143f7b034e75b668da,"bug(server): reduce server verbosity

Hide extreme debugging detail unless server is running in 'trace' mode.

Fixes #65."
127,https://api.github.com/repos/mozilla/chronicle/pulls/81,81,docs(api): first pass at Visits API docs,"Formatted preview: https://github.com/mozilla/chronicle/wiki/API-and-DB-notes
",jaredhirsch,96396,2015-01-08T01:23:32Z,MEMBER,True,55,0,1,find everything you've ever found,JavaScript,03f0bee03641b27abdd697bd7bfce094c9b681b9,"docs(api): first pass at Visits API docs

Fixes #77"
128,https://api.github.com/repos/mozilla/chronicle/pulls/80,80,chore(build): Adding grunt-rev and grunt-usemin tasks for asset versioning,,pdehaan,557895,2015-01-08T01:21:54Z,CONTRIBUTOR,True,2569,136,12,find everything you've ever found,JavaScript,415fb9a4eeac61b3d68e356035e6f10ddad23410,chore(build): Adding grunt-rev and grunt-usemin tasks for asset versioning
129,https://api.github.com/repos/mozilla/chronicle/pulls/80,80,chore(build): Adding grunt-rev and grunt-usemin tasks for asset versioning,,pdehaan,557895,2015-01-08T01:21:54Z,CONTRIBUTOR,True,2569,136,12,find everything you've ever found,JavaScript,e62019665d4388b52ec1667fd62ec94f24d1aec5,feat(app): Adding rev and usemin tasks and fixing routing
130,https://api.github.com/repos/mozilla/chronicle/pulls/80,80,chore(build): Adding grunt-rev and grunt-usemin tasks for asset versioning,,pdehaan,557895,2015-01-08T01:21:54Z,CONTRIBUTOR,True,2569,136,12,find everything you've ever found,JavaScript,42420b068978380e89e6049358837f1d98e1d39b,chore(build): Added the template task back to grunt build
131,https://api.github.com/repos/mozilla/chronicle/pulls/80,80,chore(build): Adding grunt-rev and grunt-usemin tasks for asset versioning,,pdehaan,557895,2015-01-08T01:21:54Z,CONTRIBUTOR,True,2569,136,12,find everything you've ever found,JavaScript,4685ba5166c4c79c3c06b0cb14cb6d52ac4464c7,chore(build): Updating npm-shrinkwrap.json file
132,https://api.github.com/repos/mozilla/chronicle/pulls/80,80,chore(build): Adding grunt-rev and grunt-usemin tasks for asset versioning,,pdehaan,557895,2015-01-08T01:21:54Z,CONTRIBUTOR,True,2569,136,12,find everything you've ever found,JavaScript,de5915b6d0b60f01d127b6413c1b7cb3c6832816,chore(build): Fixing catch-all route for staticPath
133,https://api.github.com/repos/mozilla/chronicle/pulls/80,80,chore(build): Adding grunt-rev and grunt-usemin tasks for asset versioning,,pdehaan,557895,2015-01-08T01:21:54Z,CONTRIBUTOR,True,2569,136,12,find everything you've ever found,JavaScript,76295f411a30beb4ab54bcdf30bee13f71d28655,chore(build): tweaking the usemin order and removing production target from grunt serve
134,https://api.github.com/repos/mozilla/chronicle/pulls/79,79,bug(api): correctly sort visits,"Yikes, dunno how I missed this.

We want to return the paged visit data in reverse chronological order: newest records first, older records later.

To test this PR actually fixed the pagination behavior:
1. `node server/db/create_db.js`
2. `node server/db/create_test_data.js --count 100` to get several pages of records
3. `npm start`
4. the first page of data should start at number 99 and move backwards through time to number 75: `localhost:8080/v1/visits`
5. grab the ID from the last item on the first page, put it in the URL, and you should see the second page of results, from number 74 to number 50: `localhost:8080/v1/visits?visitId=whatever`

@nchapman mind having a look?
",jaredhirsch,96396,2015-01-08T00:16:19Z,MEMBER,True,3,3,1,find everything you've ever found,JavaScript,0a1c06766fd8a6e0dda1ed6ccfb819d054c9b346,bug(api): correctly sort visits
135,https://api.github.com/repos/mozilla/chronicle/pulls/75,75,feat(front-end): Added basic views for actually viewing visits.,"Visualized visits are here!

![screen shot 2015-01-07 at 3 15 02 pm](https://cloud.githubusercontent.com/assets/3095/5655189/01a78732-9680-11e4-8057-d1eaf41fc629.png)

You'll need to have run `server/db/create_test_data.js` to see anything at all.

@pdehaan @6a68 r?
",nchapman,3095,2015-01-07T23:18:22Z,CONTRIBUTOR,True,86,4,7,find everything you've ever found,JavaScript,ef9253074ac17c5adcc21f4b2f83daeabee54226,feat(front-end): Added basic views for actually viewing visits.
136,https://api.github.com/repos/mozilla/chronicle/pulls/74,74,chore(git): exclude npm-shrinkwrap.json from git diffs,"This change will exclude npm-shrinkwrap.json from the output of `git diff` as well as `git log -p`.

Got the idea from [SO](http://stackoverflow.com/questions/1016798/excluding-files-from-git-diff).

Thoughts?
",jaredhirsch,96396,2015-01-07T22:39:42Z,MEMBER,True,1,0,1,find everything you've ever found,JavaScript,67fdc78e4d595abfa22648a386c12865d549b4ed,chore(git): exclude npm-shrinkwrap.json from git diffs
137,https://api.github.com/repos/mozilla/chronicle/pulls/72,72,"feat(server): add visits API, temporarily relax auth.","This PR is kind of crazy; I might start submitting large PRs with a series of nice-looking atomic commits, just to make them easier to read and review.

I'll add comments inline.

Not sure who wants it, @nchapman? @pdehaan?
",jaredhirsch,96396,2015-01-06T23:33:20Z,MEMBER,True,837,2620,12,find everything you've ever found,JavaScript,ff2a497c0f8c4a68df94368423600e84dc2bfae2,"feat(server): Add visits API, temporarily relax auth"
138,https://api.github.com/repos/mozilla/chronicle/pulls/71,71,chore(build): Embed config values in compiled JavaScript,"Fixes #47.

This could end up being brittle, but it does the job for now. We can reassess this if our requirements become more complex.

@pdehaan r?
",nchapman,3095,2015-01-06T23:13:09Z,CONTRIBUTOR,True,51,2,6,find everything you've ever found,JavaScript,615c3cfe8898075b64fdc0d1be42d5a13c8ad9f3,chore(build): Embed config values in compiled JavaScript
139,https://api.github.com/repos/mozilla/chronicle/pulls/70,70,chore(build): Updating a couple dependencies and removing jshint devDep,"Fixed a couple outdated dependencies and removed an odd ""jshint"" dependency (odd because that should be included via grunt-contrib-jshint anyways).

We should still have about 2 outdated modules (`mozlog` and `mysql`, both with patch level version bumps -- not that we really care right now):

```
$ npm run outdated

> mozilla-chronicle@0.1.0 outdated /Users/pdehaan/dev/github/chronicle
> npm outdated --depth 0

Package  Current  Wanted  Latest  Location
mozlog     1.0.0   1.0.0   1.0.1  mozlog
mysql      2.5.3   2.5.3   2.5.4  mysql
```
",pdehaan,557895,2015-01-06T22:07:47Z,CONTRIBUTOR,True,960,572,2,find everything you've ever found,JavaScript,e37139a0b7a18e9080d14e2d48c38a35fd41f24d,chore(build): Updating a couple dependencies and removing jshint devDep
140,https://api.github.com/repos/mozilla/chronicle/pulls/68,68,chore(build): Added images directory to grunt tasks,"Fixes #58.

@pdehaan r?
",nchapman,3095,2015-01-06T17:41:37Z,CONTRIBUTOR,True,14,6,5,find everything you've ever found,JavaScript,6a2d97d48f2863741ea3845c5b74d6636190088a,chore(build): Added images directory to grunt tasks.
141,https://api.github.com/repos/mozilla/chronicle/pulls/66,66,chore(build): Changing the default build directory to be public/ instead of dist/,"Note that I tweaked the _config/local.json.example_ template, so you'll need to manually tweak your _config/local.json_ config files to match (if you want to see the magic files output to ""public/"" or somewhere custom).

r? @nchapman @6a68 

Fixes #67 
",pdehaan,557895,2015-01-05T20:57:17Z,CONTRIBUTOR,True,19,15,10,find everything you've ever found,JavaScript,b24f25810f9558c6c54532823548d638996158f5,chore(build): Changing the default build directory to be public/ instead of dist/
142,https://api.github.com/repos/mozilla/chronicle/pulls/66,66,chore(build): Changing the default build directory to be public/ instead of dist/,"Note that I tweaked the _config/local.json.example_ template, so you'll need to manually tweak your _config/local.json_ config files to match (if you want to see the magic files output to ""public/"" or somewhere custom).

r? @nchapman @6a68 

Fixes #67 
",pdehaan,557895,2015-01-05T20:57:17Z,CONTRIBUTOR,True,19,15,10,find everything you've ever found,JavaScript,a177418a0cda287f31b329ecf0882063b3102be5,chore(build): Changing the default build directory to be public/ instead of dist/
143,https://api.github.com/repos/mozilla/chronicle/pulls/66,66,chore(build): Changing the default build directory to be public/ instead of dist/,"Note that I tweaked the _config/local.json.example_ template, so you'll need to manually tweak your _config/local.json_ config files to match (if you want to see the magic files output to ""public/"" or somewhere custom).

r? @nchapman @6a68 

Fixes #67 
",pdehaan,557895,2015-01-05T20:57:17Z,CONTRIBUTOR,True,19,15,10,find everything you've ever found,JavaScript,d5859b99e422386bf58a3168e7784d9ad007c1cd,chore(build): Moving the staticPath config into a grunt variable.
144,https://api.github.com/repos/mozilla/chronicle/pulls/66,66,chore(build): Changing the default build directory to be public/ instead of dist/,"Note that I tweaked the _config/local.json.example_ template, so you'll need to manually tweak your _config/local.json_ config files to match (if you want to see the magic files output to ""public/"" or somewhere custom).

r? @nchapman @6a68 

Fixes #67 
",pdehaan,557895,2015-01-05T20:57:17Z,CONTRIBUTOR,True,19,15,10,find everything you've ever found,JavaScript,3f4bbe4f881f9e97e5a0132f502dd7e72567a1fa,Removing unrelated logo
145,https://api.github.com/repos/mozilla/chronicle/pulls/63,63,fix: Updating the copyright year and adding repo link,"Also added a missing `</div>` so that we pass a basic HTML lint.

Fixes #62 
",pdehaan,557895,2015-01-03T21:28:33Z,CONTRIBUTOR,True,2,1,1,find everything you've ever found,JavaScript,6789564844f2974bf7bafe0ffd694cf186939591,fix: Updating the copyright year and adding repo link
146,https://api.github.com/repos/mozilla/chronicle/pulls/60,60,chore(build): Improved performance of watch and build tasks,"Watch is now configured to call specific tasks when certain files change. This will be more work to maintain but the performance should be much better as the project grows. The `requirejs` task now has `development` and `production` targets to improve performance when using `grunt serve`.

@6a68 @pdehaan r?
",nchapman,3095,2014-12-31T22:17:38Z,CONTRIBUTOR,True,72,45,7,find everything you've ever found,JavaScript,4324203b3ad328fe11435bb3e4863a0ec781135c,chore(build): Improved performance of watch and build tasks.
147,https://api.github.com/repos/mozilla/chronicle/pulls/59,59,chore(build): Added serve task,"- configured watch task to automatically reload server and rebuild assets
- server/index.js now exports the server object
- added bin/www to start the server directly (not sure how I feel about this)

@6a68 @pdehaan this still needs a few additions but is definitely ready for some feedback.
### TODO
- [x] Update README
",nchapman,3095,2014-12-30T22:58:56Z,CONTRIBUTOR,True,160,19,9,find everything you've ever found,JavaScript,99db36a7ae97917437b9423c3fe15eeec3951efc,chore(build): Added serve task
148,https://api.github.com/repos/mozilla/chronicle/pulls/57,57,Streamlined css flow,"Source maps seem to be mostly working for css. I'm not sure what the exact behavior is expected to be. `main.css` file is no longer created in `app` and is instead created directly in `dist`. I made the switch to `normalize-scss` and am importing that in `_base.scss` so we no longer have to copy over `bower_components`.

Fixes #56.

_This should be merged after #55._

@pdehaan r?
",nchapman,3095,2014-12-24T01:40:16Z,CONTRIBUTOR,True,14,23,7,find everything you've ever found,JavaScript,56f09b27e3ee1e11696aff515f39f31c6147ba41,Streamlined css tasks.
149,https://api.github.com/repos/mozilla/chronicle/pulls/55,55,Added require.js grunt task,"One js file with everything in it and working source map (oh my)! Hopefully I did the shrinkwrap business correctly.

@pdehaan r?
",nchapman,3095,2014-12-24T01:10:00Z,CONTRIBUTOR,True,2050,149,5,find everything you've ever found,JavaScript,0c5649294a9a9326cbf2d08a0869edc18ba313c7,Added require.js grunt task.
150,https://api.github.com/repos/mozilla/chronicle/pulls/54,54,chore(build): Adding grunt-todo task,,pdehaan,557895,2014-12-24T00:34:57Z,CONTRIBUTOR,True,52,14,7,find everything you've ever found,JavaScript,f825b07ac24a32f6a1e52bc1333ab52b76de1b11,chore(build): Adding grunt-todo task
151,https://api.github.com/repos/mozilla/chronicle/pulls/53,53,fix(build): changing route to /assets/,"Fixes #51 
",pdehaan,557895,2014-12-22T22:24:47Z,CONTRIBUTOR,True,4,4,2,find everything you've ever found,JavaScript,6b3421489873606f8fa4e40d0198f6153bf53edf,fix(build): changing route to /assets/
152,https://api.github.com/repos/mozilla/chronicle/pulls/52,52,docs(build): Clarified the grunt build task in creating /dist/ dir,,pdehaan,557895,2014-12-22T21:21:30Z,CONTRIBUTOR,True,5,4,2,find everything you've ever found,JavaScript,3fedff75afb2f22b61450e54e321300493faebe9,docs(build): Clarified the grunt build task in creating /dist/ dir
153,https://api.github.com/repos/mozilla/chronicle/pulls/50,50,chore(grunt): Added grunt build task which replaces grunt dist task,"Now running `grunt build` lints the code, deletes the /dist/ directory, copies over content from the /app/ directory, and compiles/copies the necessary CSS.
",pdehaan,557895,2014-12-22T19:55:42Z,CONTRIBUTOR,True,27,6,4,find everything you've ever found,JavaScript,2c5625bf4333906b07094e285cc362bb6ec1b851,chore(grunt): Added grunt build task which replaces grunt dist task
154,https://api.github.com/repos/mozilla/chronicle/pulls/49,49,fix(config): Tweaking server session duration to be an int,"Fixes #48
",pdehaan,557895,2014-12-22T19:24:02Z,CONTRIBUTOR,True,49,19,3,find everything you've ever found,JavaScript,9b8b6b909e07c324d1b73d1e3c242e2bb9ce88d7,fix(config): Tweaking server session duration to be an int
155,https://api.github.com/repos/mozilla/chronicle/pulls/42,42,feat(server): enable fxa oauth login + save to db,"Fixes #40, almost-kinda-fixes #24.

Changes of note:
- I'm able to do the complete oauth-driven login flow locally \o/
- Remove jscs rule requiring a space after `function`
- Serve static bits from `/dist`, not `/app`
- Add mozlog logging: heka compatible in prod, yet readable for local dev

Particularly ugly/dangerous TODOs:
- Added DB creation and DB client, (barely) functional yet so ugly
- Make outgoing HTTP requests from inside the route handler: reverse proxy
  needed
- Not yet using a nonce to avoid replay attacks, because hapi sessions
  are so &#$%! unnecessarily complicated

---

r? @pdehaan && @seanmonstar && @nchapman 

I've got a super super hacky (yet working) front-end in my [issue-24-WIP](https://github.com/6a68/chronicle/tree/issue-24-WIP) branch; next I'll try to get a client-side commit together that works well enough with the code already in the repo.
",jaredhirsch,96396,2014-12-19T20:21:37Z,MEMBER,True,735,2000,11,find everything you've ever found,JavaScript,6f6ecb3882b3ecfa42acd94a791e3f4441499794,"feat(server): enable fxa oauth login + save to db

Fixes #40, almost-kinda-fixes #24.

Changes of note:
  * I'm able to do the complete oauth-driven login flow locally \o/
  * Remove jscs rule requiring a space after `function`
  * Remove 'unused' jshint rule
  * Serve static bits from `/dist`, not `/app`
  * Add mozlog logging: heka compatible in prod, yet readable for local dev

Particularly ugly/dangerous TODOs (and tracking issue numbers):
  * Added DB creation and DB client, (barely) functional yet so ugly (#24)
  * Currently making outgoing HTTP requests from inside the route handler;
    we should use a reverse proxy worker/job instead (#46)
  * Not yet using a nonce in the oauth flow (#45)
  * Not yet validating inputs w/Joi (#43)"
156,https://api.github.com/repos/mozilla/chronicle/pulls/37,37,chore(styles): configure sass directories,"Fixes #35 
",johngruen,3323249,2014-12-18T20:52:19Z,CONTRIBUTOR,True,144,18,15,find everything you've ever found,JavaScript,228e3d31b70f5e922561d1fd9d952dcced035ea5,chore(styles): configure sass directories
157,https://api.github.com/repos/mozilla/chronicle/pulls/36,36,chore(build): Add .travis.yml file,"Fixes #16
",pdehaan,557895,2014-12-18T00:49:08Z,CONTRIBUTOR,True,16,0,1,find everything you've ever found,JavaScript,9f1bc5fdcaf9cf9914518ee1815c807be7c23a93,chore(build): Add .travis.yml file
158,https://api.github.com/repos/mozilla/chronicle/pulls/36,36,chore(build): Add .travis.yml file,"Fixes #16
",pdehaan,557895,2014-12-18T00:49:08Z,CONTRIBUTOR,True,16,0,1,find everything you've ever found,JavaScript,6bdc312155e1e0c7fdd2e9c234342ffb61a5e1db,install bower and grunt-cli on travis
159,https://api.github.com/repos/mozilla/chronicle/pulls/34,34,chore(build): Adding Sass to Grunt pipeline,"Adding Sass and Autoprefixer into the mix. This also means we need a dist/ directory, so things are about to get ""weird"".

Took a rough stab at documenting the available Grunt tasks, although some are subject to change once we get minification and other things involved.
",pdehaan,557895,2014-12-17T22:33:37Z,CONTRIBUTOR,True,1595,230,12,find everything you've ever found,JavaScript,5b846a7f6bb06be6b35a92c188fb8b396d2d6a39,"fixing merge conflicts

Updating README file w/ Grunt task details

updating shrinkwrap script to run validator"
160,https://api.github.com/repos/mozilla/chronicle/pulls/34,34,chore(build): Adding Sass to Grunt pipeline,"Adding Sass and Autoprefixer into the mix. This also means we need a dist/ directory, so things are about to get ""weird"".

Took a rough stab at documenting the available Grunt tasks, although some are subject to change once we get minification and other things involved.
",pdehaan,557895,2014-12-17T22:33:37Z,CONTRIBUTOR,True,1595,230,12,find everything you've ever found,JavaScript,1bb45c4934ca65ec35881ac39d6559b5edabb3b5,chore(build): Adding Sass to Grunt pipeline
161,https://api.github.com/repos/mozilla/chronicle/pulls/32,32,chore(build): Initial stab at Grunt skeleton,"A case study in how _not_ to do pull requests...

But we should now have a shared precommit-hook which runs lint tasks whenever you try and commit some code.
JSHint and JSCS rules are up for negotiation.

FIxes #31
Fixes #17 
FIxes #13 
Fixes #12 
Fixes #11 
",pdehaan,557895,2014-12-17T19:14:39Z,CONTRIBUTOR,True,1510,10,17,find everything you've ever found,JavaScript,7360cd9055238ef70290310f4caaa3e481e5c905,merge conflicts
162,https://api.github.com/repos/mozilla/chronicle/pulls/32,32,chore(build): Initial stab at Grunt skeleton,"A case study in how _not_ to do pull requests...

But we should now have a shared precommit-hook which runs lint tasks whenever you try and commit some code.
JSHint and JSCS rules are up for negotiation.

FIxes #31
Fixes #17 
FIxes #13 
Fixes #12 
Fixes #11 
",pdehaan,557895,2014-12-17T19:14:39Z,CONTRIBUTOR,True,1510,10,17,find everything you've ever found,JavaScript,244546f1f061af534c40ea4cda38049052ab0f59,chore(build): Initial stab at Grunt skeleton
163,https://api.github.com/repos/mozilla/chronicle/pulls/32,32,chore(build): Initial stab at Grunt skeleton,"A case study in how _not_ to do pull requests...

But we should now have a shared precommit-hook which runs lint tasks whenever you try and commit some code.
JSHint and JSCS rules are up for negotiation.

FIxes #31
Fixes #17 
FIxes #13 
Fixes #12 
Fixes #11 
",pdehaan,557895,2014-12-17T19:14:39Z,CONTRIBUTOR,True,1510,10,17,find everything you've ever found,JavaScript,74144883d03b75c4380da8382f43113937d18c4a,Tweaking jshintrc per 6a68 feedback
164,https://api.github.com/repos/mozilla/chronicle/pulls/32,32,chore(build): Initial stab at Grunt skeleton,"A case study in how _not_ to do pull requests...

But we should now have a shared precommit-hook which runs lint tasks whenever you try and commit some code.
JSHint and JSCS rules are up for negotiation.

FIxes #31
Fixes #17 
FIxes #13 
Fixes #12 
Fixes #11 
",pdehaan,557895,2014-12-17T19:14:39Z,CONTRIBUTOR,True,1510,10,17,find everything you've ever found,JavaScript,b6b2fc133e694e9461d8586f0d1838e5cb8b751c,"Removing changelog, for now"
165,https://api.github.com/repos/mozilla/chronicle/pulls/30,30,chore(docs): add front-end instructions to README,"@nchapman r? did I miss anything?
",jaredhirsch,96396,2014-12-16T19:02:41Z,MEMBER,True,9,3,2,find everything you've ever found,JavaScript,93c76c324be7102c950dfdb76e361d631e90d3e6,chore(docs): add front-end instructions to README
166,https://api.github.com/repos/mozilla/chronicle/pulls/27,27,feat(client): Initial client commit.,"This is devoid of all grunt awesomeness because I realized that's an enhancement and not a core feature to get dev up and running. Please critique throughly (especially the server side changes and config).
",nchapman,3095,2014-12-16T05:54:37Z,CONTRIBUTOR,True,384,3,13,find everything you've ever found,JavaScript,40cd7cc38a026cb80ef46561ed7309912e7542e3,feat(client): Initial client commit.
167,https://api.github.com/repos/mozilla/chronicle/pulls/19,19,fix(docs): add AUTHORS,,vladikoff,128755,2014-12-15T18:14:32Z,CONTRIBUTOR,True,7,1,2,find everything you've ever found,JavaScript,e8deeea1f9a0474a418d8e9e9cb07cd7f8c8c94d,fix(docs): add AUTHORS
168,https://api.github.com/repos/mozilla/chronicle/pulls/18,18,feat(server): define config schema using convict,"Closes #3
",jaredhirsch,96396,2014-12-13T21:09:45Z,MEMBER,True,106,3,4,find everything you've ever found,JavaScript,12816f1d39f77f82e93be4ae79904785ef6e7fe8,"feat(server): define config schema using convict

Closes #3"
169,https://api.github.com/repos/mozilla/chronicle/pulls/15,15,"chore: Basics for CHANGELOG.md, CONTRIBUTORS, and npm scripts for outdat...","...ed and shrinkwrap

Fixes #11 
Fixes #13 
",pdehaan,557895,2014-12-12T22:15:45Z,CONTRIBUTOR,False,77,16,5,find everything you've ever found,JavaScript,4cefdffc353ed20f1b088030e285f242e0d1232e,"chore: Basics for CHANGELOG.md, CONTRIBUTORS, and npm scripts for outdated and shrinkwrap

git is hard"
170,https://api.github.com/repos/mozilla/chronicle/pulls/15,15,"chore: Basics for CHANGELOG.md, CONTRIBUTORS, and npm scripts for outdat...","...ed and shrinkwrap

Fixes #11 
Fixes #13 
",pdehaan,557895,2014-12-12T22:15:45Z,CONTRIBUTOR,False,77,16,5,find everything you've ever found,JavaScript,7764179849453a25a727384cb5d7665887b65af5,fixing merge conflicts
171,https://api.github.com/repos/mozilla/chronicle/pulls/15,15,"chore: Basics for CHANGELOG.md, CONTRIBUTORS, and npm scripts for outdat...","...ed and shrinkwrap

Fixes #11 
Fixes #13 
",pdehaan,557895,2014-12-12T22:15:45Z,CONTRIBUTOR,False,77,16,5,find everything you've ever found,JavaScript,945c95c89433e2064f4e1eef8ac17a413cf95e0e,Fixing grunt contributors
172,https://api.github.com/repos/mozilla/chronicle/pulls/15,15,"chore: Basics for CHANGELOG.md, CONTRIBUTORS, and npm scripts for outdat...","...ed and shrinkwrap

Fixes #11 
Fixes #13 
",pdehaan,557895,2014-12-12T22:15:45Z,CONTRIBUTOR,False,77,16,5,find everything you've ever found,JavaScript,c22e62659798cb5d399b51ae0d03ca55e01bf0b1,Updating contributors generating script
173,https://api.github.com/repos/mozilla/chronicle/pulls/15,15,"chore: Basics for CHANGELOG.md, CONTRIBUTORS, and npm scripts for outdat...","...ed and shrinkwrap

Fixes #11 
Fixes #13 
",pdehaan,557895,2014-12-12T22:15:45Z,CONTRIBUTOR,False,77,16,5,find everything you've ever found,JavaScript,c77d27a1473fae245c2b5775097d94c84c7270e8,Adding grunt-copyright task
174,https://api.github.com/repos/mozilla/chronicle/pulls/14,14,docs: Adding LICENSE file,,pdehaan,557895,2014-12-12T21:37:58Z,CONTRIBUTOR,True,373,0,1,find everything you've ever found,JavaScript,f540cb6be308d34309a735b0916e66c59ddd3b3a,docs: Adding LICENSE file
175,https://api.github.com/repos/mozilla/chronicle/pulls/10,10,docs(contributing): add CONTRIBUTING.md,"closes #9 
",johngruen,3323249,2014-12-12T20:53:50Z,CONTRIBUTOR,True,79,0,1,find everything you've ever found,JavaScript,dbc4b1d84e4a0de242dd911a3815ed2e4faec231,docs(contributing): add CONTRIBUTING.md
176,https://api.github.com/repos/mozilla/chronicle/pulls/2,2,Initial server commit,"♬  hapi-ness is a warm, yes it is, gu-u-u-u-nn ♪ ♫

@nchapman r?
",jaredhirsch,96396,2014-12-11T22:04:48Z,MEMBER,True,102,0,5,find everything you've ever found,JavaScript,fd4080c72e540e39155aa2d47a385a7d8f990b9c,"Initial server commit

♬  hapi-ness is a warm, yes it is, gu-u-u-u-nn ♪ ♫"
