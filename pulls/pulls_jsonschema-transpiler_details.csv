,pullid,pulls_number,pulltitle,pullsbody,pullsuserlogin,pullsuserid,pullauthordate,author_association,merged_status,stats_addns,stats_delns,stats_changed_files,pull_repo_desc,pull_repo_lang,pull_commit_sha,pull_commit_message
0,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/100,100,Bump version to 1.9.0,"Forgot to add this as part of #98, looks like I disabled pushing to main for admins in the repo.",acmiyaguchi,3304040,2021-03-30T17:42:12Z,COLLABORATOR,False,124,126,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,6024dfa4cf359c0d6d6a5ee65f596edd96f6d3ce,Bump version to 1.9.0
1,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/99,99,Bump version,I forgot to bump the version previously,scholtzan,1239705,2021-03-30T17:05:01Z,CONTRIBUTOR,True,2,2,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,4c90baaa1386fdcad632814fc119551925d8d678,Bump version
2,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/98,98,Truncate descriptions,Fixes https://github.com/mozilla/mozilla-schema-generator/issues/188,scholtzan,1239705,2021-03-30T16:26:50Z,CONTRIBUTOR,True,8,1,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,c3ebae3091c0adaaa83d690ab32f23b0b99c2d28,Truncate descriptions
3,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/97,97,Add web application that runs the transpiler in wasm,,acmiyaguchi,3304040,2021-02-26T18:05:27Z,COLLABORATOR,False,2759,184,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,80850b3ffff2e3f6a5b6993737379c00bdd9fb2f,Add template for svelte app
4,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/97,97,Add web application that runs the transpiler in wasm,,acmiyaguchi,3304040,2021-02-26T18:05:27Z,COLLABORATOR,False,2759,184,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,712a5724db8533d51145e9ef1941c93988c0fd2f,Add rust plugin for rollup
5,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/97,97,Add web application that runs the transpiler in wasm,,acmiyaguchi,3304040,2021-02-26T18:05:27Z,COLLABORATOR,False,2759,184,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,fd3806c97a34f91f70c92a63284a0bf862f0a767,Add wasm-bindgen and attempt to build package with wasm target
6,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/97,97,Add web application that runs the transpiler in wasm,,acmiyaguchi,3304040,2021-02-26T18:05:27Z,COLLABORATOR,False,2759,184,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,114c04a487df4f8421fb5efe28dc072269027572,Add initial docker files
7,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/97,97,Add web application that runs the transpiler in wasm,,acmiyaguchi,3304040,2021-02-26T18:05:27Z,COLLABORATOR,False,2759,184,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,a971825ac99e1fb3c29c19a2235d285037e6a2ec,Replace oniguruma with pure rust snakecase impl
8,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/97,97,Add web application that runs the transpiler in wasm,,acmiyaguchi,3304040,2021-02-26T18:05:27Z,COLLABORATOR,False,2759,184,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,31ce78176b77f886df5154288adeb63d39289025,Add functional (but simplistic) demo page
9,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/96,96,Add repository link to crates.io and update README,It was brought to my attention that a repository link in crates.io would be nice to have. I also updated the README and bumped the patch version. ,acmiyaguchi,3304040,2020-10-16T19:00:26Z,COLLABORATOR,True,20,10,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,0ddfcdb1bf8f555bd2f0a226d42032e19ef511ae,Add link to the repository in cargo definition
10,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/96,96,Add repository link to crates.io and update README,It was brought to my attention that a repository link in crates.io would be nice to have. I also updated the README and bumped the patch version. ,acmiyaguchi,3304040,2020-10-16T19:00:26Z,COLLABORATOR,True,20,10,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,67d7146ff7659bfdf891c3a0a9a39d9a0204208b,Update README
11,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/96,96,Add repository link to crates.io and update README,It was brought to my attention that a repository link in crates.io would be nice to have. I also updated the README and bumped the patch version. ,acmiyaguchi,3304040,2020-10-16T19:00:26Z,COLLABORATOR,True,20,10,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,a252e48816057efc9d97a8f3a11180e7c49ad46d,Bump version to 1.8.1
12,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/93,93,"Propagate ""description"" from json schema to BigQuery schema","This addresses https://github.com/mozilla/jsonschema-transpiler/issues/58.

I also fixed some formatting and clippy issues in the process. For now I extended existing tests but I could also create separate test cases for testing the description propagation if wanted.",scholtzan,1239705,2020-01-11T00:20:51Z,CONTRIBUTOR,True,458,40,11,Compile JSON Schema into Avro and BigQuery schemas,Rust,6452cf4113fa2259cbc7b4058f78840720e75d07,Clippy and formatting fixes
13,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/93,93,"Propagate ""description"" from json schema to BigQuery schema","This addresses https://github.com/mozilla/jsonschema-transpiler/issues/58.

I also fixed some formatting and clippy issues in the process. For now I extended existing tests but I could also create separate test cases for testing the description propagation if wanted.",scholtzan,1239705,2020-01-11T00:20:51Z,CONTRIBUTOR,True,458,40,11,Compile JSON Schema into Avro and BigQuery schemas,Rust,fee678e7dc44fa68480991d421a98dc8d221d8a9,Propagate description from json schema to BigQuery schema
14,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/93,93,"Propagate ""description"" from json schema to BigQuery schema","This addresses https://github.com/mozilla/jsonschema-transpiler/issues/58.

I also fixed some formatting and clippy issues in the process. For now I extended existing tests but I could also create separate test cases for testing the description propagation if wanted.",scholtzan,1239705,2020-01-11T00:20:51Z,CONTRIBUTOR,True,458,40,11,Compile JSON Schema into Avro and BigQuery schemas,Rust,52941fb22b25146f9e4e977b323ad634a8fa5acb,Parse title from schemas and fix AST generation with description and title
15,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/92,92,Bug 1595899 - Add option to generate maps without value,for payload.addonDetails.XPI in bigquery,relud,433717,2019-11-18T21:55:40Z,COLLABORATOR,True,57,9,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,1e9476e0a1772702378715c49572f612931aa904,"Add option to generate maps without value

for payload.addonDetails.XPI in bigquery"
16,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/92,92,Bug 1595899 - Add option to generate maps without value,for payload.addonDetails.XPI in bigquery,relud,433717,2019-11-18T21:55:40Z,COLLABORATOR,True,57,9,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,eb696aaf0a5cfd53a568e7ab6e287535ea1f1af5,Bump to 1.7.0
17,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,4285c53c0f516fb50640658b62554da11f3c54ca,Add maplit to package
18,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,a10e54c872c1c5d67891d4cc1d8f8468cc8a2444,Add tests for nested arrays
19,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,72e5796da118914c7b9cdb6cdebe72ec94d43336,Add general approach to nested arrays
20,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,b5721cbbf03dbaf82867d2f36c7e0f0413b36fce,Use list intead of item for naming nested fields in avro
21,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,9f685b872e6c3a525c2d3bebcd6dedd459c081b6,Add vestigial code for expanding arrays in ast
22,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,ca59e6d47954a0d137e171bbff98e9b6352ea562,Remove ast::Tag::expand_nested_array
23,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,a029d9b8428d28233fae515592102cf8c4595113,Fix nested arrays in json by adding a record with a single list field
24,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,335b4ee577d56884449bb86e5ea9e1c38d72fb4f,Fix nits from clippy
25,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,d180f3fde37e61a0d514f3171b53a77e9f283d12,Add script for verifying nested lists
26,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,ee0c4e4c2a3433ad868d5e361c45897681a497f0,Fix nested array issue with avro
27,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,603ff6a815750d994bfaaaa5422297e1efb0c08d,Update avro decoder to expand nested list items
28,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,4b53c6852a8764b2ed1f0274adbb9f7f5ab93d11,Nest record in another layer
29,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/90,90,Nested list,"Nested lists are not handled correctly.

```bash
$ echo '{
            ""type"": ""array"",
            ""items"": {
                ""type"": ""array"",
                ""items"": {
                    ""type"": ""array"",
                    ""items"": [
                        {""type"": ""integer""}
                    ],
                    ""additionalItems"": false
                }
            }
        }' | jsonschema-transpiler --type bigquery --tuple-struct
```
results in 

```json
[
  {
    ""fields"": [
      {
        ""mode"": ""REQUIRED"",
        ""name"": ""f0_"",
        ""type"": ""INT64""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

This PR fixes this so there is an intermediate layer that can be used for unnesting.

```json
[
  {
    ""fields"": [
      {
        ""fields"": [
          {
            ""mode"": ""REQUIRED"",
            ""name"": ""f0_"",
            ""type"": ""INT64""
          }
        ],
        ""mode"": ""REPEATED"",
        ""name"": ""list"",
        ""type"": ""RECORD""
      }
    ],
    ""mode"": ""REPEATED"",
    ""name"": ""root"",
    ""type"": ""RECORD""
  }
]
```

See this gist for the result of the verification script: https://gist.github.com/acmiyaguchi/619b113f0b536480919ecf90a4028036. This lines up with the experience with the third party modules and untrusted modules pings, which are likely the only pings with nested arrays.
",acmiyaguchi,3304040,2019-09-30T15:56:17Z,COLLABORATOR,True,436,24,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,27b0da53e398b375bbcb994ba48500b3b20d2b24,Fix excess nesting for nested lists in avro schemas
30,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,4f58f87f73d84dd93f0b8e97c16c38162eb7c2ce,Add failing tests for tuple validation
31,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,7e202266fa3a9bbdb8e106f188c4ee1f8c6c1660,Add extra properties into jsonschema::Array type
32,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,c6cf977dd9025a1e098880fb5d6da568a62eb781,Update type_into_ast and atom_into_ast to return Result
33,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,c8cbf58a97ef0307976e83a8878a4b466c7d8ea5,Add --tuple-struct option for enabling treatment of tuples as structs
34,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,2189311a9ec32ec2d065fec8eaf31067e12b0e93,"Add implementation for avro tuples

This may be a less than ideal implementation, because it assumes that the AST
representation is already in the correct format. This doesn't take into account
the context passed from the command-line -- however, this interface is only
used from the main translate functions in the library."
35,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,a25163d079dc4a3a8486b677cb8805b1e80f0db5,Move tests into a rust file for context
36,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,6769effc5217909f47d66c0aecc04f0d8c99bae4,Add tuple support for BigQuery
37,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,ffe9506f7b94b961c89235142bea31ed205248df,Update mps-download-schemas to point to master
38,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,6a254ed9fb350394656981c38bd3d69c7693e603,Remove short version of --tuple-struct due to conflicts
39,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,38382926064b4c293e50bd3473f50f23448a5ea3,Add script for testing --tuple-struct option
40,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,c267d1b08174b6a61b8ba4221e26966ac649ac3c,Allow static length tuples
41,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,59120dfeee7684214affd8ce4d1d1a040504450f,Add support for optional tuple fields when `--force-nullable` not set
42,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,4ed2dddca20dd23cac54f207282099491d50f717,Use `f{index}_` convention
43,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,9fcb42a12593c26b26ba0b03735b305ee4f93922,Update scripts
44,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,d145f58d82fd8df67329b357e7ee8e4f0a47cfc5,Handle tests for objects inside of tuples
45,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/89,89,Add support for tuples in the form of anonymous structs,"This should fix #38 by adding support for anonymous structs via tuple validation in the form of a `--tuple-struct` flag. This requires a custom JSON decoder that maps JSON lists into structs given the anonymous struct naming convention (`f0_`, `f1_`, ...`f{n}_`).

I've tested this against the latest version of mozilla-pipeline-schemas. There are no regressions between this PR and the latest tag (`v1.4.1`) when leaving off `--tuple-struct`. Also, cursory glance of the diff seems show the behavior described in #36. 

See https://gist.github.com/acmiyaguchi/b1ee9a93f17f605995453251b7e34316",acmiyaguchi,3304040,2019-09-18T00:03:15Z,COLLABORATOR,True,711,46,16,Compile JSON Schema into Avro and BigQuery schemas,Rust,204bb7d22ce18e29948dcd4beb0b7521b51c3216,Bump version to 1.5.0
46,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/86,86,Remove redundant code path,"> I don't understand why this is setting `self.nullable = true` both here and on 537

_Originally posted by @relud in https://github.com/mozilla/jsonschema-transpiler/pull/85_

Thanks for pushing on this one, it's certainly a redundant path. I should have spent more time looking it over. ",acmiyaguchi,3304040,2019-08-20T00:15:25Z,COLLABORATOR,True,3,8,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,447e82645c2cf35b2d0842f70b387e579ec821fc,Remove redundant code artifact
47,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/86,86,Remove redundant code path,"> I don't understand why this is setting `self.nullable = true` both here and on 537

_Originally posted by @relud in https://github.com/mozilla/jsonschema-transpiler/pull/85_

Thanks for pushing on this one, it's certainly a redundant path. I should have spent more time looking it over. ",acmiyaguchi,3304040,2019-08-20T00:15:25Z,COLLABORATOR,True,3,8,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,de6be5650eccd469aee639bce2340252209dca42,Bump version to 1.4.1
48,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/85,85,Fix #84 - Add `--force-nullable` option to make all fields nullable,"This adds a flag for making all output fields nullable. In addition to the unit tests, I've also manually tested against mozilla-pipeline-schemas.

```bash
$ scripts/mps-download-schemas.sh

# generate flat folder with generated schemas
$ ./scripts/mps-generate-schemas.sh control --type bigquery
$ ./scripts/mps-generate-schemas.sh nullable --type bigquery --enforce-nullable
$ git checkout v1.3.0
$ ./scripts/mps-generate-schemas.sh old --type bigquery

# First check -- check that REQUIRED are being transformed into NULLABLE
# see https://gist.github.com/acmiyaguchi/42d30c313baf0a7aaa7aca653f7d54d0
$ diff control nullable

# Second check -- number of ocurrances of REQUIRED
$ grep -rnw control -e 'REQUIRED' | wc
    2935    8805  245939
$ grep -rnw nullable -e 'REQUIRED' | wc
       0       0       0
$ grep -rnw old -e 'REQUIRED' | wc
    2935    8805  234199

# Third check -- control and old are the same
$ diff control old | wc
       0       0       0

# Fourth check -- control and old are the same with respect to nullable
$ diff <(diff control nullable) <(diff old nullable) | grep NULLABLE | wc
       0       0       0
```
",acmiyaguchi,3304040,2019-08-19T21:27:09Z,COLLABORATOR,True,369,13,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,5bf92b520aa2426cb2d8481a5bc421dc72ff3e0e,Fix #84 - Add `--enforce-nullable` option to make all fields nullable
49,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/85,85,Fix #84 - Add `--force-nullable` option to make all fields nullable,"This adds a flag for making all output fields nullable. In addition to the unit tests, I've also manually tested against mozilla-pipeline-schemas.

```bash
$ scripts/mps-download-schemas.sh

# generate flat folder with generated schemas
$ ./scripts/mps-generate-schemas.sh control --type bigquery
$ ./scripts/mps-generate-schemas.sh nullable --type bigquery --enforce-nullable
$ git checkout v1.3.0
$ ./scripts/mps-generate-schemas.sh old --type bigquery

# First check -- check that REQUIRED are being transformed into NULLABLE
# see https://gist.github.com/acmiyaguchi/42d30c313baf0a7aaa7aca653f7d54d0
$ diff control nullable

# Second check -- number of ocurrances of REQUIRED
$ grep -rnw control -e 'REQUIRED' | wc
    2935    8805  245939
$ grep -rnw nullable -e 'REQUIRED' | wc
       0       0       0
$ grep -rnw old -e 'REQUIRED' | wc
    2935    8805  234199

# Third check -- control and old are the same
$ diff control old | wc
       0       0       0

# Fourth check -- control and old are the same with respect to nullable
$ diff <(diff control nullable) <(diff old nullable) | grep NULLABLE | wc
       0       0       0
```
",acmiyaguchi,3304040,2019-08-19T21:27:09Z,COLLABORATOR,True,369,13,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,13646385b67d47b0968355d8b2101cefb86b44af,Rename enforce-nullable to force-nullable
50,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/85,85,Fix #84 - Add `--force-nullable` option to make all fields nullable,"This adds a flag for making all output fields nullable. In addition to the unit tests, I've also manually tested against mozilla-pipeline-schemas.

```bash
$ scripts/mps-download-schemas.sh

# generate flat folder with generated schemas
$ ./scripts/mps-generate-schemas.sh control --type bigquery
$ ./scripts/mps-generate-schemas.sh nullable --type bigquery --enforce-nullable
$ git checkout v1.3.0
$ ./scripts/mps-generate-schemas.sh old --type bigquery

# First check -- check that REQUIRED are being transformed into NULLABLE
# see https://gist.github.com/acmiyaguchi/42d30c313baf0a7aaa7aca653f7d54d0
$ diff control nullable

# Second check -- number of ocurrances of REQUIRED
$ grep -rnw control -e 'REQUIRED' | wc
    2935    8805  245939
$ grep -rnw nullable -e 'REQUIRED' | wc
       0       0       0
$ grep -rnw old -e 'REQUIRED' | wc
    2935    8805  234199

# Third check -- control and old are the same
$ diff control old | wc
       0       0       0

# Fourth check -- control and old are the same with respect to nullable
$ diff <(diff control nullable) <(diff old nullable) | grep NULLABLE | wc
       0       0       0
```
",acmiyaguchi,3304040,2019-08-19T21:27:09Z,COLLABORATOR,True,369,13,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,adbe459c4ba1c4cc866a20953e5a96f145d360c3,Rename test file to force_nullable
51,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/85,85,Fix #84 - Add `--force-nullable` option to make all fields nullable,"This adds a flag for making all output fields nullable. In addition to the unit tests, I've also manually tested against mozilla-pipeline-schemas.

```bash
$ scripts/mps-download-schemas.sh

# generate flat folder with generated schemas
$ ./scripts/mps-generate-schemas.sh control --type bigquery
$ ./scripts/mps-generate-schemas.sh nullable --type bigquery --enforce-nullable
$ git checkout v1.3.0
$ ./scripts/mps-generate-schemas.sh old --type bigquery

# First check -- check that REQUIRED are being transformed into NULLABLE
# see https://gist.github.com/acmiyaguchi/42d30c313baf0a7aaa7aca653f7d54d0
$ diff control nullable

# Second check -- number of ocurrances of REQUIRED
$ grep -rnw control -e 'REQUIRED' | wc
    2935    8805  245939
$ grep -rnw nullable -e 'REQUIRED' | wc
       0       0       0
$ grep -rnw old -e 'REQUIRED' | wc
    2935    8805  234199

# Third check -- control and old are the same
$ diff control old | wc
       0       0       0

# Fourth check -- control and old are the same with respect to nullable
$ diff <(diff control nullable) <(diff old nullable) | grep NULLABLE | wc
       0       0       0
```
",acmiyaguchi,3304040,2019-08-19T21:27:09Z,COLLABORATOR,True,369,13,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,17793aba0dbb1f30b55a6c87d15025029bc187c8,Bump version v1.4.0
52,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/82,82,Support bytes as a data type,fixes #81 ,relud,433717,2019-08-05T21:23:56Z,COLLABORATOR,True,130,2,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,2dfa521f11dddb2b985e7d9009a0bf78d9b3f9ef,Support bytes as a data type
53,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/82,82,Support bytes as a data type,fixes #81 ,relud,433717,2019-08-05T21:23:56Z,COLLABORATOR,True,130,2,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,1a34b3d92e54dff533bb80ac146e9f683eb5f84c,Bump version to v1.3.0
54,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,3487666ee4405f814a4f79e4339d9654efdc010c,Reorder functions and remove excess clones
55,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,447d5612a231d8f24970724a10ed3987d7d1ba92,Rename normalize_properties and refactor recurse_infer_name
56,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,c577171dfedd0d83d4748b5f6b767cbfd5eafeb8,Add `--normalize-case` and implement Default for Context
57,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,80a25051799b4bd69ba6b4d9be2dcf1fd341d220,Add failing test for normalizing casing
58,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,2333ccc88bcb1f8416b6108a95740cc9b2d1c2c5,Add normalize_case to function definitions
59,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,a8a5e5a6220ec90a415c892e229c8fed23400fe5,Fix broken tests and sort property names properly
60,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,95024dc77e07051b71bb8a6672051a5ad5abe98d,Use heck to snake_case column names
61,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,cae22801863efa8f4d151b7cefff2ef55f47ee50,Add a new test-case asserting names that start with numbers
62,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,ac64a47b54364cf3e8903dbd363a1aad103c88e9,Rename prefix_numeric to normalize_numeric_prefix
63,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,5132a170f0041f657c532b54789fa60277b2fdfb,Update context in main
64,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,5567a02ec12e8426d55642d69de00b46fe1951f1,Update scripts for generating a diff
65,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,708689f4ea465f4ba56d348041dcc7f1a70f2ff3,Add test cases for casing and move cases for translating schemas
66,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,1ef4ced10239098f51331b24f122294ad74086b3,Expose snake casing as a public module for integration testing
67,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,a50eda3372367a0081a5bbdc6d6405039604b199,Replace regex with oniguruma; implement to_snake_case with regexes
68,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,4a376fd1a657a75162049ae7c97cbf367b6a2532,Check-in latest implementation of casing; use static_lazy
69,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,c6f7e6f98a6e022f636e36d62399c60acc02d669,Add comment for test case when normalizing property names
70,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,b8c9bb6e364970fdf2d62dee721f15de25ff697f,Add docstring to to_snake_case
71,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,d307cc37725b3bd05d305d9a495c45781f91375c,Update documentation to be more specific
72,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/79,79,Add `--normalize-case` option for snake_casing column names,"This PR fixes #77 by adding a new option to `snake_case` all column names in a schema. This should be used by adding a `--normalize-case` flag to the command. By default, this option is turned off.

I've chosen [heck](https://crates.io/crates/heck) as the casing library, since it seems to have the largest number of active users. It uses the [`unicode_segmentation`](https://crates.io/crates/unicode-segmentation) crate to find word boundaries and performs `snake_casing` consistently across mixed casing. 

I've refactored the code to remove extra clones and to make the order of the functions flow better when reading top-down. I also added a few comments here and there. 

",acmiyaguchi,3304040,2019-07-09T00:32:25Z,COLLABORATOR,True,1213,244,25,Compile JSON Schema into Avro and BigQuery schemas,Rust,bc9c0ec4aa5f515f09a4f909ab071762ad975616,Bump version to 1.2.0
73,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,017594ea8cd4965182982a82f2afa74a914bb31d,Add env_logger initialization to application
74,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,8999cbbace64baaa88de5c8123ddf32e20f72ee2,Add an initial trait for translating between schema formats
75,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,11828b535379fbc1e1cf484f9134e44bca852127,Implement Translate trait for ast::Tag
76,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,8b4a43bfe817afd84cf20119f7f4c1219f426169,"Add Translate trait to the bigquery module

This is not using the Result type in any meaningful way."
77,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,58d3b4d26ae41c217aed9f27a02fe2421967ef96,Add Translate trait for Avro
78,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,a78c3fff996e0c85b19d9574034e297948a4faa4,Add context and resolution method to CLI and library
79,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,6fc68f6904a2b4e1050b7ba613efac2174f6a75f,Update translate functions with context
80,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,c007772c2628a9689d07bb7c34698cd4fd86bf02,Add context inside of tests
81,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,f1d5d51967f8d5291f6159a1e1c253be34bed01d,Add WIP support for resolve methods
82,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,4ea16d8980ae9106ca353278ac3e10498eb5ab7f,Add tests for testing error resolution methods
83,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,56493e96a2bd339911abaa821ffcbfdabebbf19d,Update bigquery for resolving errors
84,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,2ecc87a468793285ade1a6c7562594c4a08e4170,Refactor bigquery tests to use a common transform
85,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,995faceb5409effc5bc25fc777b4ee401fba3e11,Fix BigQuery unit-tests to use casting behavior by default
86,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,dcb9872d4898b0eb84e29221b552af29107b8932,Update tests to cast on incompatible schemas
87,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,2e02ceaf44f43f40a792e9410b5053ca1d2c02f6,Update generated tests with resolve_method in context
88,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,7d027c3b8c2435a61b99fbd88e1986256076a303,Update autogenerated tests to format correctly
89,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,5dc9375dc2beba721f0574a36a6cd82dbb2d174d,Update avro to cast using the context variables
90,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,1bd480d1f48dd5bb88015242eece2866618fd47a,Address issues found during `cargo clippy`
91,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,928af29d0571dc3802e0d9da6870b51b720783d6,Use hashmap notation in tests (clippy)
92,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,24c135a04cc5361c39ad4c982ffffcefd15571a1,Rename Translate to TranslateFrom
93,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,819817a5fbd1c2d2ec5e3f3ace50ebdfe68c907e,Refactor tests for jsonschema
94,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,951daeff9887fa06cbbca98d661f13e83b2ff77c,Make context required
95,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,c2287c2c3a2c5b31f2c0fde5f8dcc73585e5ebb9,Categorize each test as compatible and test for panics
96,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,270e4d31a57eef224f8c1bec8acbb3a808a661ed,Add consistent error messages in panic cases
97,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,e87ed38b175419c4ab7520ded7e54304691c7d8a,Refactor error resolving tests for one test per case
98,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,5d04c7adc5707208bdd211457f0c4534b9f5c391,Use `should_panic` instead of `catch_unwind`
99,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,4fa5195340112e9c291dc6b8a0b84ae52c5632de,"Add docstrings for Context, ResolveMethod, and TranslateFrom"
100,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,aed9fea9cc68713c79f78fe9eca977f06b65f76d,Reorder and include tag names in panic
101,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,c056849c37b7a0de8d012f55071de198e8344bb6,Refactor error handling
102,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/76,76,Add a flag to drop fields instead of casting,"This PR should (mostly) fix #75.

The approach in this pull-request is as follows:

* [x] Create a new trait that replaces `From` and `Into` with something akin to `TryFrom` and `TryInto` which includes a Result type. This new trait is named `Translate`. 
* [x] Add a new Context structure for passing flags from the command line into the `Translate` trait (requires modifying the trait definition).
* [x] Modify bigquery.rs to drop, cast, or panic
* [x] Modify avro.rs to drop, cast, or panic
* [x] Add unit tests where appropriate

---

At a high-level, we should be dropping fields during transpilation if the subschema is incompatible (tuple validation) or if the schema is underspecified (an empty object). This adds error-handling when translating between schemas (`ast -> bigquery` and `ast -> avro` in particular).

I've tried to keep the changes minimal. I've left most of the unit-tests alone aside from refactoring so it is easier to add the context variable. I've added a `compatible` field to the `TestCase` object which makes it easy to retrofit the existing tests to determine whether fields will be dropped with the new error handling methods.

See https://github.com/mozilla/jsonschema-transpiler/pull/76#issuecomment-505630943 for a sanity check on the full schema repository. ",acmiyaguchi,3304040,2019-06-11T00:09:49Z,COLLABORATOR,True,943,321,18,Compile JSON Schema into Avro and BigQuery schemas,Rust,bf7932461f4fad376a18279e5f6457d80c32de93,Update docstrings
103,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/73,73,Add instructions on publishing to crates.io,"This adds a section for publishing to crates.io. It also updates the status badge and removes the version number from the cli help page.

",acmiyaguchi,3304040,2019-05-03T17:58:32Z,COLLABORATOR,True,11,3,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,4aba72405bb9522f9fac65b4e570999cd1f87842,Remove version from cli output (use --version)
104,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/73,73,Add instructions on publishing to crates.io,"This adds a section for publishing to crates.io. It also updates the status badge and removes the version number from the cli help page.

",acmiyaguchi,3304040,2019-05-03T17:58:32Z,COLLABORATOR,True,11,3,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,2c11ec6701ac2596aa34d024bc15b9c95e0168d3,Add instructions for publishing the crate
105,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/73,73,Add instructions on publishing to crates.io,"This adds a section for publishing to crates.io. It also updates the status badge and removes the version number from the cli help page.

",acmiyaguchi,3304040,2019-05-03T17:58:32Z,COLLABORATOR,True,11,3,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,cc6e8db6794a6bda5f0e700221f0d35db1c52501,Update status badge
106,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/72,72,Bump version to 0.5.0,,acmiyaguchi,3304040,2019-05-03T17:32:31Z,COLLABORATOR,True,2,2,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,395379ebfa65528ee8799e2ec10e835cf11835b0,Bump version to 0.5.0
107,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/71,71,Ignore hidden test resource files,"This makes working on these files in your editor of choice a bit easier, because you don't need to close and/or delete workspace files (e.g. `.swp` files in vim).",fbertsch,20819040,2019-05-03T15:24:19Z,COLLABORATOR,True,4,0,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,1c97dece47c88d4964db9f5263535dddb4a9381d,Ignore hidden test resource files
108,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/71,71,Ignore hidden test resource files,"This makes working on these files in your editor of choice a bit easier, because you don't need to close and/or delete workspace files (e.g. `.swp` files in vim).",fbertsch,20819040,2019-05-03T15:24:19Z,COLLABORATOR,True,4,0,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,2ce14fcc47e46da35e146564e9c5e9be4d44cc56,Fix lint
109,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/70,70,Use timestamp rather than datetime in bq,"TIMESTAMP type can be partioned on, but DATETIME cannot.

I ain't angry, but I'm angry.",fbertsch,20819040,2019-05-02T22:32:38Z,COLLABORATOR,True,4,4,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,0aa753284115bf7c4ec512b3cfd06202e620301b,"Use timestamp rather than datetime in bq

TIMESTAMP type can be partioned on, but DATETIME cannot."
110,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/68,68,Bump version to 0.4.0,,acmiyaguchi,3304040,2019-05-02T21:21:00Z,COLLABORATOR,True,4,4,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,550f796b923df75e1c88ac0bb2dde9fdb2705595,Bump version to 0.4.0
111,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/67,67,Make Jsonschema datetime a format,,fbertsch,20819040,2019-05-02T19:43:11Z,COLLABORATOR,True,37,9,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,9b0a8c941b7cf02b6648e7f71d71dce08a63e09e,Make Jsonschema datetime a format
112,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/67,67,Make Jsonschema datetime a format,,fbertsch,20819040,2019-05-02T19:43:11Z,COLLABORATOR,True,37,9,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,9a90d49c54bf9e7269e1affce13049cd24d0e55b,Lint and update integration test
113,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/67,67,Make Jsonschema datetime a format,,fbertsch,20819040,2019-05-02T19:43:11Z,COLLABORATOR,True,37,9,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,f1e240a885e53aa3afdfba33ea5cf6f26a17a156,More linting...
114,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/67,67,Make Jsonschema datetime a format,,fbertsch,20819040,2019-05-02T19:43:11Z,COLLABORATOR,True,37,9,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,2a93063fb6e362788e7cee0b5a0f1a27ca217d08,"Make format its own type

Ignore formatting for other types"
115,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/67,67,Make Jsonschema datetime a format,,fbertsch,20819040,2019-05-02T19:43:11Z,COLLABORATOR,True,37,9,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,b39fae98abf4124742d53f97342328d8a6ea14d8,Ignore other format types
116,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/67,67,Make Jsonschema datetime a format,,fbertsch,20819040,2019-05-02T19:43:11Z,COLLABORATOR,True,37,9,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,197d6c09441b69abc466f8def26f9da39e773f6c,Fix lint
117,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/66,66,Bump to 0.3.0,,acmiyaguchi,3304040,2019-05-01T23:26:16Z,COLLABORATOR,True,2,2,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,ed271e3e0e4eaefb3ed71320c4ebbeb0ba832507,Bump to 0.3.0
118,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/65,65,WIP: Datetime support for bq,,fbertsch,20819040,2019-05-01T17:42:30Z,COLLABORATOR,True,101,1,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,4a5ad259cfa68fdb4ee5b393c042b96bb56c9894,"Add datetime support

These are represented as strings in avro"
119,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/65,65,WIP: Datetime support for bq,,fbertsch,20819040,2019-05-01T17:42:30Z,COLLABORATOR,True,101,1,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,09dde11fa93a8d1eb791d32ad06a5d8f9a19f429,Make json-schema atoms kebab-cased
120,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,f970986969e74ac4698b20fe73b22dd884c6b06f,Modify tests to assert that top-level should be an array
121,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,4e013203e9d9f74867d00a52e9bf814fb8fa2b71,Add initial implementation for the root record
122,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,bb0ce6e4988a57d7233ac82a5bde49fa2d07c8df,Format and removed unneeded dependencies
123,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,68511aebecb35f226d279cdd44749e9ac672f5e4,Add a top-level schema enum for handling logic at the root
124,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,a75f2d44fa5fa3517a0cf054e2b7c9f0b5998c7d,"Update tests to include root name for atoms, lists, and maps"
125,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,499f2e99d77e2d452667158111445974916a8204,Implement logic for ensuring output bigquery schema is a list
126,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,04f6c6934e534e4106877193c07dc80c297acf4c,Set default column name as a constant
127,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,60f545cd83eb7b17b8f1c01f935500154feb6d0a,Add unit-tests for bigquery::Schema
128,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,59cc08908c7d6d7e75908b387fc981a2708fdfb8,Fix datetime test from rebase
129,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,9770f87987f481e857dc9ff135d5980c76a37174,Add comments to bigquery definitions
130,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,297c6707eba9483baf4b28a381421a2242838b5a,Merge remote-tracking branch 'upstream/master' into bq-columns
131,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/61,61,Extract root columns from BigQuery schema,"This will fix #51 by making the starting the resulting BigQuery schema as a list of elements.

EDIT: This is ready for review. See https://github.com/mozilla/jsonschema-transpiler/pull/61#issuecomment-488427937

---
There are currently 5 failures:

```
failures:
    bigquery_test_object_empty_record
    bigquery_test_oneof_object_merge
    bigquery_test_oneof_object_merge_nullability
    bigquery_test_oneof_object_merge_with_complex
    bigquery_test_oneof_object_with_atomics

test result: FAILED. 23 passed; 5 failed; 0 ignored; 0 measured; 0 filtered out
```

In particular, the edge case that needs to be captured is the empty schema. Should the resulting table be an empty schema i.e. `[]`?

```json
    {
      ""description"": [
        ""Empty structs are not supported in BigQuery, so we treat them "",
        ""as empty documents.""
      ],
      ""name"": ""test_object_empty_record"",
      ""test"": {
        ""avro"": {
          ""type"": ""string""
        },
        ""bigquery"": {
          ""mode"": ""REQUIRED"",
          ""type"": ""STRING""
        },
        ""json"": {
          ""properties"": {},
          ""type"": ""object""
        }
      }
    }
```

I suspect there may need to be changes to the avro implementation too. However, all real-world documents will never reach this edge-case due to valid metadata. 
",acmiyaguchi,3304040,2019-05-01T00:45:40Z,COLLABORATOR,True,919,779,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,83e24b3e66aa499f2bf7cca80320cf23808f2c4f,"Bump version to 1.0.0

BigQuery schemas are now arrays at the top-level which fixes #51. Schemas
generated before this point were not actually accepted as valid BigQuery
schemas and needed to be modified before loads."
132,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/57,57,Require compiled test updates were committed in CI,,relud,433717,2019-04-26T22:32:22Z,COLLABORATOR,True,4,0,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,4daa41c3e385c5073aa1e23eecae1c87964ae035,Require compiled test updates were committed in CI
133,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/56,56,Refactor scripts/format-tests.py into build.rs,"Fixes #53

~~it's unclear to me when/where this should be called, so I started with only copying the logic.~~",relud,433717,2019-04-26T19:03:06Z,COLLABORATOR,True,1908,1970,10,Compile JSON Schema into Avro and BigQuery schemas,Rust,d1796c173cbdc31cf95f4523cb5e6727d657b2d4,"Refactor scripts/format-tests.py into build.rs

Fixes #53"
134,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/50,50,Remove allOf case and add linting to CI,"This makes the default status of CI green by removing a test case for a feature that is currently not supported. The `allOf` (as well as tuples) are not being converted.

This also adds `cargo fmt` as a required linting step, and warnings on `clippy`. ",acmiyaguchi,3304040,2019-04-23T00:31:00Z,COLLABORATOR,True,12,138,5,Compile JSON Schema into Avro and BigQuery schemas,Rust,fdad0a33d5f7ec3919f16dbc9b6ce04b0fb42c45,Remove failing allOf test
135,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/50,50,Remove allOf case and add linting to CI,"This makes the default status of CI green by removing a test case for a feature that is currently not supported. The `allOf` (as well as tuples) are not being converted.

This also adds `cargo fmt` as a required linting step, and warnings on `clippy`. ",acmiyaguchi,3304040,2019-04-23T00:31:00Z,COLLABORATOR,True,12,138,5,Compile JSON Schema into Avro and BigQuery schemas,Rust,ce0a7a2f00b93880fb5eb908e3847a0e493cd227,Fix issues caught by cargo-clippy
136,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/50,50,Remove allOf case and add linting to CI,"This makes the default status of CI green by removing a test case for a feature that is currently not supported. The `allOf` (as well as tuples) are not being converted.

This also adds `cargo fmt` as a required linting step, and warnings on `clippy`. ",acmiyaguchi,3304040,2019-04-23T00:31:00Z,COLLABORATOR,True,12,138,5,Compile JSON Schema into Avro and BigQuery schemas,Rust,1a7e06af412a956eed197a1c07098773f459621c,Fail on format and warn on clippy
137,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/50,50,Remove allOf case and add linting to CI,"This makes the default status of CI green by removing a test case for a feature that is currently not supported. The `allOf` (as well as tuples) are not being converted.

This also adds `cargo fmt` as a required linting step, and warnings on `clippy`. ",acmiyaguchi,3304040,2019-04-23T00:31:00Z,COLLABORATOR,True,12,138,5,Compile JSON Schema into Avro and BigQuery schemas,Rust,472c83bfcf0528bab293efde0bba1067e1c5028e,Add trailing newline
138,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/49,49,Enable avro tests in generated test suite,This updates the generated test-suite to include the avro tests.,acmiyaguchi,3304040,2019-04-23T00:06:29Z,COLLABORATOR,True,1651,100,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,34d1a4943b200a266d7edb5b676e4a32b8eef2be,Add avro to the generated test suite
139,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/49,49,Enable avro tests in generated test suite,This updates the generated test-suite to include the avro tests.,acmiyaguchi,3304040,2019-04-23T00:06:29Z,COLLABORATOR,True,1651,100,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,91d5fe8b20e0f7fa6b6e1bf331ab28b747b39590,Update expected avro in atomic
140,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/49,49,Enable avro tests in generated test suite,This updates the generated test-suite to include the avro tests.,acmiyaguchi,3304040,2019-04-23T00:06:29Z,COLLABORATOR,True,1651,100,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,a8c27c72e3ac9f6ed4f665970dc91f3bb1082ab9,Update tests for object
141,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/49,49,Enable avro tests in generated test suite,This updates the generated test-suite to include the avro tests.,acmiyaguchi,3304040,2019-04-23T00:06:29Z,COLLABORATOR,True,1651,100,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,36cac8fda679859551f8b006505d585478ff8300,Update array tests
142,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/49,49,Enable avro tests in generated test suite,This updates the generated test-suite to include the avro tests.,acmiyaguchi,3304040,2019-04-23T00:06:29Z,COLLABORATOR,True,1651,100,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,e901d6a2789154e708fcf8aef8796fe9f1999378,Update tests for map
143,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/49,49,Enable avro tests in generated test suite,This updates the generated test-suite to include the avro tests.,acmiyaguchi,3304040,2019-04-23T00:06:29Z,COLLABORATOR,True,1651,100,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,1f5e337610cede6936b47745f5a208a6012ed80b,Fix tests for oneof
144,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/49,49,Enable avro tests in generated test suite,This updates the generated test-suite to include the avro tests.,acmiyaguchi,3304040,2019-04-23T00:06:29Z,COLLABORATOR,True,1651,100,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,2331cebb7f6c04f32f1d005eca46e797616678f1,Fix avro tests for incompatible cases
145,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/49,49,Enable avro tests in generated test suite,This updates the generated test-suite to include the avro tests.,acmiyaguchi,3304040,2019-04-23T00:06:29Z,COLLABORATOR,True,1651,100,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,ab83728e9176cf6f7254de60b46b43a9f0eeca52,Add generated test file for avro
146,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/49,49,Enable avro tests in generated test suite,This updates the generated test-suite to include the avro tests.,acmiyaguchi,3304040,2019-04-23T00:06:29Z,COLLABORATOR,True,1651,100,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,18f831a3b3097c6bfcfffbe9e5b47c045f61bb75,Merge remote-tracking branch 'origin/dev' into avro-tests
147,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/48,48,Update README; add license and CoC,"This PR should address #46. It shortens the README to be friendlier at a glance, moves more detailed documentation into a `docs/` folder, adds a code of conduct, and adds a license. ",acmiyaguchi,3304040,2019-04-22T23:38:24Z,COLLABORATOR,True,538,165,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,8eed974f832b8969575a4d5fe1b8b46a77ac2b42,Reduce README and add a docs/ section
148,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/48,48,Update README; add license and CoC,"This PR should address #46. It shortens the README to be friendlier at a glance, moves more detailed documentation into a `docs/` folder, adds a code of conduct, and adds a license. ",acmiyaguchi,3304040,2019-04-22T23:38:24Z,COLLABORATOR,True,538,165,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,5dfa51c274e3ac0c906cdb1d7e0b68eaf2af8b34,Add Mozilla Code of Conduct
149,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/48,48,Update README; add license and CoC,"This PR should address #46. It shortens the README to be friendlier at a glance, moves more detailed documentation into a `docs/` folder, adds a code of conduct, and adds a license. ",acmiyaguchi,3304040,2019-04-22T23:38:24Z,COLLABORATOR,True,538,165,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,c2ca9aae0d745f77019dc7e832453fbeeb94a68b,Add MPL-2.0
150,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/48,48,Update README; add license and CoC,"This PR should address #46. It shortens the README to be friendlier at a glance, moves more detailed documentation into a `docs/` folder, adds a code of conduct, and adds a license. ",acmiyaguchi,3304040,2019-04-22T23:38:24Z,COLLABORATOR,True,538,165,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,a748b4eeaf1a9ff3205841c8094a12775ec0220f,Add short description of where the tool is used
151,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/48,48,Update README; add license and CoC,"This PR should address #46. It shortens the README to be friendlier at a glance, moves more detailed documentation into a `docs/` folder, adds a code of conduct, and adds a license. ",acmiyaguchi,3304040,2019-04-22T23:38:24Z,COLLABORATOR,True,538,165,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,baeec2c6f49e3723ea58d20d7a122ecfd603c79e,Update CODE_OF_CONDUCT.md
152,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/45,45,Modify cli name and options; bump version to 0.2,"The changes the package name from `jsonschema_transpiler` to `jsonschema-transpiler`. The application now reads from stdin when no argument are provided or from a file when a positional argument is passed. This should make it easier to run the tool in bash via xargs and pipelining.

I've also updated the README to include some example usage that shows what the input and outputs look like.",acmiyaguchi,3304040,2019-03-29T20:19:58Z,COLLABORATOR,True,173,22,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,04b44d94651850194750c6db3ec1a48ea4a12c0f,Update package name and arguments
153,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/45,45,Modify cli name and options; bump version to 0.2,"The changes the package name from `jsonschema_transpiler` to `jsonschema-transpiler`. The application now reads from stdin when no argument are provided or from a file when a positional argument is passed. This should make it easier to run the tool in bash via xargs and pipelining.

I've also updated the README to include some example usage that shows what the input and outputs look like.",acmiyaguchi,3304040,2019-03-29T20:19:58Z,COLLABORATOR,True,173,22,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,0694b0204bb595161d330d12392027b9d4c11993,Move file to a positional argument and read from stdin
154,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/45,45,Modify cli name and options; bump version to 0.2,"The changes the package name from `jsonschema_transpiler` to `jsonschema-transpiler`. The application now reads from stdin when no argument are provided or from a file when a positional argument is passed. This should make it easier to run the tool in bash via xargs and pipelining.

I've also updated the README to include some example usage that shows what the input and outputs look like.",acmiyaguchi,3304040,2019-03-29T20:19:58Z,COLLABORATOR,True,173,22,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,ed0497d1c20e085526e4cca6ae3974d78030420d,Bump version to 0.2.0
155,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/45,45,Modify cli name and options; bump version to 0.2,"The changes the package name from `jsonschema_transpiler` to `jsonschema-transpiler`. The application now reads from stdin when no argument are provided or from a file when a positional argument is passed. This should make it easier to run the tool in bash via xargs and pipelining.

I've also updated the README to include some example usage that shows what the input and outputs look like.",acmiyaguchi,3304040,2019-03-29T20:19:58Z,COLLABORATOR,True,173,22,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,1de4429f006b29b93dcaf5dd90b4968a39c1254c,Update README with example usage
156,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/45,45,Modify cli name and options; bump version to 0.2,"The changes the package name from `jsonschema_transpiler` to `jsonschema-transpiler`. The application now reads from stdin when no argument are provided or from a file when a positional argument is passed. This should make it easier to run the tool in bash via xargs and pipelining.

I've also updated the README to include some example usage that shows what the input and outputs look like.",acmiyaguchi,3304040,2019-03-29T20:19:58Z,COLLABORATOR,True,173,22,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,6a61916e5fdce21e74f14d573671b115436d3796,"Handle ""-"" file as stdin"
157,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/44,44,Update README,"This updates the README with some high level information about this tool. This addresses #17 by giving an example of using Rust's enums as an algebraic data type, and by separating types from their associated attributes.",acmiyaguchi,3304040,2019-03-28T22:57:28Z,COLLABORATOR,True,135,42,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,fc448d94ed98b9bd9b8a60491b3a082f7f1791b3,Add information about contributing with high level description of the main pattern in the code.
158,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/44,44,Update README,"This updates the README with some high level information about this tool. This addresses #17 by giving an example of using Rust's enums as an algebraic data type, and by separating types from their associated attributes.",acmiyaguchi,3304040,2019-03-28T22:57:28Z,COLLABORATOR,True,135,42,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,42f54e19b39f0730a44c09f5dbee1241d888439a,Add CircleCI badge
159,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/44,44,Update README,"This updates the README with some high level information about this tool. This addresses #17 by giving an example of using Rust's enums as an algebraic data type, and by separating types from their associated attributes.",acmiyaguchi,3304040,2019-03-28T22:57:28Z,COLLABORATOR,True,135,42,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,d8b1e56a654e7d650b9651ec5b175364f0e3b507,Add usage text from `cargo run -- --help`
160,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/43,43,Integration test scripts,"This PR includes a set of integration tests used to verify the serialization of telemetry and structured ingestion against a sampled set of landfill documents. These tests require valid aws credentials (via boto3) and gcp credentials (via the gcloud cli).

Is this the appropriate repository for these scripts?",acmiyaguchi,3304040,2019-03-25T20:12:50Z,COLLABORATOR,True,359,27,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,95b9609ecbec0ac172f364c357dcfca16a494f4a,Add scripts for testing avro serialization of generated schemas
161,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/43,43,Integration test scripts,"This PR includes a set of integration tests used to verify the serialization of telemetry and structured ingestion against a sampled set of landfill documents. These tests require valid aws credentials (via boto3) and gcp credentials (via the gcloud cli).

Is this the appropriate repository for these scripts?",acmiyaguchi,3304040,2019-03-25T20:12:50Z,COLLABORATOR,True,359,27,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,67e4298437d5a1653b2b3fff1c44d39673dc8015,Fix newline delimited json when downloading mps
162,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/43,43,Integration test scripts,"This PR includes a set of integration tests used to verify the serialization of telemetry and structured ingestion against a sampled set of landfill documents. These tests require valid aws credentials (via boto3) and gcp credentials (via the gcloud cli).

Is this the appropriate repository for these scripts?",acmiyaguchi,3304040,2019-03-25T20:12:50Z,COLLABORATOR,True,359,27,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,59033ff1db086f399df62822bb456950125c756b,Use rapidjson to validate data before writing to disk
163,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/43,43,Integration test scripts,"This PR includes a set of integration tests used to verify the serialization of telemetry and structured ingestion against a sampled set of landfill documents. These tests require valid aws credentials (via boto3) and gcp credentials (via the gcloud cli).

Is this the appropriate repository for these scripts?",acmiyaguchi,3304040,2019-03-25T20:12:50Z,COLLABORATOR,True,359,27,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,5c43c692c99e898b7da40071e27a402c7fd317c7,Update generate-avro script for sanitizing column names
164,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/43,43,Integration test scripts,"This PR includes a set of integration tests used to verify the serialization of telemetry and structured ingestion against a sampled set of landfill documents. These tests require valid aws credentials (via boto3) and gcp credentials (via the gcloud cli).

Is this the appropriate repository for these scripts?",acmiyaguchi,3304040,2019-03-25T20:12:50Z,COLLABORATOR,True,359,27,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,7a502c358186576d3422cff5a7ef094e11703913,Add a script for loading avro files into bigquery
165,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/43,43,Integration test scripts,"This PR includes a set of integration tests used to verify the serialization of telemetry and structured ingestion against a sampled set of landfill documents. These tests require valid aws credentials (via boto3) and gcp credentials (via the gcloud cli).

Is this the appropriate repository for these scripts?",acmiyaguchi,3304040,2019-03-25T20:12:50Z,COLLABORATOR,True,359,27,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,a602f1d9958dc7f2fd429f948dd03fd7111d7056,Rename mps integration scripts in a consistent way
166,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/43,43,Integration test scripts,"This PR includes a set of integration tests used to verify the serialization of telemetry and structured ingestion against a sampled set of landfill documents. These tests require valid aws credentials (via boto3) and gcp credentials (via the gcloud cli).

Is this the appropriate repository for these scripts?",acmiyaguchi,3304040,2019-03-25T20:12:50Z,COLLABORATOR,True,359,27,9,Compile JSON Schema into Avro and BigQuery schemas,Rust,e237c64a80ee748c237aa4150feaa0916325c3c7,Add readme for integration scripts
167,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/40,40,Modify schema transformations for BigQuery compatibility,"Based on a run of an integration suite (in a separate PR), I found a few bugs and BigQuery related quirks that I needed to fix.

* Always use 64-bit representation of numbers
* Use intersection instead of union when determining whether a field is required
* Do not serialize empty structs, instead downcast into JSON
* Convert/sanitize object names to match BigQuery column name requirements

I've added most of the relevant unit tests for these, and ran integration tests against a recent version of mps and sampled landfill data. 
",acmiyaguchi,3304040,2019-03-22T00:10:37Z,COLLABORATOR,True,574,53,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,e09b36439d4097f2e766819dc5da1c8338a38a5b,Add default case for avro documents
168,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/40,40,Modify schema transformations for BigQuery compatibility,"Based on a run of an integration suite (in a separate PR), I found a few bugs and BigQuery related quirks that I needed to fix.

* Always use 64-bit representation of numbers
* Use intersection instead of union when determining whether a field is required
* Do not serialize empty structs, instead downcast into JSON
* Convert/sanitize object names to match BigQuery column name requirements

I've added most of the relevant unit tests for these, and ran integration tests against a recent version of mps and sampled landfill data. 
",acmiyaguchi,3304040,2019-03-22T00:10:37Z,COLLABORATOR,True,574,53,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,33556d2839e0ec234645b372b65721ba1246080a,"Modify tests to include defaults for avro

This actually increases the number of failing tests because of modifications to
the json formatting. The test suite needs another look over to ensure that the
cases are valid."
169,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/40,40,Modify schema transformations for BigQuery compatibility,"Based on a run of an integration suite (in a separate PR), I found a few bugs and BigQuery related quirks that I needed to fix.

* Always use 64-bit representation of numbers
* Use intersection instead of union when determining whether a field is required
* Do not serialize empty structs, instead downcast into JSON
* Convert/sanitize object names to match BigQuery column name requirements

I've added most of the relevant unit tests for these, and ran integration tests against a recent version of mps and sampled landfill data. 
",acmiyaguchi,3304040,2019-03-22T00:10:37Z,COLLABORATOR,True,574,53,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,9e37a16ff8bd17ca3c51c2002cb0b63ea8448a68,Represent integers and numbers using 64-bit types in avro
170,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/40,40,Modify schema transformations for BigQuery compatibility,"Based on a run of an integration suite (in a separate PR), I found a few bugs and BigQuery related quirks that I needed to fix.

* Always use 64-bit representation of numbers
* Use intersection instead of union when determining whether a field is required
* Do not serialize empty structs, instead downcast into JSON
* Convert/sanitize object names to match BigQuery column name requirements

I've added most of the relevant unit tests for these, and ran integration tests against a recent version of mps and sampled landfill data. 
",acmiyaguchi,3304040,2019-03-22T00:10:37Z,COLLABORATOR,True,574,53,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,02efea1379065788ca311bbaff33444749612e74,Add a test for overlapping required fields in oneOf
171,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/40,40,Modify schema transformations for BigQuery compatibility,"Based on a run of an integration suite (in a separate PR), I found a few bugs and BigQuery related quirks that I needed to fix.

* Always use 64-bit representation of numbers
* Use intersection instead of union when determining whether a field is required
* Do not serialize empty structs, instead downcast into JSON
* Convert/sanitize object names to match BigQuery column name requirements

I've added most of the relevant unit tests for these, and ran integration tests against a recent version of mps and sampled landfill data. 
",acmiyaguchi,3304040,2019-03-22T00:10:37Z,COLLABORATOR,True,574,53,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,96f080f06c06cbd65520f145f95f2b2e25702b0e,Update rules for determining whether a field is required
172,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/40,40,Modify schema transformations for BigQuery compatibility,"Based on a run of an integration suite (in a separate PR), I found a few bugs and BigQuery related quirks that I needed to fix.

* Always use 64-bit representation of numbers
* Use intersection instead of union when determining whether a field is required
* Do not serialize empty structs, instead downcast into JSON
* Convert/sanitize object names to match BigQuery column name requirements

I've added most of the relevant unit tests for these, and ran integration tests against a recent version of mps and sampled landfill data. 
",acmiyaguchi,3304040,2019-03-22T00:10:37Z,COLLABORATOR,True,574,53,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,36326f9eb09bf464769cd9e17e62a15fb106d6fc,Do not serialize empty structs in avro or bigquery
173,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/40,40,Modify schema transformations for BigQuery compatibility,"Based on a run of an integration suite (in a separate PR), I found a few bugs and BigQuery related quirks that I needed to fix.

* Always use 64-bit representation of numbers
* Use intersection instead of union when determining whether a field is required
* Do not serialize empty structs, instead downcast into JSON
* Convert/sanitize object names to match BigQuery column name requirements

I've added most of the relevant unit tests for these, and ran integration tests against a recent version of mps and sampled landfill data. 
",acmiyaguchi,3304040,2019-03-22T00:10:37Z,COLLABORATOR,True,574,53,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,d103f10b2ddfc23fc49a7ea585e185d044c476b3,Log implicit JSON string conversions when RUST_LOG=warn
174,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/40,40,Modify schema transformations for BigQuery compatibility,"Based on a run of an integration suite (in a separate PR), I found a few bugs and BigQuery related quirks that I needed to fix.

* Always use 64-bit representation of numbers
* Use intersection instead of union when determining whether a field is required
* Do not serialize empty structs, instead downcast into JSON
* Convert/sanitize object names to match BigQuery column name requirements

I've added most of the relevant unit tests for these, and ran integration tests against a recent version of mps and sampled landfill data. 
",acmiyaguchi,3304040,2019-03-22T00:10:37Z,COLLABORATOR,True,574,53,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,870d4fd9fc5958b978404f5119c0e7d1cb37ed5b,Add function for fixing property names and removing invalid ones
175,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/40,40,Modify schema transformations for BigQuery compatibility,"Based on a run of an integration suite (in a separate PR), I found a few bugs and BigQuery related quirks that I needed to fix.

* Always use 64-bit representation of numbers
* Use intersection instead of union when determining whether a field is required
* Do not serialize empty structs, instead downcast into JSON
* Convert/sanitize object names to match BigQuery column name requirements

I've added most of the relevant unit tests for these, and ran integration tests against a recent version of mps and sampled landfill data. 
",acmiyaguchi,3304040,2019-03-22T00:10:37Z,COLLABORATOR,True,574,53,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,06d5583872f072c3ff3c1e12a0d745314b59fc8d,"Convert fields into valid BigQuery column names

Some example of columns include `$schema` and `64bit`."
176,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/40,40,Modify schema transformations for BigQuery compatibility,"Based on a run of an integration suite (in a separate PR), I found a few bugs and BigQuery related quirks that I needed to fix.

* Always use 64-bit representation of numbers
* Use intersection instead of union when determining whether a field is required
* Do not serialize empty structs, instead downcast into JSON
* Convert/sanitize object names to match BigQuery column name requirements

I've added most of the relevant unit tests for these, and ran integration tests against a recent version of mps and sampled landfill data. 
",acmiyaguchi,3304040,2019-03-22T00:10:37Z,COLLABORATOR,True,574,53,13,Compile JSON Schema into Avro and BigQuery schemas,Rust,2ec5ec5c6b74da56a3afcc62b4e13adf95a87781,Rename sanitize_string to rename_string_bigquery and address review
177,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,5e59a94cb9a58ffce43263c189703410c186cf81,Add pretty_assertion for pretty diff view in unit tests
178,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,955a3b25d6651e9f9fc2787b60db2ae694621fb5,Fix nested avro definition
179,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,bde6d5fec4c3c31e56399b83fd62575913df2a8d,Fix definition of union type
180,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,c54225e009eaa6959077c23031653221f704f198,Fix failing tests for union types in avro
181,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,325e56bcb76a1d89316c07c5a5f87f0fa6ebbd7c,Add support for deserializing tuple types
182,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,ef26e281074434d0a38e9d4114ea409d5a883089,Add namespace field to ast::Tag
183,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,8a488702137b969f64f15a7a2202599c866407a5,Add name for array items when inferring namespaces
184,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,210985e23554daa3282cb22d795384a52a4f9764,Update namespace generation
185,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,975c4184f94cafa6e41dd0f294f22015dca7f154,Add a root anchor to the schema for path construction
186,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,c6f8046604f7e9fd1772bea07a1e3c28e1ec063c,Add wrapper to Union::collapse into Tag for namespace calculations
187,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/37,37,Fix namespace generation in Avro schemas,"This PR, along with [mps#291](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/291) and [mps#290](https://github.com/mozilla-services/mozilla-pipeline-schemas/pull/290) fixes generation of Avro schemas.

* Fixed the type of record fields. They were originally flattened to look like this:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"":""record"",
         ""fields"":[
            {
               ""name"":""bar"",
               ""type"":""null""
            }
         ]
      }
   ]
}
```
They should actually be in the following simplified form:
```json
{
   ""type"":""record"",
   ""fields"":[
      {
         ""name"":""foo"",
         ""type"": {
             ""name"": ""foo"",
             ""type"":""record"",
             ""fields"":[
                { 
                   ""name"":""bar"",
                   ""type"":""null""
                }
             ]
           }
      }
   ]
}
```
However, this forces the more verbose form of simple types due to serialization.

* Fixed union types by using vectors directly e.g. `{""type"": [""null"", ""int""]}` to `[""null"", ""int""]`.
* Support de-serialization of tuple-types in jsonschema, but not conversion. 
* Add support for inferring namespaces in `Tag::infer_name`

After running the `./test-mps-valid-avro.py` script, all of the generated schemas should now be valid avro. ",acmiyaguchi,3304040,2019-03-14T23:44:54Z,COLLABORATOR,True,544,106,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,c73fcc6d1921c283f7d7474fde1ebaf00fcf1f6a,Modify avro and bigquery to use Tag::collapse instead of Union::collapse
188,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/36,36,Add scripts for testing transpilation against mps,"This adds some small scripts for testing the success rate on mozilla-pipeline-schemas. There are currently 25 schemas that fail to convert at all. I think a significant portion of these can be attributed to things like mozilla-services/mozilla-pipeline-schemas#288. 

Here's an example of the output:
```
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//pocket/fire-tv-events/fire-tv-events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""data did not match any variant of untagged enum AdditionalProperties"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//firefox-launcher-process/launcher-process-failure/launcher-process-failure.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/mobile-event/mobile-event.1.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/testpilot/testpilot.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/update/update.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/untrustedModules/untrustedModules.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/focus-event/focus-event.1.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/crash/crash.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/shield-icq-v1/shield-icq-v1.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/new-profile/new-profile.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/main/main.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/modules/modules.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/mobile-metrics/mobile-metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/event/event.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/saved-session/saved-session.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//mozza/event/event.1.schema.json
25/65 failures
```",acmiyaguchi,3304040,2019-03-13T18:55:32Z,COLLABORATOR,True,109,1,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,72232cf379c547164854a5bf75ec14c26ca5b2d5,Add scripts for testing transpilation against mps
189,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/36,36,Add scripts for testing transpilation against mps,"This adds some small scripts for testing the success rate on mozilla-pipeline-schemas. There are currently 25 schemas that fail to convert at all. I think a significant portion of these can be attributed to things like mozilla-services/mozilla-pipeline-schemas#288. 

Here's an example of the output:
```
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//pocket/fire-tv-events/fire-tv-events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""data did not match any variant of untagged enum AdditionalProperties"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//firefox-launcher-process/launcher-process-failure/launcher-process-failure.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/mobile-event/mobile-event.1.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/testpilot/testpilot.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/update/update.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/untrustedModules/untrustedModules.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/focus-event/focus-event.1.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/crash/crash.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/shield-icq-v1/shield-icq-v1.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/new-profile/new-profile.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/main/main.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/modules/modules.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/mobile-metrics/mobile-metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/event/event.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/saved-session/saved-session.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//mozza/event/event.1.schema.json
25/65 failures
```",acmiyaguchi,3304040,2019-03-13T18:55:32Z,COLLABORATOR,True,109,1,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,00a013b41bd914e3c3251998c716bd8c97d4c5b5,Use binary from build instead of reinstalling
190,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/36,36,Add scripts for testing transpilation against mps,"This adds some small scripts for testing the success rate on mozilla-pipeline-schemas. There are currently 25 schemas that fail to convert at all. I think a significant portion of these can be attributed to things like mozilla-services/mozilla-pipeline-schemas#288. 

Here's an example of the output:
```
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//pocket/fire-tv-events/fire-tv-events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""data did not match any variant of untagged enum AdditionalProperties"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//firefox-launcher-process/launcher-process-failure/launcher-process-failure.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/mobile-event/mobile-event.1.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/testpilot/testpilot.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/update/update.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/untrustedModules/untrustedModules.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/focus-event/focus-event.1.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/crash/crash.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/shield-icq-v1/shield-icq-v1.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/new-profile/new-profile.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/main/main.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/modules/modules.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/mobile-metrics/mobile-metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/event/event.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/saved-session/saved-session.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//mozza/event/event.1.schema.json
25/65 failures
```",acmiyaguchi,3304040,2019-03-13T18:55:32Z,COLLABORATOR,True,109,1,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,74c1622ba29b3e3dcb2926200d28a7a6c3b9c58c,Run shellcheck on scripts
191,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/36,36,Add scripts for testing transpilation against mps,"This adds some small scripts for testing the success rate on mozilla-pipeline-schemas. There are currently 25 schemas that fail to convert at all. I think a significant portion of these can be attributed to things like mozilla-services/mozilla-pipeline-schemas#288. 

Here's an example of the output:
```
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//pocket/fire-tv-events/fire-tv-events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""data did not match any variant of untagged enum AdditionalProperties"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//firefox-launcher-process/launcher-process-failure/launcher-process-failure.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/mobile-event/mobile-event.1.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/testpilot/testpilot.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/update/update.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/untrustedModules/untrustedModules.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/focus-event/focus-event.1.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/crash/crash.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/shield-icq-v1/shield-icq-v1.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/new-profile/new-profile.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/main/main.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/modules/modules.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/mobile-metrics/mobile-metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/event/event.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/saved-session/saved-session.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//mozza/event/event.1.schema.json
25/65 failures
```",acmiyaguchi,3304040,2019-03-13T18:55:32Z,COLLABORATOR,True,109,1,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,65255345b49fc5619a4d716cb50f6229076a58a9,Add script for testing validity of mps avro schemas
192,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/36,36,Add scripts for testing transpilation against mps,"This adds some small scripts for testing the success rate on mozilla-pipeline-schemas. There are currently 25 schemas that fail to convert at all. I think a significant portion of these can be attributed to things like mozilla-services/mozilla-pipeline-schemas#288. 

Here's an example of the output:
```
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//pocket/fire-tv-events/fire-tv-events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-reference-browser/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//org-mozilla-samples-glean/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""data did not match any variant of untagged enum AdditionalProperties"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//firefox-launcher-process/launcher-process-failure/launcher-process-failure.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/metrics/metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/baseline/baseline.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//glean/events/events.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/mobile-event/mobile-event.1.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/testpilot/testpilot.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/update/update.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/untrustedModules/untrustedModules.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/focus-event/focus-event.1.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/crash/crash.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/shield-icq-v1/shield-icq-v1.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/new-profile/new-profile.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/main/main.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/modules/modules.4.schema.json
thread 'main' panicked at 'array missing item', src/jsonschema.rs:191:21
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/mobile-metrics/mobile-metrics.1.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/event/event.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//telemetry/saved-session/saved-session.4.schema.json
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(""invalid type: sequence, expected struct Tag"", line: 0, column: 0)', src/libcore/result.rs:1009:5
note: Run with `RUST_BACKTRACE=1` for a backtrace.
Failed on schemas//mozza/event/event.1.schema.json
25/65 failures
```",acmiyaguchi,3304040,2019-03-13T18:55:32Z,COLLABORATOR,True,109,1,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,bc7612c8458a1c8965c1261a99b7dcd207debac4,Add build step to script
193,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/33,33,Add minimal CircleCI config,"You would only need to enable it on Travis
(though tests are currently failing)",badboy,2129,2019-02-21T12:39:41Z,MEMBER,True,29,0,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,c72b5cde67f745f5b3b7066e241b287bea963414,Add minimal CircleCI config
194,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/31,31,Add option to convert avro in the cli,,acmiyaguchi,3304040,2019-02-15T22:35:17Z,COLLABORATOR,True,44,28,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,1c1820fa7c93b41f3e134227e2ebc652b8e4006c,Make avro interface public
195,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/31,31,Add option to convert avro in the cli,,acmiyaguchi,3304040,2019-02-15T22:35:17Z,COLLABORATOR,True,44,28,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,d000c6ed8e88fabe7b63c73a51ed05b344612521,Add avro conversion to cli
196,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/30,30,Implement `Into<ast::AST> for avro::Type`,This PR contains a functional implementation of the `Into` trait for the avro schema.,acmiyaguchi,3304040,2019-02-15T21:14:27Z,COLLABORATOR,True,135,6,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,48bc2501857646262feb5cd497c0be1f6239a31c,Add avro implementation for primitives and record
197,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/30,30,Implement `Into<ast::AST> for avro::Type`,This PR contains a functional implementation of the `Into` trait for the avro schema.,acmiyaguchi,3304040,2019-02-15T21:14:27Z,COLLABORATOR,True,135,6,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,56222765373c5a7e1af094f5a968958815c2cce8,Add working implementation of From<ast::Tag> for avro::Type
198,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/29,29,Implement grammar for Avro,"This fixes #14. I learned from the other grammars, making the avro schema cleaner than the other implementations. The spec was useful during this stage.

https://avro.apache.org/docs/current/spec.html

* Unions are implemented using standard serde attributes. In the `jsonschema` module, I left the type as a `Value` and implemented a helper function to disambiguate `Type` from a `Vec<Type>`.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/jsonschema.rs#L79-L85

* I also avoided the issue with flattening a nested enum by treating the enum as an untagged union. I ran into issues treating the main `Type` as a tagged-union and adding the `serde(tag = ""type"")` attribute on all involved types. This will flatten to the correct json format, but it requires custom deserialization. In the `bigquery` module, I had to implement a custom deserializer.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L52-L55

* I avoided using HashMaps for things that are actually Vecs. In `bigquery`, I did this because it makes traversal and insertions into the schema simpler. However, most of the manipulation is done within `ast` now, so having an easily traversable schema by path is no longer of concern.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L69-L72",acmiyaguchi,3304040,2019-02-14T22:28:50Z,COLLABORATOR,True,396,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,d089cd324187985947a98e1eafbbfcabbbc8e600,Add initial data-structure for avro
199,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/29,29,Implement grammar for Avro,"This fixes #14. I learned from the other grammars, making the avro schema cleaner than the other implementations. The spec was useful during this stage.

https://avro.apache.org/docs/current/spec.html

* Unions are implemented using standard serde attributes. In the `jsonschema` module, I left the type as a `Value` and implemented a helper function to disambiguate `Type` from a `Vec<Type>`.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/jsonschema.rs#L79-L85

* I also avoided the issue with flattening a nested enum by treating the enum as an untagged union. I ran into issues treating the main `Type` as a tagged-union and adding the `serde(tag = ""type"")` attribute on all involved types. This will flatten to the correct json format, but it requires custom deserialization. In the `bigquery` module, I had to implement a custom deserializer.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L52-L55

* I avoided using HashMaps for things that are actually Vecs. In `bigquery`, I did this because it makes traversal and insertions into the schema simpler. However, most of the manipulation is done within `ast` now, so having an easily traversable schema by path is no longer of concern.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L69-L72",acmiyaguchi,3304040,2019-02-14T22:28:50Z,COLLABORATOR,True,396,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,fff69931ea1c9c59c7384f7350df142d9fc21a4f,Add stubs for all the unimplemented tests
200,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/29,29,Implement grammar for Avro,"This fixes #14. I learned from the other grammars, making the avro schema cleaner than the other implementations. The spec was useful during this stage.

https://avro.apache.org/docs/current/spec.html

* Unions are implemented using standard serde attributes. In the `jsonschema` module, I left the type as a `Value` and implemented a helper function to disambiguate `Type` from a `Vec<Type>`.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/jsonschema.rs#L79-L85

* I also avoided the issue with flattening a nested enum by treating the enum as an untagged union. I ran into issues treating the main `Type` as a tagged-union and adding the `serde(tag = ""type"")` attribute on all involved types. This will flatten to the correct json format, but it requires custom deserialization. In the `bigquery` module, I had to implement a custom deserializer.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L52-L55

* I avoided using HashMaps for things that are actually Vecs. In `bigquery`, I did this because it makes traversal and insertions into the schema simpler. However, most of the manipulation is done within `ast` now, so having an easily traversable schema by path is no longer of concern.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L69-L72",acmiyaguchi,3304040,2019-02-14T22:28:50Z,COLLABORATOR,True,396,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,7773fd53dc7d28ccb83f2c5983da7f8d0c68d590,Add serialize tests for primitive and record
201,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/29,29,Implement grammar for Avro,"This fixes #14. I learned from the other grammars, making the avro schema cleaner than the other implementations. The spec was useful during this stage.

https://avro.apache.org/docs/current/spec.html

* Unions are implemented using standard serde attributes. In the `jsonschema` module, I left the type as a `Value` and implemented a helper function to disambiguate `Type` from a `Vec<Type>`.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/jsonschema.rs#L79-L85

* I also avoided the issue with flattening a nested enum by treating the enum as an untagged union. I ran into issues treating the main `Type` as a tagged-union and adding the `serde(tag = ""type"")` attribute on all involved types. This will flatten to the correct json format, but it requires custom deserialization. In the `bigquery` module, I had to implement a custom deserializer.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L52-L55

* I avoided using HashMaps for things that are actually Vecs. In `bigquery`, I did this because it makes traversal and insertions into the schema simpler. However, most of the manipulation is done within `ast` now, so having an easily traversable schema by path is no longer of concern.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L69-L72",acmiyaguchi,3304040,2019-02-14T22:28:50Z,COLLABORATOR,True,396,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,5d8efd182cb8afbb5214369da444385cabcfca0f,"Add deserialize tests for enum, array, and map"
202,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/29,29,Implement grammar for Avro,"This fixes #14. I learned from the other grammars, making the avro schema cleaner than the other implementations. The spec was useful during this stage.

https://avro.apache.org/docs/current/spec.html

* Unions are implemented using standard serde attributes. In the `jsonschema` module, I left the type as a `Value` and implemented a helper function to disambiguate `Type` from a `Vec<Type>`.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/jsonschema.rs#L79-L85

* I also avoided the issue with flattening a nested enum by treating the enum as an untagged union. I ran into issues treating the main `Type` as a tagged-union and adding the `serde(tag = ""type"")` attribute on all involved types. This will flatten to the correct json format, but it requires custom deserialization. In the `bigquery` module, I had to implement a custom deserializer.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L52-L55

* I avoided using HashMaps for things that are actually Vecs. In `bigquery`, I did this because it makes traversal and insertions into the schema simpler. However, most of the manipulation is done within `ast` now, so having an easily traversable schema by path is no longer of concern.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L69-L72",acmiyaguchi,3304040,2019-02-14T22:28:50Z,COLLABORATOR,True,396,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,e130c5ccc731def3d9ccc3d6e8e896da454ef851,Add test for serialize fixed
203,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/29,29,Implement grammar for Avro,"This fixes #14. I learned from the other grammars, making the avro schema cleaner than the other implementations. The spec was useful during this stage.

https://avro.apache.org/docs/current/spec.html

* Unions are implemented using standard serde attributes. In the `jsonschema` module, I left the type as a `Value` and implemented a helper function to disambiguate `Type` from a `Vec<Type>`.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/jsonschema.rs#L79-L85

* I also avoided the issue with flattening a nested enum by treating the enum as an untagged union. I ran into issues treating the main `Type` as a tagged-union and adding the `serde(tag = ""type"")` attribute on all involved types. This will flatten to the correct json format, but it requires custom deserialization. In the `bigquery` module, I had to implement a custom deserializer.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L52-L55

* I avoided using HashMaps for things that are actually Vecs. In `bigquery`, I did this because it makes traversal and insertions into the schema simpler. However, most of the manipulation is done within `ast` now, so having an easily traversable schema by path is no longer of concern.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L69-L72",acmiyaguchi,3304040,2019-02-14T22:28:50Z,COLLABORATOR,True,396,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,5c1d8148db32213347320c877588e49b8cb1fc25,"Add serde attributes as needed, remove HashMap in Fields"
204,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/29,29,Implement grammar for Avro,"This fixes #14. I learned from the other grammars, making the avro schema cleaner than the other implementations. The spec was useful during this stage.

https://avro.apache.org/docs/current/spec.html

* Unions are implemented using standard serde attributes. In the `jsonschema` module, I left the type as a `Value` and implemented a helper function to disambiguate `Type` from a `Vec<Type>`.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/jsonschema.rs#L79-L85

* I also avoided the issue with flattening a nested enum by treating the enum as an untagged union. I ran into issues treating the main `Type` as a tagged-union and adding the `serde(tag = ""type"")` attribute on all involved types. This will flatten to the correct json format, but it requires custom deserialization. In the `bigquery` module, I had to implement a custom deserializer.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L52-L55

* I avoided using HashMaps for things that are actually Vecs. In `bigquery`, I did this because it makes traversal and insertions into the schema simpler. However, most of the manipulation is done within `ast` now, so having an easily traversable schema by path is no longer of concern.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L69-L72",acmiyaguchi,3304040,2019-02-14T22:28:50Z,COLLABORATOR,True,396,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,f3070c91881d08461697a9ead4bc6f243677ff05,Implement deserialize for avro
205,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/29,29,Implement grammar for Avro,"This fixes #14. I learned from the other grammars, making the avro schema cleaner than the other implementations. The spec was useful during this stage.

https://avro.apache.org/docs/current/spec.html

* Unions are implemented using standard serde attributes. In the `jsonschema` module, I left the type as a `Value` and implemented a helper function to disambiguate `Type` from a `Vec<Type>`.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/jsonschema.rs#L79-L85

* I also avoided the issue with flattening a nested enum by treating the enum as an untagged union. I ran into issues treating the main `Type` as a tagged-union and adding the `serde(tag = ""type"")` attribute on all involved types. This will flatten to the correct json format, but it requires custom deserialization. In the `bigquery` module, I had to implement a custom deserializer.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L52-L55

* I avoided using HashMaps for things that are actually Vecs. In `bigquery`, I did this because it makes traversal and insertions into the schema simpler. However, most of the manipulation is done within `ast` now, so having an easily traversable schema by path is no longer of concern.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L69-L72",acmiyaguchi,3304040,2019-02-14T22:28:50Z,COLLABORATOR,True,396,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,48498aab11781e806c9a4073fb49425151d12ac7,Add union type and test for serialize
206,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/29,29,Implement grammar for Avro,"This fixes #14. I learned from the other grammars, making the avro schema cleaner than the other implementations. The spec was useful during this stage.

https://avro.apache.org/docs/current/spec.html

* Unions are implemented using standard serde attributes. In the `jsonschema` module, I left the type as a `Value` and implemented a helper function to disambiguate `Type` from a `Vec<Type>`.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/jsonschema.rs#L79-L85

* I also avoided the issue with flattening a nested enum by treating the enum as an untagged union. I ran into issues treating the main `Type` as a tagged-union and adding the `serde(tag = ""type"")` attribute on all involved types. This will flatten to the correct json format, but it requires custom deserialization. In the `bigquery` module, I had to implement a custom deserializer.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L52-L55

* I avoided using HashMaps for things that are actually Vecs. In `bigquery`, I did this because it makes traversal and insertions into the schema simpler. However, most of the manipulation is done within `ast` now, so having an easily traversable schema by path is no longer of concern.

https://github.com/acmiyaguchi/jsonschema-transpiler/blob/2de4ca494d8488fac1d4041ddcf88d189bbf3671/src/bigquery.rs#L69-L72",acmiyaguchi,3304040,2019-02-14T22:28:50Z,COLLABORATOR,True,396,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,44406df3f3f0cd9e78008609043e98aedd1fa8dd,Add implementation of union for serde
207,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/28,28,Specify version numbers in cargo manifest,This pins dependencies to known versions. ,acmiyaguchi,3304040,2019-02-14T00:18:44Z,COLLABORATOR,True,8,10,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,4c60c03e6e7d64c6dfce7d976ffa02e107e1b5c3,Specify version numbers in cargo manifest
208,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/26,26,Replace `convert_bigquery` with a transpilation path,"This PR is a bit large, but this completes the refactoring of the original `lib.rs` for `convert_bigquery`.

* Updates tests to use `INT64` and `FLOAT64`
* Add post-processing for making the nullable state consistent
  * Fix nested unions that were breaking nullability
* Fixed a bug around `additionalProperties` and `patternProperties` into maps
* Fix various linting things

",acmiyaguchi,3304040,2019-02-13T22:39:29Z,COLLABORATOR,True,248,657,14,Compile JSON Schema into Avro and BigQuery schemas,Rust,f337e29ce93f02ee85c130b24be62a50038dce98,Update tests for standard SQL bigquery
209,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/26,26,Replace `convert_bigquery` with a transpilation path,"This PR is a bit large, but this completes the refactoring of the original `lib.rs` for `convert_bigquery`.

* Updates tests to use `INT64` and `FLOAT64`
* Add post-processing for making the nullable state consistent
  * Fix nested unions that were breaking nullability
* Fixed a bug around `additionalProperties` and `patternProperties` into maps
* Fix various linting things

",acmiyaguchi,3304040,2019-02-13T22:39:29Z,COLLABORATOR,True,248,657,14,Compile JSON Schema into Avro and BigQuery schemas,Rust,7d3e1be9ffc6e36bbf5fadcbe82e86b21051eecc,Fix rules around nullability in ast/bigquery implementation
210,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/26,26,Replace `convert_bigquery` with a transpilation path,"This PR is a bit large, but this completes the refactoring of the original `lib.rs` for `convert_bigquery`.

* Updates tests to use `INT64` and `FLOAT64`
* Add post-processing for making the nullable state consistent
  * Fix nested unions that were breaking nullability
* Fixed a bug around `additionalProperties` and `patternProperties` into maps
* Fix various linting things

",acmiyaguchi,3304040,2019-02-13T22:39:29Z,COLLABORATOR,True,248,657,14,Compile JSON Schema into Avro and BigQuery schemas,Rust,6a67fbae44894c5d237c2860612d593e59183830,Fix nested unions
211,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/26,26,Replace `convert_bigquery` with a transpilation path,"This PR is a bit large, but this completes the refactoring of the original `lib.rs` for `convert_bigquery`.

* Updates tests to use `INT64` and `FLOAT64`
* Add post-processing for making the nullable state consistent
  * Fix nested unions that were breaking nullability
* Fixed a bug around `additionalProperties` and `patternProperties` into maps
* Fix various linting things

",acmiyaguchi,3304040,2019-02-13T22:39:29Z,COLLABORATOR,True,248,657,14,Compile JSON Schema into Avro and BigQuery schemas,Rust,648be316588ca4df15a1bc90f0db489a8f8d9e18,Fix linting from clippy
212,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/26,26,Replace `convert_bigquery` with a transpilation path,"This PR is a bit large, but this completes the refactoring of the original `lib.rs` for `convert_bigquery`.

* Updates tests to use `INT64` and `FLOAT64`
* Add post-processing for making the nullable state consistent
  * Fix nested unions that were breaking nullability
* Fixed a bug around `additionalProperties` and `patternProperties` into maps
* Fix various linting things

",acmiyaguchi,3304040,2019-02-13T22:39:29Z,COLLABORATOR,True,248,657,14,Compile JSON Schema into Avro and BigQuery schemas,Rust,4c60c03e6e7d64c6dfce7d976ffa02e107e1b5c3,Specify version numbers in cargo manifest
213,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/26,26,Replace `convert_bigquery` with a transpilation path,"This PR is a bit large, but this completes the refactoring of the original `lib.rs` for `convert_bigquery`.

* Updates tests to use `INT64` and `FLOAT64`
* Add post-processing for making the nullable state consistent
  * Fix nested unions that were breaking nullability
* Fixed a bug around `additionalProperties` and `patternProperties` into maps
* Fix various linting things

",acmiyaguchi,3304040,2019-02-13T22:39:29Z,COLLABORATOR,True,248,657,14,Compile JSON Schema into Avro and BigQuery schemas,Rust,1384886d3c9dc8e39c95585361d29b3c4ec23fc5,"Merge pull request #28 from acmiyaguchi/cargo

Specify version numbers in cargo manifest"
214,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/26,26,Replace `convert_bigquery` with a transpilation path,"This PR is a bit large, but this completes the refactoring of the original `lib.rs` for `convert_bigquery`.

* Updates tests to use `INT64` and `FLOAT64`
* Add post-processing for making the nullable state consistent
  * Fix nested unions that were breaking nullability
* Fixed a bug around `additionalProperties` and `patternProperties` into maps
* Fix various linting things

",acmiyaguchi,3304040,2019-02-13T22:39:29Z,COLLABORATOR,True,248,657,14,Compile JSON Schema into Avro and BigQuery schemas,Rust,6bb3789431db5ddd0de63910c97bcfd0004c9788,Add full test case and idiomatic construction of vec
215,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/26,26,Replace `convert_bigquery` with a transpilation path,"This PR is a bit large, but this completes the refactoring of the original `lib.rs` for `convert_bigquery`.

* Updates tests to use `INT64` and `FLOAT64`
* Add post-processing for making the nullable state consistent
  * Fix nested unions that were breaking nullability
* Fixed a bug around `additionalProperties` and `patternProperties` into maps
* Fix various linting things

",acmiyaguchi,3304040,2019-02-13T22:39:29Z,COLLABORATOR,True,248,657,14,Compile JSON Schema into Avro and BigQuery schemas,Rust,1dc4e787b70167de13fdbf55c07c5839e960d402,Merge branch 'jsonschema-v2' of github.com:acmiyaguchi/jsonschema-transpiler into jsonschema-v2
216,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/25,25,Implement `From<ast::Tag> for bigquery::Tag` ,"This fixes #13. 

* Adds public visibility for ast structs
* Adds implementation of `From<ast::Tag> for bigquery::Tag`
* Adds tests",acmiyaguchi,3304040,2019-02-12T22:49:09Z,COLLABORATOR,True,209,17,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,fc1bca30f7629d2920498f8ca6dc7a75d1649ab3,Add public visibility to ast structs and run infer_name
217,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/25,25,Implement `From<ast::Tag> for bigquery::Tag` ,"This fixes #13. 

* Adds public visibility for ast structs
* Adds implementation of `From<ast::Tag> for bigquery::Tag`
* Adds tests",acmiyaguchi,3304040,2019-02-12T22:49:09Z,COLLABORATOR,True,209,17,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,809886ef19f4aaf65fd69cc5d52fedcf3aab30a6,Add tests for From<ast::Tag> for bigquery::Tag
218,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/24,24,Refactor tests into submodules,,acmiyaguchi,3304040,2019-02-12T22:45:45Z,COLLABORATOR,True,825,814,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,c1c8bfecbc2ec1976cd37a2099a9d1faf2fb1b99,Rename autogenerated tests to `transpile_*`
219,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/24,24,Refactor tests into submodules,,acmiyaguchi,3304040,2019-02-12T22:45:45Z,COLLABORATOR,True,825,814,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,a3e2eb73579390cd9700c52d23bf40c334055441,Rename `converter` to `jst`
220,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/24,24,Refactor tests into submodules,,acmiyaguchi,3304040,2019-02-12T22:45:45Z,COLLABORATOR,True,825,814,7,Compile JSON Schema into Avro and BigQuery schemas,Rust,d730d2b4a582e7e8b03ce5c5d8beb99f69573d7e,Move tests into tests submodule
221,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/23,23,Collapse Type::Union into a flattened type,"This fixes #19.

This PR adds a few things to the ast module:

* Derive `Clone` for most of the structs
* Implements a `Union::collapse` method which flattens union types
* Implements a `Tag::infer_name` method for adding missing names to objects
* Moves `Into<ast::Tag> for jsonschema::Tag` to `From<jsonschema::Tag> for ast::Tag`, which gives `into` for free. ",acmiyaguchi,3304040,2019-02-11T20:06:59Z,COLLABORATOR,True,407,21,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,8dcfb7e16620dcf1f04dc6d0c97539a02098c30f,Move from Into trait to From trait
222,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/23,23,Collapse Type::Union into a flattened type,"This fixes #19.

This PR adds a few things to the ast module:

* Derive `Clone` for most of the structs
* Implements a `Union::collapse` method which flattens union types
* Implements a `Tag::infer_name` method for adding missing names to objects
* Moves `Into<ast::Tag> for jsonschema::Tag` to `From<jsonschema::Tag> for ast::Tag`, which gives `into` for free. ",acmiyaguchi,3304040,2019-02-11T20:06:59Z,COLLABORATOR,True,407,21,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,ab92f91363e6eeea233c7f71f34692224f38b0a4,Add tests for union collapse
223,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/23,23,Collapse Type::Union into a flattened type,"This fixes #19.

This PR adds a few things to the ast module:

* Derive `Clone` for most of the structs
* Implements a `Union::collapse` method which flattens union types
* Implements a `Tag::infer_name` method for adding missing names to objects
* Moves `Into<ast::Tag> for jsonschema::Tag` to `From<jsonschema::Tag> for ast::Tag`, which gives `into` for free. ",acmiyaguchi,3304040,2019-02-11T20:06:59Z,COLLABORATOR,True,407,21,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,eaa100b558ab28a61e85a8c56027d6faabcd1ca3,Add formatting for jsonschema
224,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/23,23,Collapse Type::Union into a flattened type,"This fixes #19.

This PR adds a few things to the ast module:

* Derive `Clone` for most of the structs
* Implements a `Union::collapse` method which flattens union types
* Implements a `Tag::infer_name` method for adding missing names to objects
* Moves `Into<ast::Tag> for jsonschema::Tag` to `From<jsonschema::Tag> for ast::Tag`, which gives `into` for free. ",acmiyaguchi,3304040,2019-02-11T20:06:59Z,COLLABORATOR,True,407,21,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,a81745e06c697d7fc34853189f694fe568f88239,Add partial implementation for collapsing atoms
225,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/23,23,Collapse Type::Union into a flattened type,"This fixes #19.

This PR adds a few things to the ast module:

* Derive `Clone` for most of the structs
* Implements a `Union::collapse` method which flattens union types
* Implements a `Tag::infer_name` method for adding missing names to objects
* Moves `Into<ast::Tag> for jsonschema::Tag` to `From<jsonschema::Tag> for ast::Tag`, which gives `into` for free. ",acmiyaguchi,3304040,2019-02-11T20:06:59Z,COLLABORATOR,True,407,21,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,b093d7ec718d1ae70d1e747f2ecccf3b96ad60ba,"Add completed implementation of Union::collapse

This required adding the Clone trait to all of the AST types."
226,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/23,23,Collapse Type::Union into a flattened type,"This fixes #19.

This PR adds a few things to the ast module:

* Derive `Clone` for most of the structs
* Implements a `Union::collapse` method which flattens union types
* Implements a `Tag::infer_name` method for adding missing names to objects
* Moves `Into<ast::Tag> for jsonschema::Tag` to `From<jsonschema::Tag> for ast::Tag`, which gives `into` for free. ",acmiyaguchi,3304040,2019-02-11T20:06:59Z,COLLABORATOR,True,407,21,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,2bc7a29be913834aea20707c5f20d77c29169cae,Move tests to the end of the file
227,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/23,23,Collapse Type::Union into a flattened type,"This fixes #19.

This PR adds a few things to the ast module:

* Derive `Clone` for most of the structs
* Implements a `Union::collapse` method which flattens union types
* Implements a `Tag::infer_name` method for adding missing names to objects
* Moves `Into<ast::Tag> for jsonschema::Tag` to `From<jsonschema::Tag> for ast::Tag`, which gives `into` for free. ",acmiyaguchi,3304040,2019-02-11T20:06:59Z,COLLABORATOR,True,407,21,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,560092b5fede93224a0b3cd0019ca3f7fc93cc69,Add recursive case for infer_name
228,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/22,22,Implement `Into<ast::Tag> for jsonschema::Tag`,Fixes #12. ,acmiyaguchi,3304040,2019-02-08T01:10:44Z,COLLABORATOR,True,338,30,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,da6d591481ec4649564c15a9210eddf77d1070b0,Update public interface of ast module
229,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/22,22,Implement `Into<ast::Tag> for jsonschema::Tag`,Fixes #12. ,acmiyaguchi,3304040,2019-02-08T01:10:44Z,COLLABORATOR,True,338,30,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,e578e5bbd7f9db62c1eedc70b32f6a917ae4d2b5,[wip] Add skeleton for into trait for jsonschema
230,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/22,22,Implement `Into<ast::Tag> for jsonschema::Tag`,Fixes #12. ,acmiyaguchi,3304040,2019-02-08T01:10:44Z,COLLABORATOR,True,338,30,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,d1fad32315229de1e4d6fac84c549dfc148fbec8,Add an implementation of Into<ast::Tag> for jsonschema::Tag
231,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/22,22,Implement `Into<ast::Tag> for jsonschema::Tag`,Fixes #12. ,acmiyaguchi,3304040,2019-02-08T01:10:44Z,COLLABORATOR,True,338,30,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,ef8fe8e8a9eb10eb45ac201f0cd58b96a038d36b,Remove unused struct
232,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/22,22,Implement `Into<ast::Tag> for jsonschema::Tag`,Fixes #12. ,acmiyaguchi,3304040,2019-02-08T01:10:44Z,COLLABORATOR,True,338,30,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,e690822226f926716c7fe0b0065c408d820605bb,Fix additionalProperties and add tests for into
233,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/22,22,Implement `Into<ast::Tag> for jsonschema::Tag`,Fixes #12. ,acmiyaguchi,3304040,2019-02-08T01:10:44Z,COLLABORATOR,True,338,30,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,5583daa6a64598779a4a2f6e902f4a0da9c9acc0,Add working implementation of into for jsonschema
234,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,1b7328c97ffa9714ddd55f630926ad83a337d5da,Add initial tests (without shim code for new project)
235,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,83b1eb583f868afee6f725b76a5634820295c408,Format tests with black
236,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,330578804d71a037b647e1bcb34abf135056d1ac,Add basic project structure
237,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,b42e6e820981bd5975d2f8d073010311eb5d3c43,Add the initial commandline tool
238,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,c6eff4a9e58e1426e1661d885b21ac2798ebac7c,Add shims for transpiler targets
239,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,36438c001eefac97787b037342bcada5a4826d17,Remove python code
240,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,9070cded7d16cfdd335e92d78b51c97d492d60e9,Add initial rust code
241,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,9fb2b31ff375d2ea20c786231967efab6a8cca10,Add a few test-cases in a json file
242,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,91adab944d0bccfb022895a9f1f43a2ce0296a2e,Move python tests into a different folder before deleting
243,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,4b73a93840bc8f15eee12f53956c2ff729fc4207,Add test generator from json files
244,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,506b604f64a28cb7e24c313b517fe9e821527223,Update project with failing tests
245,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,2d6f8ac45a3db987e1ca390988a76078c6f97061,Update test schema
246,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,f2172ece8c2ae99fff1267054c5f03b965e0bb5c,Add script for formatting json documents
247,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,393ee47f2921b519bdb3af25a35ffb27c4a61724,Add the rest of the atomic tests
248,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,bce49bca400085db8c8d843a45d0be557501d6d0,Add description to the json tests.
249,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,522229d65fea24ed77bf40589f03c76b58de9a84,Add object tests
250,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,1a5f0f4dd8dae52d84adc3113b7a04d8e930da5c,Add array to tests
251,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,31f9e8d0e3091252fe21b3ac739668ddbe81dcfa,Add map tests
252,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,5c89a1e40ba8e55b8f0ee7d1cba1ecfd0624a09e,Remove simple test from main.rs
253,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,67420a8633d2766a16c5438ab07ba3284e2f0ca3,Add tests for allof
254,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,76fa25a25db1ab3bee584a46bc1c8e8fec7e63ab,Add tests for oneof
255,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,63091c57e2948e960736c4ab9fd4003ef95a05c4,Remove python tests
256,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,fa07542abeca5705f8698bddb815c084ded48291,Fix indentation of generated tests
257,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,9cd3a5489b6fd678ae6eb7f51e41032202a4ce9c,"Merge pull request #1 from acmiyaguchi/rust

Add programmatic test generation in rust"
258,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,2ebfd872c367832bba8b7257a86df2765dbf0499,Update README.md
259,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,7d3ec35131e6ecba3004a50406b52f3ccdcf5f0e,Match based on type
260,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,eb648154ecc6c772a80517f197468871b137d641,Namespace tests with avro
261,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,dfd7c1d14716e6bb9138b0f8a5b3d7a07b0ad5d3,Add stubs for bigquery tests
262,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,d9d2821dfe53f3ccdabbd5b53abeb53fc69b0bf0,Add bigquery tests
263,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/21,21,Initial release,"This PR would make the main branch master, after which I'll move the repository into the mozilla organization. The largest piece I'd like to see is more robust error handling, but the code here is stable enough to plumb mozilla-pipeline-schemas into BigQuery and successfully load data.

Refactoring will be a task of its own, but hopefully the test suite here will make it easier to transition to something with a saner error-handling interface that's more configurable. ",acmiyaguchi,3304040,2019-02-07T23:59:19Z,COLLABORATOR,True,8563,14,34,Compile JSON Schema into Avro and BigQuery schemas,Rust,42412c52e6fe6e205b85ec11e793215591d4ec02,Rename module
264,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/20,20,Fix #16: Add Union type to ast::Type,Fixes #16. This adds a union type to the AST that can be mapped to the JSON Schema's oneOf construct. This also adds defaults to the data structure. ,acmiyaguchi,3304040,2019-02-07T23:20:53Z,COLLABORATOR,True,44,6,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,b0e189d784315e4dfd20b17029cc6426f11a71cf,Implement defaults for ast::Tag
265,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/20,20,Fix #16: Add Union type to ast::Type,Fixes #16. This adds a union type to the AST that can be mapped to the JSON Schema's oneOf construct. This also adds defaults to the data structure. ,acmiyaguchi,3304040,2019-02-07T23:20:53Z,COLLABORATOR,True,44,6,1,Compile JSON Schema into Avro and BigQuery schemas,Rust,23e3aeda0bf7340e515da036c2c1d2a8c800ad1f,Add ast::Type::Union
266,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/11,11,Add grammar for JSONSchema,"This adds serde for JSONSchema that can be used to transform into and out of the ast module.

I referenced the JSON Schema v4 meta-schema when building up the grammar: http://json-schema.org/draft-04/schema. It provides a useful skeleton for how to create the data structure.

There are some interesting bits to serde attributes that were also pain points -- I'll point them out in a self-review.
",acmiyaguchi,3304040,2019-02-07T21:21:01Z,COLLABORATOR,True,236,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,7dfd88fad24cab6976c7b9d480b8cddd1a7fe284,Add initial jsonschema serde module
267,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/11,11,Add grammar for JSONSchema,"This adds serde for JSONSchema that can be used to transform into and out of the ast module.

I referenced the JSON Schema v4 meta-schema when building up the grammar: http://json-schema.org/draft-04/schema. It provides a useful skeleton for how to create the data structure.

There are some interesting bits to serde attributes that were also pain points -- I'll point them out in a self-review.
",acmiyaguchi,3304040,2019-02-07T21:21:01Z,COLLABORATOR,True,236,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,9887dd4a2a761b71941db60bf9e41c59678a88fc,[wip] Add broke simple and multitype serde
268,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/11,11,Add grammar for JSONSchema,"This adds serde for JSONSchema that can be used to transform into and out of the ast module.

I referenced the JSON Schema v4 meta-schema when building up the grammar: http://json-schema.org/draft-04/schema. It provides a useful skeleton for how to create the data structure.

There are some interesting bits to serde attributes that were also pain points -- I'll point them out in a self-review.
",acmiyaguchi,3304040,2019-02-07T21:21:01Z,COLLABORATOR,True,236,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,cbe039c2fa6b2dd494a8bf53f29e764df5ab50f7,Add test stubs and skeleton for main jsonschema implementation
269,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/11,11,Add grammar for JSONSchema,"This adds serde for JSONSchema that can be used to transform into and out of the ast module.

I referenced the JSON Schema v4 meta-schema when building up the grammar: http://json-schema.org/draft-04/schema. It provides a useful skeleton for how to create the data structure.

There are some interesting bits to serde attributes that were also pain points -- I'll point them out in a self-review.
",acmiyaguchi,3304040,2019-02-07T21:21:01Z,COLLABORATOR,True,236,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,35a01f1fee8263e5732a7d7ade7943aee8418cc8,Add deserialization tests and comments for jsonschema
270,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/11,11,Add grammar for JSONSchema,"This adds serde for JSONSchema that can be used to transform into and out of the ast module.

I referenced the JSON Schema v4 meta-schema when building up the grammar: http://json-schema.org/draft-04/schema. It provides a useful skeleton for how to create the data structure.

There are some interesting bits to serde attributes that were also pain points -- I'll point them out in a self-review.
",acmiyaguchi,3304040,2019-02-07T21:21:01Z,COLLABORATOR,True,236,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,f710ca54aea863a64e914135d4fd9e3fac7bfad6,Fix type in jsonschema:: tests
271,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/11,11,Add grammar for JSONSchema,"This adds serde for JSONSchema that can be used to transform into and out of the ast module.

I referenced the JSON Schema v4 meta-schema when building up the grammar: http://json-schema.org/draft-04/schema. It provides a useful skeleton for how to create the data structure.

There are some interesting bits to serde attributes that were also pain points -- I'll point them out in a self-review.
",acmiyaguchi,3304040,2019-02-07T21:21:01Z,COLLABORATOR,True,236,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,8f5f53b241c541e44676f4704192951c952c5ac2,Make extra non-optional
272,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/11,11,Add grammar for JSONSchema,"This adds serde for JSONSchema that can be used to transform into and out of the ast module.

I referenced the JSON Schema v4 meta-schema when building up the grammar: http://json-schema.org/draft-04/schema. It provides a useful skeleton for how to create the data structure.

There are some interesting bits to serde attributes that were also pain points -- I'll point them out in a self-review.
",acmiyaguchi,3304040,2019-02-07T21:21:01Z,COLLABORATOR,True,236,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,a32ff41f869c5f08a30127c3d5de8b080cf195e1,Add required to object
273,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/11,11,Add grammar for JSONSchema,"This adds serde for JSONSchema that can be used to transform into and out of the ast module.

I referenced the JSON Schema v4 meta-schema when building up the grammar: http://json-schema.org/draft-04/schema. It provides a useful skeleton for how to create the data structure.

There are some interesting bits to serde attributes that were also pain points -- I'll point them out in a self-review.
",acmiyaguchi,3304040,2019-02-07T21:21:01Z,COLLABORATOR,True,236,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,37a2e45bde5827832a256b47009fbd7fac6cf83d,Add module docs
274,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/10,10,Add a grammar and serde for bigquery,"This adds a grammar for BigQuery. There are a couple noteworthy bits in this file.

* Using `#[serde(tag=""type"")]` on all enums does the correct thing when serializing. This saved time writing a custom serializer.
* The inverse is not true, so there is a custom deserializer for the `Type` enum. This is to handle the case `{""type"": ""INT64""}` and the ilk -- this is actually a `Type::Atom(Atom::Int64)`. The deserializer first tries to deserialize as an atom, and then matches against the other types.
* There is a custom serializer/deserializer for the `Record` struct. For ergonomics, I've treated the BigQuery field as a HashMap instead of a Vector. This assumption breaks if the type does not have a name...

",acmiyaguchi,3304040,2019-02-06T00:57:20Z,COLLABORATOR,True,293,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,1bca2d431d2f79a0e05827f15b0fca214d8e81db,Add grammar for bigquery
275,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/10,10,Add a grammar and serde for bigquery,"This adds a grammar for BigQuery. There are a couple noteworthy bits in this file.

* Using `#[serde(tag=""type"")]` on all enums does the correct thing when serializing. This saved time writing a custom serializer.
* The inverse is not true, so there is a custom deserializer for the `Type` enum. This is to handle the case `{""type"": ""INT64""}` and the ilk -- this is actually a `Type::Atom(Atom::Int64)`. The deserializer first tries to deserialize as an atom, and then matches against the other types.
* There is a custom serializer/deserializer for the `Record` struct. For ergonomics, I've treated the BigQuery field as a HashMap instead of a Vector. This assumption breaks if the type does not have a name...

",acmiyaguchi,3304040,2019-02-06T00:57:20Z,COLLABORATOR,True,293,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,91a0e64810597ecfcac4c442f46071b58a83edb7,Add a custom deserializer for bigquery fields
276,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/10,10,Add a grammar and serde for bigquery,"This adds a grammar for BigQuery. There are a couple noteworthy bits in this file.

* Using `#[serde(tag=""type"")]` on all enums does the correct thing when serializing. This saved time writing a custom serializer.
* The inverse is not true, so there is a custom deserializer for the `Type` enum. This is to handle the case `{""type"": ""INT64""}` and the ilk -- this is actually a `Type::Atom(Atom::Int64)`. The deserializer first tries to deserialize as an atom, and then matches against the other types.
* There is a custom serializer/deserializer for the `Record` struct. For ergonomics, I've treated the BigQuery field as a HashMap instead of a Vector. This assumption breaks if the type does not have a name...

",acmiyaguchi,3304040,2019-02-06T00:57:20Z,COLLABORATOR,True,293,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,b6d12cb709dc500657da09e465ee3b7259b0db77,Replace type-signature comments with link to the documentation
277,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/10,10,Add a grammar and serde for bigquery,"This adds a grammar for BigQuery. There are a couple noteworthy bits in this file.

* Using `#[serde(tag=""type"")]` on all enums does the correct thing when serializing. This saved time writing a custom serializer.
* The inverse is not true, so there is a custom deserializer for the `Type` enum. This is to handle the case `{""type"": ""INT64""}` and the ilk -- this is actually a `Type::Atom(Atom::Int64)`. The deserializer first tries to deserialize as an atom, and then matches against the other types.
* There is a custom serializer/deserializer for the `Record` struct. For ergonomics, I've treated the BigQuery field as a HashMap instead of a Vector. This assumption breaks if the type does not have a name...

",acmiyaguchi,3304040,2019-02-06T00:57:20Z,COLLABORATOR,True,293,0,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,35bcd96dd631ddbc951336a9b7b5f8e8d6257f0c,Add custom deserializer for BigQuery
278,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/9,9,Add an abstract schema tree ,"This adds an intermediate AST to transform into and from. This captures enough information to be useful for constructing both avro and bigquery schemas. 

There are some other changes that are tagging along due to refactoring:

* A change caught by `cargo fmt`
* Minor clean-up in `convert_bigquery`",acmiyaguchi,3304040,2019-02-05T01:57:29Z,COLLABORATOR,True,230,7,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,217240bd22968fcfc6487f465d627cbc57430d8f,Refactor array into a seperate function
279,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/9,9,Add an abstract schema tree ,"This adds an intermediate AST to transform into and from. This captures enough information to be useful for constructing both avro and bigquery schemas. 

There are some other changes that are tagging along due to refactoring:

* A change caught by `cargo fmt`
* Minor clean-up in `convert_bigquery`",acmiyaguchi,3304040,2019-02-05T01:57:29Z,COLLABORATOR,True,230,7,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,5ae5c284d5f578276dd9f745a3e75dd29082e5f5,Add an abstract schema tree for processing
280,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/9,9,Add an abstract schema tree ,"This adds an intermediate AST to transform into and from. This captures enough information to be useful for constructing both avro and bigquery schemas. 

There are some other changes that are tagging along due to refactoring:

* A change caught by `cargo fmt`
* Minor clean-up in `convert_bigquery`",acmiyaguchi,3304040,2019-02-05T01:57:29Z,COLLABORATOR,True,230,7,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,0fed0a9597e917da71234a4664773e312accc1b1,Add a barebones AST with proper serialization
281,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/9,9,Add an abstract schema tree ,"This adds an intermediate AST to transform into and from. This captures enough information to be useful for constructing both avro and bigquery schemas. 

There are some other changes that are tagging along due to refactoring:

* A change caught by `cargo fmt`
* Minor clean-up in `convert_bigquery`",acmiyaguchi,3304040,2019-02-05T01:57:29Z,COLLABORATOR,True,230,7,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,e2979016fb0c8f0702e8c0de7361a4ac3777a607,Use `test-*` pattern for arbitrary strings
282,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/9,9,Add an abstract schema tree ,"This adds an intermediate AST to transform into and from. This captures enough information to be useful for constructing both avro and bigquery schemas. 

There are some other changes that are tagging along due to refactoring:

* A change caught by `cargo fmt`
* Minor clean-up in `convert_bigquery`",acmiyaguchi,3304040,2019-02-05T01:57:29Z,COLLABORATOR,True,230,7,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,dd768934160b3d4f1a108e7a3ef71be671da08ce,Add an Atom type
283,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/9,9,Add an abstract schema tree ,"This adds an intermediate AST to transform into and from. This captures enough information to be useful for constructing both avro and bigquery schemas. 

There are some other changes that are tagging along due to refactoring:

* A change caught by `cargo fmt`
* Minor clean-up in `convert_bigquery`",acmiyaguchi,3304040,2019-02-05T01:57:29Z,COLLABORATOR,True,230,7,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,c5a4be142e94281452b6bbcfa19011a117826512,Test array type behavior
284,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/9,9,Add an abstract schema tree ,"This adds an intermediate AST to transform into and from. This captures enough information to be useful for constructing both avro and bigquery schemas. 

There are some other changes that are tagging along due to refactoring:

* A change caught by `cargo fmt`
* Minor clean-up in `convert_bigquery`",acmiyaguchi,3304040,2019-02-05T01:57:29Z,COLLABORATOR,True,230,7,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,067e2f1d3e4fec50b302e74fe3086d69658342e1,Change Fields to Tag type
285,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/9,9,Add an abstract schema tree ,"This adds an intermediate AST to transform into and from. This captures enough information to be useful for constructing both avro and bigquery schemas. 

There are some other changes that are tagging along due to refactoring:

* A change caught by `cargo fmt`
* Minor clean-up in `convert_bigquery`",acmiyaguchi,3304040,2019-02-05T01:57:29Z,COLLABORATOR,True,230,7,3,Compile JSON Schema into Avro and BigQuery schemas,Rust,78e230f1bce57a753bc2711c3701a02ada202a2e,Add Null type
286,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/8,8,Add a command-line interface via clap,"This uses https://github.com/clap-rs/clap to add a minimal command line interface. The program will ideally work like `jq` by reading from an IO stream and writing out to stdout.

From `cargo run -- --help`:

```
amiyaguchi-vxjgh5:jsonschema-transpiler amiyaguchi$ cargo run -- --help
   Compiling jsonschema_transpiler v0.1.0 (/Users/amiyaguchi/Work/jsonschema-transpiler)
    Finished dev [unoptimized + debuginfo] target(s) in 1.88s
     Running `target/debug/jsonschema_transpiler --help`
jst 0.1
Anthony Miyaguchi <amiyaguchi@mozilla.com>

USAGE:
    jsonschema_transpiler [OPTIONS]

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

OPTIONS:
    -f, --from-file <FILE>
```",acmiyaguchi,3304040,2019-02-04T20:17:21Z,COLLABORATOR,True,154,21,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,59fb0946374644e82866a08502ed7a3b4be7c394,Add clap and initial app to project
287,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/8,8,Add a command-line interface via clap,"This uses https://github.com/clap-rs/clap to add a minimal command line interface. The program will ideally work like `jq` by reading from an IO stream and writing out to stdout.

From `cargo run -- --help`:

```
amiyaguchi-vxjgh5:jsonschema-transpiler amiyaguchi$ cargo run -- --help
   Compiling jsonschema_transpiler v0.1.0 (/Users/amiyaguchi/Work/jsonschema-transpiler)
    Finished dev [unoptimized + debuginfo] target(s) in 1.88s
     Running `target/debug/jsonschema_transpiler --help`
jst 0.1
Anthony Miyaguchi <amiyaguchi@mozilla.com>

USAGE:
    jsonschema_transpiler [OPTIONS]

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

OPTIONS:
    -f, --from-file <FILE>
```",acmiyaguchi,3304040,2019-02-04T20:17:21Z,COLLABORATOR,True,154,21,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,e428f706a28cb30d7b0a56f5ab03bf93bbb32793,Ignore a top-level data folder for testing
288,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/8,8,Add a command-line interface via clap,"This uses https://github.com/clap-rs/clap to add a minimal command line interface. The program will ideally work like `jq` by reading from an IO stream and writing out to stdout.

From `cargo run -- --help`:

```
amiyaguchi-vxjgh5:jsonschema-transpiler amiyaguchi$ cargo run -- --help
   Compiling jsonschema_transpiler v0.1.0 (/Users/amiyaguchi/Work/jsonschema-transpiler)
    Finished dev [unoptimized + debuginfo] target(s) in 1.88s
     Running `target/debug/jsonschema_transpiler --help`
jst 0.1
Anthony Miyaguchi <amiyaguchi@mozilla.com>

USAGE:
    jsonschema_transpiler [OPTIONS]

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

OPTIONS:
    -f, --from-file <FILE>
```",acmiyaguchi,3304040,2019-02-04T20:17:21Z,COLLABORATOR,True,154,21,4,Compile JSON Schema into Avro and BigQuery schemas,Rust,f3792f0dbd23e9ce9602a16dda255ca48446d62b,Read from file and print formatted schemas to stdout
289,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/7,7,Remove allOf tests as unimplemented edge-case,"`allOf` is an infrequently used keyword that doesn't add much structural expressiveness when converting into BigQuery. Here, the tests are removed because this pattern isn't found in mozilla-pipeline-schemas.",acmiyaguchi,3304040,2019-02-04T18:38:49Z,COLLABORATOR,False,0,135,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,ffd5ea6d9b78091a666932aa37011bd420f0d78f,Remove allOf tests as unimplemented edge-case
290,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,a1eb0f7ab1dae804caa658d2b1f4b032aced58dd,Refactor individual cases into functions for clarity
291,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,25b9b810cabe59fb37c9c0268701b8a89565b026,Add support for arrays
292,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,5c1c29e8072dfd52280a9651400267e13abcbfe9,Add initial conflict-resolution for oneOf
293,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,5ff7297c0927fe625d65d63d576f83f0f92e7c97,Add a serializable struct for creating a BigQuery record
294,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,fb7018f72dad848496720fcd7ec9d273ec6c2997,Add name as a required field
295,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,3acd573311182a35c101117ff22753740cce4189,[wip] Add implementation for a BigQueryRecord builder
296,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,02021afa65aac0e931749a371700604d5ec638a5,Implement BigQuery record building using insert
297,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,9f6b0ce8becc67a6f7416dbd646436d32d7606ba,Add implementation of handling oneOf
298,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,ca494c2abcbfc0df208526d4db492f13f0056461,[wip] Refactoring main transformation function
299,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,2c618c8ab342480c015eb0bc63b9c6b5a0c77aa1,[WIP] Use proper struct and enum for processing json schema
300,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,ba436afa76955ad0dabab287a2109ef86119071a,Fix unused warnings
301,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,a833da65ff6cb5e6ba147227f7c9112efbbb0855,Implement main method for transpiling
302,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,1d34472074562c14381dd3db4c5ac0ed56b0a1bc,Fix general oneOf issues
303,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,bf1dd9cc580800bf213d8a6d67144bf3e3f51e02,Fix nullable oneOf case
304,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,194e2dd1dd34b15a787c96fc586ba966a300e86b,Fix consistency checking due to broken insertions
305,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,61e2bb6694d4e75fe910cb0f42c2b8d0cda75f8f,Remove avro conversion for refactor
306,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,b2853002ba367082be278704a063e6b64650c484,Remove unused code and rename main conversion method
307,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/6,6,Add implementation of `convert_bigquery`,"This adds a mostly complete implementation of the `json_to_bigquery` converted that can be found [here](https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py). The `allOf` case is currently not handled, but it not a case that is prevalent in mozilla-pipeline-schemas.

I removed the avro tests, which will be handled once the command-line interface is added to this project.",acmiyaguchi,3304040,2019-02-02T00:53:06Z,COLLABORATOR,True,564,1190,6,Compile JSON Schema into Avro and BigQuery schemas,Rust,77be03cf265aabd2b007bdfd796c8c801024fb61,Refactor library for reading
308,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/5,5,Refactor generated tests into a single file,"The output of the generated tests with multiple files is hard to reason about. Instead, all of the test cases are in a single file instead of being split across multiple files.",acmiyaguchi,3304040,2019-01-30T23:46:53Z,COLLABORATOR,True,2108,2144,15,Compile JSON Schema into Avro and BigQuery schemas,Rust,2dd57ad376da960230704681cb8be99864ff873a,Refactor generated tests into a single file
309,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/4,4,Handle object conversion,,acmiyaguchi,3304040,2019-01-25T23:48:08Z,COLLABORATOR,True,88,6,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,20678b29ca416d196e9be1a4b18fbd9b50b603f1,Add match statement for handling objects
310,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/4,4,Handle object conversion,,acmiyaguchi,3304040,2019-01-25T23:48:08Z,COLLABORATOR,True,88,6,2,Compile JSON Schema into Avro and BigQuery schemas,Rust,51c9b98f2bec2e56e2caad15c6f995db022dd386,Add support for object conversion
311,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/3,3,Handle simple types when converting to BigQuery schema,"This passes the `bigquery_atomic` test suite, run with the following command:

`cargo test --no-fail-fast bigquery`

This also runs all of the other tests, most of which are failing. 

---

The most significant hurdle here is working with the borrow checker. When recursively generating values, the values must be cloned/moved during the matching process in order to prevent a ""use after free"" or ""variable out of scope"" issue. My limited fluency with the standard library means that there are quite a few calls to `.to_owned()` and `.into()`.

https://doc.rust-lang.org/std/vec/struct.Vec.html
https://doc.rust-lang.org/rust-by-example/primitives/tuples.html
https://users.rust-lang.org/t/to-string-vs-to-owned-for-string-literals/1441
https://doc.rust-lang.org/std/collections/struct.HashSet.html",acmiyaguchi,3304040,2019-01-24T01:02:49Z,COLLABORATOR,True,84,30,8,Compile JSON Schema into Avro and BigQuery schemas,Rust,0c2708f21c41fbd8af1c6f90c8d03ca5eacfa4e7,Add type mapping for bq conversion
312,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/3,3,Handle simple types when converting to BigQuery schema,"This passes the `bigquery_atomic` test suite, run with the following command:

`cargo test --no-fail-fast bigquery`

This also runs all of the other tests, most of which are failing. 

---

The most significant hurdle here is working with the borrow checker. When recursively generating values, the values must be cloned/moved during the matching process in order to prevent a ""use after free"" or ""variable out of scope"" issue. My limited fluency with the standard library means that there are quite a few calls to `.to_owned()` and `.into()`.

https://doc.rust-lang.org/std/vec/struct.Vec.html
https://doc.rust-lang.org/rust-by-example/primitives/tuples.html
https://users.rust-lang.org/t/to-string-vs-to-owned-for-string-literals/1441
https://doc.rust-lang.org/std/collections/struct.HashSet.html",acmiyaguchi,3304040,2019-01-24T01:02:49Z,COLLABORATOR,True,84,30,8,Compile JSON Schema into Avro and BigQuery schemas,Rust,115945112c1e70015ebf17f9e378dd3f6670028b,Handle multi-types
313,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/3,3,Handle simple types when converting to BigQuery schema,"This passes the `bigquery_atomic` test suite, run with the following command:

`cargo test --no-fail-fast bigquery`

This also runs all of the other tests, most of which are failing. 

---

The most significant hurdle here is working with the borrow checker. When recursively generating values, the values must be cloned/moved during the matching process in order to prevent a ""use after free"" or ""variable out of scope"" issue. My limited fluency with the standard library means that there are quite a few calls to `.to_owned()` and `.into()`.

https://doc.rust-lang.org/std/vec/struct.Vec.html
https://doc.rust-lang.org/rust-by-example/primitives/tuples.html
https://users.rust-lang.org/t/to-string-vs-to-owned-for-string-literals/1441
https://doc.rust-lang.org/std/collections/struct.HashSet.html",acmiyaguchi,3304040,2019-01-24T01:02:49Z,COLLABORATOR,True,84,30,8,Compile JSON Schema into Avro and BigQuery schemas,Rust,ecf5033b523909f144ec51da8f14f4714fd4f9db,Pass 3 of the initial tests
314,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/3,3,Handle simple types when converting to BigQuery schema,"This passes the `bigquery_atomic` test suite, run with the following command:

`cargo test --no-fail-fast bigquery`

This also runs all of the other tests, most of which are failing. 

---

The most significant hurdle here is working with the borrow checker. When recursively generating values, the values must be cloned/moved during the matching process in order to prevent a ""use after free"" or ""variable out of scope"" issue. My limited fluency with the standard library means that there are quite a few calls to `.to_owned()` and `.into()`.

https://doc.rust-lang.org/std/vec/struct.Vec.html
https://doc.rust-lang.org/rust-by-example/primitives/tuples.html
https://users.rust-lang.org/t/to-string-vs-to-owned-for-string-literals/1441
https://doc.rust-lang.org/std/collections/struct.HashSet.html",acmiyaguchi,3304040,2019-01-24T01:02:49Z,COLLABORATOR,True,84,30,8,Compile JSON Schema into Avro and BigQuery schemas,Rust,82e6305e78ca7a634fcb72ba95bf0395f84b37e8,Refactor to handle multi-types properly
315,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/2,2,Add BigQuery tests,"This adds BigQuery tests by updating the build script.

I ran into issues when re-using the deserialized data-structure related to borrow checking. Initially, I tried using the `#[serde(borrow)]` flag on the nested structs, but this doesn't work because the copy trait needs to be implemented. I borrow the struct in the functions instead of moving them and copy the serde_json Value's at the leaf of the document.

This commit also contains minor changes to the avro conversion function. This may not be a sustainable path forward -- instead I'll be focused on implementing most of the functionality found in the following script.

https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py

Finally, there is a minor update related to the renaming of this module. There won't be too many Telemetry specific additions (e.g. compiling and testing against mozilla-pipeline-schemas) until the base functionality is done here.",acmiyaguchi,3304040,2019-01-23T00:51:09Z,COLLABORATOR,True,1204,66,17,Compile JSON Schema into Avro and BigQuery schemas,Rust,7d3ec35131e6ecba3004a50406b52f3ccdcf5f0e,Match based on type
316,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/2,2,Add BigQuery tests,"This adds BigQuery tests by updating the build script.

I ran into issues when re-using the deserialized data-structure related to borrow checking. Initially, I tried using the `#[serde(borrow)]` flag on the nested structs, but this doesn't work because the copy trait needs to be implemented. I borrow the struct in the functions instead of moving them and copy the serde_json Value's at the leaf of the document.

This commit also contains minor changes to the avro conversion function. This may not be a sustainable path forward -- instead I'll be focused on implementing most of the functionality found in the following script.

https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py

Finally, there is a minor update related to the renaming of this module. There won't be too many Telemetry specific additions (e.g. compiling and testing against mozilla-pipeline-schemas) until the base functionality is done here.",acmiyaguchi,3304040,2019-01-23T00:51:09Z,COLLABORATOR,True,1204,66,17,Compile JSON Schema into Avro and BigQuery schemas,Rust,eb648154ecc6c772a80517f197468871b137d641,Namespace tests with avro
317,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/2,2,Add BigQuery tests,"This adds BigQuery tests by updating the build script.

I ran into issues when re-using the deserialized data-structure related to borrow checking. Initially, I tried using the `#[serde(borrow)]` flag on the nested structs, but this doesn't work because the copy trait needs to be implemented. I borrow the struct in the functions instead of moving them and copy the serde_json Value's at the leaf of the document.

This commit also contains minor changes to the avro conversion function. This may not be a sustainable path forward -- instead I'll be focused on implementing most of the functionality found in the following script.

https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py

Finally, there is a minor update related to the renaming of this module. There won't be too many Telemetry specific additions (e.g. compiling and testing against mozilla-pipeline-schemas) until the base functionality is done here.",acmiyaguchi,3304040,2019-01-23T00:51:09Z,COLLABORATOR,True,1204,66,17,Compile JSON Schema into Avro and BigQuery schemas,Rust,dfd7c1d14716e6bb9138b0f8a5b3d7a07b0ad5d3,Add stubs for bigquery tests
318,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/2,2,Add BigQuery tests,"This adds BigQuery tests by updating the build script.

I ran into issues when re-using the deserialized data-structure related to borrow checking. Initially, I tried using the `#[serde(borrow)]` flag on the nested structs, but this doesn't work because the copy trait needs to be implemented. I borrow the struct in the functions instead of moving them and copy the serde_json Value's at the leaf of the document.

This commit also contains minor changes to the avro conversion function. This may not be a sustainable path forward -- instead I'll be focused on implementing most of the functionality found in the following script.

https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py

Finally, there is a minor update related to the renaming of this module. There won't be too many Telemetry specific additions (e.g. compiling and testing against mozilla-pipeline-schemas) until the base functionality is done here.",acmiyaguchi,3304040,2019-01-23T00:51:09Z,COLLABORATOR,True,1204,66,17,Compile JSON Schema into Avro and BigQuery schemas,Rust,d9d2821dfe53f3ccdabbd5b53abeb53fc69b0bf0,Add bigquery tests
319,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/2,2,Add BigQuery tests,"This adds BigQuery tests by updating the build script.

I ran into issues when re-using the deserialized data-structure related to borrow checking. Initially, I tried using the `#[serde(borrow)]` flag on the nested structs, but this doesn't work because the copy trait needs to be implemented. I borrow the struct in the functions instead of moving them and copy the serde_json Value's at the leaf of the document.

This commit also contains minor changes to the avro conversion function. This may not be a sustainable path forward -- instead I'll be focused on implementing most of the functionality found in the following script.

https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py

Finally, there is a minor update related to the renaming of this module. There won't be too many Telemetry specific additions (e.g. compiling and testing against mozilla-pipeline-schemas) until the base functionality is done here.",acmiyaguchi,3304040,2019-01-23T00:51:09Z,COLLABORATOR,True,1204,66,17,Compile JSON Schema into Avro and BigQuery schemas,Rust,42412c52e6fe6e205b85ec11e793215591d4ec02,Rename module
320,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/2,2,Add BigQuery tests,"This adds BigQuery tests by updating the build script.

I ran into issues when re-using the deserialized data-structure related to borrow checking. Initially, I tried using the `#[serde(borrow)]` flag on the nested structs, but this doesn't work because the copy trait needs to be implemented. I borrow the struct in the functions instead of moving them and copy the serde_json Value's at the leaf of the document.

This commit also contains minor changes to the avro conversion function. This may not be a sustainable path forward -- instead I'll be focused on implementing most of the functionality found in the following script.

https://github.com/relud/telemetry-sample/blob/master/scripts/jsonschema_to_bigquery.py

Finally, there is a minor update related to the renaming of this module. There won't be too many Telemetry specific additions (e.g. compiling and testing against mozilla-pipeline-schemas) until the base functionality is done here.",acmiyaguchi,3304040,2019-01-23T00:51:09Z,COLLABORATOR,True,1204,66,17,Compile JSON Schema into Avro and BigQuery schemas,Rust,afc3edb9825570d47f82757adeafa002fe2f9927,Fixed typo from pasting
321,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,36438c001eefac97787b037342bcada5a4826d17,Remove python code
322,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,9070cded7d16cfdd335e92d78b51c97d492d60e9,Add initial rust code
323,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,9fb2b31ff375d2ea20c786231967efab6a8cca10,Add a few test-cases in a json file
324,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,91adab944d0bccfb022895a9f1f43a2ce0296a2e,Move python tests into a different folder before deleting
325,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,4b73a93840bc8f15eee12f53956c2ff729fc4207,Add test generator from json files
326,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,506b604f64a28cb7e24c313b517fe9e821527223,Update project with failing tests
327,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,2d6f8ac45a3db987e1ca390988a76078c6f97061,Update test schema
328,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,f2172ece8c2ae99fff1267054c5f03b965e0bb5c,Add script for formatting json documents
329,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,393ee47f2921b519bdb3af25a35ffb27c4a61724,Add the rest of the atomic tests
330,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,bce49bca400085db8c8d843a45d0be557501d6d0,Add description to the json tests.
331,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,522229d65fea24ed77bf40589f03c76b58de9a84,Add object tests
332,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,1a5f0f4dd8dae52d84adc3113b7a04d8e930da5c,Add array to tests
333,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,31f9e8d0e3091252fe21b3ac739668ddbe81dcfa,Add map tests
334,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,5c89a1e40ba8e55b8f0ee7d1cba1ecfd0624a09e,Remove simple test from main.rs
335,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,67420a8633d2766a16c5438ab07ba3284e2f0ca3,Add tests for allof
336,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,76fa25a25db1ab3bee584a46bc1c8e8fec7e63ab,Add tests for oneof
337,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,63091c57e2948e960736c4ab9fd4003ef95a05c4,Remove python tests
338,https://api.github.com/repos/mozilla/jsonschema-transpiler/pulls/1,1,Add programmatic test generation in rust,"This PR programmatically generates tests in Rust, reading tests from JSON. A temporary method that passes a few tests is included as a skeleton.",acmiyaguchi,3304040,2019-01-18T01:22:38Z,COLLABORATOR,True,2584,564,33,Compile JSON Schema into Avro and BigQuery schemas,Rust,fa07542abeca5705f8698bddb815c084ded48291,Fix indentation of generated tests
