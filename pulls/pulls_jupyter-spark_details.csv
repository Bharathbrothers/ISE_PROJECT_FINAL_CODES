,pullid,pulls_number,pulltitle,pullsbody,pullsuserlogin,pullsuserid,pullauthordate,author_association,merged_status,stats_addns,stats_delns,stats_changed_files,pull_repo_desc,pull_repo_lang,pull_commit_sha,pull_commit_message
0,https://api.github.com/repos/mozilla/jupyter-spark/pulls/54,54,Pin tornado version at < 6,"Resolves the error ""AttributeError: module 'tornado.web' has no attribute 'asynchronous'"", which is breaking the Travis build.",chmreid,53452777,2019-12-19T22:06:57Z,NONE,False,1,1,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,1af3b292ad2b5ee7971db5a5dfec3bfd40027698,pin tornado version at < 6
1,https://api.github.com/repos/mozilla/jupyter-spark/pulls/51,51,Add Mozilla Code of Conduct,"Fixes #50


As of January 1 2019, Mozilla requires that all GitHub projects include this [CODE_OF_CONDUCT.md](https://github.com/mozilla/repo-templates/blob/master/templates/CODE_OF_CONDUCT.md) file in the project root. The file has two parts:

1. Required Text - All text under the headings *Community Participation Guidelines and How to Report*, are required, and should not be altered.
2. Optional Text - The Project Specific Etiquette heading provides a space to speak more specifically about ways people can work effectively and inclusively together. Some examples of those can be found on the [Firefox Debugger](https://github.com/devtools-html/debugger.html/blob/master/CODE_OF_CONDUCT.md) project, and [Common Voice](https://github.com/mozilla/voice-web/blob/master/CODE_OF_CONDUCT.md). (The optional part is commented out in the [raw template file](https://raw.githubusercontent.com/mozilla/repo-templates/blob/master/templates/CODE_OF_CONDUCT.md), and will not be visible until you modify and uncomment that part.)

If you have any questions about this file, or Code of Conduct policies and procedures, please see [Mozilla-GitHub-Standards](https://wiki.mozilla.org/GitHub/Repository_Requirements) or email Mozilla-GitHub-Standards+CoC@mozilla.com.

_(Message COC002)_",Mozilla-GitHub-Standards,48073334,2019-03-29T17:10:03Z,CONTRIBUTOR,True,8,0,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,ba3c2e569b650259537e2d9584f4b404fd878e5e,"Add Mozilla Code of Conduct file

Fixes #50.

_(Message COC002)_"
2,https://api.github.com/repos/mozilla/jupyter-spark/pulls/51,51,Add Mozilla Code of Conduct,"Fixes #50


As of January 1 2019, Mozilla requires that all GitHub projects include this [CODE_OF_CONDUCT.md](https://github.com/mozilla/repo-templates/blob/master/templates/CODE_OF_CONDUCT.md) file in the project root. The file has two parts:

1. Required Text - All text under the headings *Community Participation Guidelines and How to Report*, are required, and should not be altered.
2. Optional Text - The Project Specific Etiquette heading provides a space to speak more specifically about ways people can work effectively and inclusively together. Some examples of those can be found on the [Firefox Debugger](https://github.com/devtools-html/debugger.html/blob/master/CODE_OF_CONDUCT.md) project, and [Common Voice](https://github.com/mozilla/voice-web/blob/master/CODE_OF_CONDUCT.md). (The optional part is commented out in the [raw template file](https://raw.githubusercontent.com/mozilla/repo-templates/blob/master/templates/CODE_OF_CONDUCT.md), and will not be visible until you modify and uncomment that part.)

If you have any questions about this file, or Code of Conduct policies and procedures, please see [Mozilla-GitHub-Standards](https://wiki.mozilla.org/GitHub/Repository_Requirements) or email Mozilla-GitHub-Standards+CoC@mozilla.com.

_(Message COC002)_",Mozilla-GitHub-Standards,48073334,2019-03-29T17:10:03Z,CONTRIBUTOR,True,8,0,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,727f38867d4665b4c578b42a3d5f832fa59efb33,Remove comment.
3,https://api.github.com/repos/mozilla/jupyter-spark/pulls/41,41,Add JupyterLab support,"Fix #39.

This adds a Spark status window to the side pane in Jupyter Lab.  I played with making a modal dialog like the old extension, but it feels like the side pane is more in keeping with the ""JupyterLab way"".

It looks like:

![image](https://user-images.githubusercontent.com/38294/39948413-dfc91d84-5543-11e8-9f73-f6e2b5d6813c.png)

@teonbrooks, @jezdez",mdboom,38294,2018-05-11T21:51:36Z,CONTRIBUTOR,False,296,147,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,ee464cdba04913312441e887f04fc1d0d0ece88e,Add JupyterLab support
4,https://api.github.com/repos/mozilla/jupyter-spark/pulls/41,41,Add JupyterLab support,"Fix #39.

This adds a Spark status window to the side pane in Jupyter Lab.  I played with making a modal dialog like the old extension, but it feels like the side pane is more in keeping with the ""JupyterLab way"".

It looks like:

![image](https://user-images.githubusercontent.com/38294/39948413-dfc91d84-5543-11e8-9f73-f6e2b5d6813c.png)

@teonbrooks, @jezdez",mdboom,38294,2018-05-11T21:51:36Z,CONTRIBUTOR,False,296,147,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,82271971e223335ecbd91b063fd6b3635bcb9ea5,Update docs
5,https://api.github.com/repos/mozilla/jupyter-spark/pulls/41,41,Add JupyterLab support,"Fix #39.

This adds a Spark status window to the side pane in Jupyter Lab.  I played with making a modal dialog like the old extension, but it feels like the side pane is more in keeping with the ""JupyterLab way"".

It looks like:

![image](https://user-images.githubusercontent.com/38294/39948413-dfc91d84-5543-11e8-9f73-f6e2b5d6813c.png)

@teonbrooks, @jezdez",mdboom,38294,2018-05-11T21:51:36Z,CONTRIBUTOR,False,296,147,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,bbefd0d808c608ed1278f2f0455f8efbb68ed833,Remove unneeded imports
6,https://api.github.com/repos/mozilla/jupyter-spark/pulls/41,41,Add JupyterLab support,"Fix #39.

This adds a Spark status window to the side pane in Jupyter Lab.  I played with making a modal dialog like the old extension, but it feels like the side pane is more in keeping with the ""JupyterLab way"".

It looks like:

![image](https://user-images.githubusercontent.com/38294/39948413-dfc91d84-5543-11e8-9f73-f6e2b5d6813c.png)

@teonbrooks, @jezdez",mdboom,38294,2018-05-11T21:51:36Z,CONTRIBUTOR,False,296,147,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,95e78cfc01e313c065b14d3461d7bf10cd388dcb,Fix typo
7,https://api.github.com/repos/mozilla/jupyter-spark/pulls/41,41,Add JupyterLab support,"Fix #39.

This adds a Spark status window to the side pane in Jupyter Lab.  I played with making a modal dialog like the old extension, but it feels like the side pane is more in keeping with the ""JupyterLab way"".

It looks like:

![image](https://user-images.githubusercontent.com/38294/39948413-dfc91d84-5543-11e8-9f73-f6e2b5d6813c.png)

@teonbrooks, @jezdez",mdboom,38294,2018-05-11T21:51:36Z,CONTRIBUTOR,False,296,147,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,430f6195a518a8897656dd80a25fcc4005f69738,Use baseUrl in server URL
8,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,4d8d866bd82d083f5f31b2e58ae5f021f574a685,Spark API Url forwarded from SparkSession
9,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,4f2e7b2ff7e31cae63973af19fca44a1b99fa9a2,adapted tests to new Spark API Url handling
10,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,fc602828adb59ec32f49c54b92454d891e340f70,urllib quote for py2 and py3
11,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,93d46c46109457ad1ee2c1efcc1d2c2ad4b054d7,added load_ipython_extension and moved magic into module
12,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,1712ac9b2610f86858ec88ab2a9689b20c330355,remove yarn-client mode and use %load_ext
13,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,3af70acfeac096c8828325931904fd2af224aaf0,use proxy_url instead of proxy_root
14,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,3d37729d3d64c2936b5076bb37902ca3c907aea1,fixed typo
15,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,664231a1a5e938f7ef97345b34fca8f3a9c083a7,Adapt to chenged implementation
16,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,c7f72f0e3734dde555aac6f11efab15e01d98444,removed unused import
17,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,6138ad34f964128107750e874a98e41adc1a2357,fixed test
18,https://api.github.com/repos/mozilla/jupyter-spark/pulls/40,40,Handle multiple Spark Sessions,"Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()[""spark""].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.",bernhard-42,954408,2018-05-01T20:03:31Z,NONE,False,180,55,9,Jupyter Notebook extension for Apache Spark integration,JavaScript,38ed34acedcae6558c6cd0112d73ec84f50dfeef,added autostart during %load_ext
19,https://api.github.com/repos/mozilla/jupyter-spark/pulls/38,38,Setup automatic PyPI releases via Git tags.,,jezdez,1610,2018-02-26T17:00:40Z,MEMBER,True,37,11,3,Jupyter Notebook extension for Apache Spark integration,JavaScript,47af89a0b811450316ba2414cfa99e719cfc3b04,Setup automatic PyPI releases via Git tags.
20,https://api.github.com/repos/mozilla/jupyter-spark/pulls/38,38,Setup automatic PyPI releases via Git tags.,,jezdez,1610,2018-02-26T17:00:40Z,MEMBER,True,37,11,3,Jupyter Notebook extension for Apache Spark integration,JavaScript,7c73351d3a6ea5eea94aa6400921b8458322dbd3,Move PyPI deployment into own build stage.
21,https://api.github.com/repos/mozilla/jupyter-spark/pulls/37,37,Lint the Javascript,Fixes #35.,mdboom,38294,2018-02-15T16:20:48Z,CONTRIBUTOR,True,1212,36,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,e205bdc2cc27d5d68ec4f72d10da663403792634,Enable linting of Javascript
22,https://api.github.com/repos/mozilla/jupyter-spark/pulls/37,37,Lint the Javascript,Fixes #35.,mdboom,38294,2018-02-15T16:20:48Z,CONTRIBUTOR,True,1212,36,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,ecfaa26eb20d3989b13cdfad9dad560df36e83a8,LINT
23,https://api.github.com/repos/mozilla/jupyter-spark/pulls/34,34, Bug 1286859 - Add a 'cancel' link for spark jobs,"Supercedes #26.

In addition to #26, this makes the cancel link look like a button.

![image](https://user-images.githubusercontent.com/38294/36120754-34deb8d2-1012-11e8-9644-f54d8f78b54d.png)
",mdboom,38294,2018-02-12T21:31:50Z,CONTRIBUTOR,True,26,17,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,953c3e14af8cb720db8e7b128245a1b86fa0eec8,Add a 'cancel' link for spark jobs
24,https://api.github.com/repos/mozilla/jupyter-spark/pulls/34,34, Bug 1286859 - Add a 'cancel' link for spark jobs,"Supercedes #26.

In addition to #26, this makes the cancel link look like a button.

![image](https://user-images.githubusercontent.com/38294/36120754-34deb8d2-1012-11e8-9644-f54d8f78b54d.png)
",mdboom,38294,2018-02-12T21:31:50Z,CONTRIBUTOR,True,26,17,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,6c3d5913b6081645c174f5564d015f89b369986b,"CSS improvements to make ""cancel"" look like a button."
25,https://api.github.com/repos/mozilla/jupyter-spark/pulls/33,33,Fix #29: Include base URL in the handler path,"The handler isn't set up to be at the full path, which should include `base_url`, if set (which it isn't by default).  See #29.",mdboom,38294,2018-02-12T19:44:26Z,CONTRIBUTOR,True,1,1,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,8861b0fd9e7e1e98fc51f4fb58ee823ff65a0ebd,Fix #29: Include base URL in the handler path
26,https://api.github.com/repos/mozilla/jupyter-spark/pulls/32,32,Add testing for Python 3.6,,mdboom,38294,2018-02-12T18:09:23Z,CONTRIBUTOR,True,1,0,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,441f435f839105bfde9a8f2af34578d6c0e7850e,Add testing for Python 3.6
27,https://api.github.com/repos/mozilla/jupyter-spark/pulls/31,31,Add an example,"Fix #30.

This is just a simple example so it's easier for users to fire something up and test that their installation is working.",mdboom,38294,2018-02-12T16:48:33Z,CONTRIBUTOR,True,180,0,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,c30bff122005ff0efcf1f0915a83b79d48a511bb,Add an example
28,https://api.github.com/repos/mozilla/jupyter-spark/pulls/31,31,Add an example,"Fix #30.

This is just a simple example so it's easier for users to fire something up and test that their installation is working.",mdboom,38294,2018-02-12T16:48:33Z,CONTRIBUTOR,True,180,0,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,bef9d9e68f38be40f3ec8f5dac728cf34de0e7ac,Add mention of example in README
29,https://api.github.com/repos/mozilla/jupyter-spark/pulls/28,28,Fixed #24 - Progress bar colors where not shown,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/28)
<!-- Reviewable:end -->
",andershammar1,25075644,2017-10-06T21:45:38Z,CONTRIBUTOR,True,5,5,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,05e16c90b1b5c9c5d9dd9af493f458901b0ed7c0,Fixed #23 - Progress bar colors where not shown
30,https://api.github.com/repos/mozilla/jupyter-spark/pulls/27,27,Use tox-travis.,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/27)
<!-- Reviewable:end -->
",jezdez,1610,2017-09-20T18:13:21Z,MEMBER,True,10,19,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,7b0cc7db352310bd30ce1d32c6bb081b025e1b19,Use tox-travis.
31,https://api.github.com/repos/mozilla/jupyter-spark/pulls/27,27,Use tox-travis.,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/27)
<!-- Reviewable:end -->
",jezdez,1610,2017-09-20T18:13:21Z,MEMBER,True,10,19,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,33ed512b93f40ba42c2fa82532943d14d672ca6a,Remove hardcoded pytest-flake8 version.
32,https://api.github.com/repos/mozilla/jupyter-spark/pulls/27,27,Use tox-travis.,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/27)
<!-- Reviewable:end -->
",jezdez,1610,2017-09-20T18:13:21Z,MEMBER,True,10,19,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,5da76fb4c8b16167bd643a18c49b896bf966be4e,Appease isort.
33,https://api.github.com/repos/mozilla/jupyter-spark/pulls/27,27,Use tox-travis.,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/27)
<!-- Reviewable:end -->
",jezdez,1610,2017-09-20T18:13:21Z,MEMBER,True,10,19,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,dbad7d1f9c2d5d0430156edcf08ec085c9291892,Fix Codecov.
34,https://api.github.com/repos/mozilla/jupyter-spark/pulls/27,27,Use tox-travis.,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/27)
<!-- Reviewable:end -->
",jezdez,1610,2017-09-20T18:13:21Z,MEMBER,True,10,19,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,84cc4f312860acec663601fde1fe80563f422657,Write coverage report to xml file.
35,https://api.github.com/repos/mozilla/jupyter-spark/pulls/27,27,Use tox-travis.,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/27)
<!-- Reviewable:end -->
",jezdez,1610,2017-09-20T18:13:21Z,MEMBER,True,10,19,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,a6f969a84d2e65d29b6e1206f5d4a30b583cfa89,Stop using the src directory for the report.
36,https://api.github.com/repos/mozilla/jupyter-spark/pulls/26,26,Bug 1286859 - Add a 'cancel' link for spark jobs,"See also https://github.com/mozilla/emr-bootstrap-spark/pull/110, necessary for this to work

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/26)
<!-- Reviewable:end -->
",washort,947487,2017-08-15T15:41:36Z,NONE,False,7,2,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,5775dc8e0ba522220e3202a7ac58ab22e6e800bf,Add a 'cancel' link for spark jobs
37,https://api.github.com/repos/mozilla/jupyter-spark/pulls/23,23,Fixed proxy_url in backend,"`base_url` was ignored in handler. 
This uses the joined root and base url for the web handler.

Can you please add a test for this ;)

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/23)

<!-- Reviewable:end -->
",WaeCo,12046123,2016-07-29T18:22:48Z,CONTRIBUTOR,False,1,1,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,7df510d81c40c5f3440cb734eaadd5e4623407c4,"Fixed proxy_url in backend

`base_url` was ignored in handler. 
This uses the joined root and base url for the web handler."
38,https://api.github.com/repos/mozilla/jupyter-spark/pulls/21,21,Fix typo in the README file,,maurodoglio,508377,2016-06-30T12:24:11Z,CONTRIBUTOR,True,1,1,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,098976fa339f8ad3850f5bbd2d1d883ac2e4fb27,Fix typo in the README file
39,https://api.github.com/repos/mozilla/jupyter-spark/pulls/20,20,Fixed support for base_url in backend,"`base_url` was used in routing but not in `SparkHandler`.
This caused `HTTPError: URI did not start with /spark`

Additionaly now only html files are parsed for links to avoid `>` being replaced by `&gt;` in css files
",WaeCo,12046123,2016-06-21T21:17:13Z,CONTRIBUTOR,True,4,4,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,0d298e50ef5317d32744f1dc5e123d47e2e46955,"Fixed support for base_url in backend

`base_url` was used in routing but not in `SparkHandler`.
This caused `HTTPError: URI did not start with /spark`

Additionaly now only html files are parsed for links to avoid `>` being replaced by `&gt;` in css files"
40,https://api.github.com/repos/mozilla/jupyter-spark/pulls/20,20,Fixed support for base_url in backend,"`base_url` was used in routing but not in `SparkHandler`.
This caused `HTTPError: URI did not start with /spark`

Additionaly now only html files are parsed for links to avoid `>` being replaced by `&gt;` in css files
",WaeCo,12046123,2016-06-21T21:17:13Z,CONTRIBUTOR,True,4,4,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,a01a31cf1ed698664974f1a8d0e1239e33c216a6,Merge branch 'master' of https://github.com/mozilla/jupyter-spark
41,https://api.github.com/repos/mozilla/jupyter-spark/pulls/19,19,Fixed packaging and installation issues.,"This requires to use the official jupyter CLI tool to enable the extension
after pip-installing it instead of using a custom solution.

It enables the option to install from whl files and fixes a threadsafety
issue with the Tornado handler.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/19)

<!-- Reviewable:end -->
",jezdez,1610,2016-06-21T14:01:41Z,MEMBER,True,245,159,12,Jupyter Notebook extension for Apache Spark integration,JavaScript,55c525937de55fb27e5daa644491d20d39070b4a,Moved notebooks into a separate directory.
42,https://api.github.com/repos/mozilla/jupyter-spark/pulls/19,19,Fixed packaging and installation issues.,"This requires to use the official jupyter CLI tool to enable the extension
after pip-installing it instead of using a custom solution.

It enables the option to install from whl files and fixes a threadsafety
issue with the Tornado handler.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/19)

<!-- Reviewable:end -->
",jezdez,1610,2016-06-21T14:01:41Z,MEMBER,True,245,159,12,Jupyter Notebook extension for Apache Spark integration,JavaScript,1c4b301731f107ccf97200f7323ed036ccb9012b,Renamed Python package to jupyter_spark to match Python practices.
43,https://api.github.com/repos/mozilla/jupyter-spark/pulls/19,19,Fixed packaging and installation issues.,"This requires to use the official jupyter CLI tool to enable the extension
after pip-installing it instead of using a custom solution.

It enables the option to install from whl files and fixes a threadsafety
issue with the Tornado handler.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/19)

<!-- Reviewable:end -->
",jezdez,1610,2016-06-21T14:01:41Z,MEMBER,True,245,159,12,Jupyter Notebook extension for Apache Spark integration,JavaScript,4d588aebed29f0a8ffce635006648552ec93f16d,"Simplify packaginag and installation.

This does a few things:
- simplified setup.py drastically and fixed shipping static files
- enabled building of wheel files
- gets rid of automatic configuration and enabling of the Jupyter extension in favor of official use of Jupyter CLI tool
- documents official use of Jupyter CLI tool to install/enable/disable/uninstall
- uses official Tornado API to pass config parameters to handlers to be threadsafe
- updated README"
44,https://api.github.com/repos/mozilla/jupyter-spark/pulls/19,19,Fixed packaging and installation issues.,"This requires to use the official jupyter CLI tool to enable the extension
after pip-installing it instead of using a custom solution.

It enables the option to install from whl files and fixes a threadsafety
issue with the Tornado handler.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mozilla/jupyter-spark/19)

<!-- Reviewable:end -->
",jezdez,1610,2016-06-21T14:01:41Z,MEMBER,True,245,159,12,Jupyter Notebook extension for Apache Spark integration,JavaScript,a7027cac5b09b624beb6d0de8bc1f273368900c2,Fixed using a deprecated Jupyter API to get the base URL.
45,https://api.github.com/repos/mozilla/jupyter-spark/pulls/18,18,Support baseUrl in frontend,"`BaseURL` was used in backend but not in frontend. 
This will add the `BaseURL` in front of the Spark API URL
",WaeCo,12046123,2016-06-17T23:19:44Z,CONTRIBUTOR,True,3,2,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,01a95607a27f82104ce8a3ab9936fb8cc6f7c373,"Support baseUrl in frontend

`BaseURL` was used in backend but not in frontend. 
This will add the `BaseURL` in front of the Spark API URL"
46,https://api.github.com/repos/mozilla/jupyter-spark/pulls/16,16,Add the job name to the progress counter,"Makes it a little easier to figure out what is currently executing
",mreid-moz,969479,2016-05-03T14:51:12Z,COLLABORATOR,True,4,1,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,7d31747c0e9c3ca34dacd13461198db10768fe68,Add the job name to the progress counter
47,https://api.github.com/repos/mozilla/jupyter-spark/pulls/15,15,Added screenshots to readme,"Anything else that should be added to the readme?
",musicaljelly,1386678,2016-04-07T00:10:53Z,CONTRIBUTOR,True,11,8,4,Jupyter Notebook extension for Apache Spark integration,JavaScript,c419fe52edcf7ef5daddcafffe2ca661d280b6a3,Add documentation on how to view all running Spark jobs.
48,https://api.github.com/repos/mozilla/jupyter-spark/pulls/15,15,Added screenshots to readme,"Anything else that should be added to the readme?
",musicaljelly,1386678,2016-04-07T00:10:53Z,CONTRIBUTOR,True,11,8,4,Jupyter Notebook extension for Apache Spark integration,JavaScript,7329ef135528c929a7d164815e869071c27bc12a,Getting this branch back up-to-date
49,https://api.github.com/repos/mozilla/jupyter-spark/pulls/15,15,Added screenshots to readme,"Anything else that should be added to the readme?
",musicaljelly,1386678,2016-04-07T00:10:53Z,CONTRIBUTOR,True,11,8,4,Jupyter Notebook extension for Apache Spark integration,JavaScript,95c9c0b698f248e74f2a51b88751c26f7ff978c3,Added screenshot links
50,https://api.github.com/repos/mozilla/jupyter-spark/pulls/15,15,Added screenshots to readme,"Anything else that should be added to the readme?
",musicaljelly,1386678,2016-04-07T00:10:53Z,CONTRIBUTOR,True,11,8,4,Jupyter Notebook extension for Apache Spark integration,JavaScript,190111810e0e8472befc5be00d764d2e51966c46,Added screenshots for the readme.md
51,https://api.github.com/repos/mozilla/jupyter-spark/pulls/15,15,Added screenshots to readme,"Anything else that should be added to the readme?
",musicaljelly,1386678,2016-04-07T00:10:53Z,CONTRIBUTOR,True,11,8,4,Jupyter Notebook extension for Apache Spark integration,JavaScript,50ae8934d7b0bc3e076b28a5d52c954bbf8df5d2,Merge branch 'documentation' of https://github.com/mreid-moz/jupyter-spark into documentation
52,https://api.github.com/repos/mozilla/jupyter-spark/pulls/15,15,Added screenshots to readme,"Anything else that should be added to the readme?
",musicaljelly,1386678,2016-04-07T00:10:53Z,CONTRIBUTOR,True,11,8,4,Jupyter Notebook extension for Apache Spark integration,JavaScript,66c3cc8fc06ea71b4431774a49e9ceaa2ccf4d15,Tweaking readme
53,https://api.github.com/repos/mozilla/jupyter-spark/pulls/15,15,Added screenshots to readme,"Anything else that should be added to the readme?
",musicaljelly,1386678,2016-04-07T00:10:53Z,CONTRIBUTOR,True,11,8,4,Jupyter Notebook extension for Apache Spark integration,JavaScript,f1acea955fbc382a496cfac4c6b5532fb020548c,Fixed spacing in readme
54,https://api.github.com/repos/mozilla/jupyter-spark/pulls/15,15,Added screenshots to readme,"Anything else that should be added to the readme?
",musicaljelly,1386678,2016-04-07T00:10:53Z,CONTRIBUTOR,True,11,8,4,Jupyter Notebook extension for Apache Spark integration,JavaScript,91b23e1e27a4cf3c539ac62fd771d70500bc8f6e,"Revert ""Getting this branch back up-to-date""

This reverts commit 7329ef135528c929a7d164815e869071c27bc12a."
55,https://api.github.com/repos/mozilla/jupyter-spark/pulls/15,15,Added screenshots to readme,"Anything else that should be added to the readme?
",musicaljelly,1386678,2016-04-07T00:10:53Z,CONTRIBUTOR,True,11,8,4,Jupyter Notebook extension for Apache Spark integration,JavaScript,e701e8fba348303902111a1f1986ac820a1be4a3,"Merge branch 'master' into documentation

Conflicts:
	README.md"
56,https://api.github.com/repos/mozilla/jupyter-spark/pulls/15,15,Added screenshots to readme,"Anything else that should be added to the readme?
",musicaljelly,1386678,2016-04-07T00:10:53Z,CONTRIBUTOR,True,11,8,4,Jupyter Notebook extension for Apache Spark integration,JavaScript,9ff7421156fbf28af550d1996c16f8e00a327d81,Added blurb about localhost:8888/spark
57,https://api.github.com/repos/mozilla/jupyter-spark/pulls/14,14,Documentation,"Added screenshots to the readme
",musicaljelly,1386678,2016-04-07T00:02:34Z,CONTRIBUTOR,False,61,20,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,c419fe52edcf7ef5daddcafffe2ca661d280b6a3,Add documentation on how to view all running Spark jobs.
58,https://api.github.com/repos/mozilla/jupyter-spark/pulls/14,14,Documentation,"Added screenshots to the readme
",musicaljelly,1386678,2016-04-07T00:02:34Z,CONTRIBUTOR,False,61,20,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,7329ef135528c929a7d164815e869071c27bc12a,Getting this branch back up-to-date
59,https://api.github.com/repos/mozilla/jupyter-spark/pulls/14,14,Documentation,"Added screenshots to the readme
",musicaljelly,1386678,2016-04-07T00:02:34Z,CONTRIBUTOR,False,61,20,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,95c9c0b698f248e74f2a51b88751c26f7ff978c3,Added screenshot links
60,https://api.github.com/repos/mozilla/jupyter-spark/pulls/14,14,Documentation,"Added screenshots to the readme
",musicaljelly,1386678,2016-04-07T00:02:34Z,CONTRIBUTOR,False,61,20,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,190111810e0e8472befc5be00d764d2e51966c46,Added screenshots for the readme.md
61,https://api.github.com/repos/mozilla/jupyter-spark/pulls/14,14,Documentation,"Added screenshots to the readme
",musicaljelly,1386678,2016-04-07T00:02:34Z,CONTRIBUTOR,False,61,20,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,50ae8934d7b0bc3e076b28a5d52c954bbf8df5d2,Merge branch 'documentation' of https://github.com/mreid-moz/jupyter-spark into documentation
62,https://api.github.com/repos/mozilla/jupyter-spark/pulls/14,14,Documentation,"Added screenshots to the readme
",musicaljelly,1386678,2016-04-07T00:02:34Z,CONTRIBUTOR,False,61,20,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,66c3cc8fc06ea71b4431774a49e9ceaa2ccf4d15,Tweaking readme
63,https://api.github.com/repos/mozilla/jupyter-spark/pulls/14,14,Documentation,"Added screenshots to the readme
",musicaljelly,1386678,2016-04-07T00:02:34Z,CONTRIBUTOR,False,61,20,6,Jupyter Notebook extension for Apache Spark integration,JavaScript,f1acea955fbc382a496cfac4c6b5532fb020548c,Fixed spacing in readme
64,https://api.github.com/repos/mozilla/jupyter-spark/pulls/13,13,pep8 fixes,,musicaljelly,1386678,2016-03-24T20:07:55Z,CONTRIBUTOR,True,15,18,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,40906ce3d35be0575e2e79eb4f22f94092a693cd,pep8 fixes
65,https://api.github.com/repos/mozilla/jupyter-spark/pulls/12,12,Add documentation on how to view all running Spark jobs.,"(finally.)
",yeah568,1504986,2016-03-22T00:36:32Z,COLLABORATOR,False,2,0,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,c419fe52edcf7ef5daddcafffe2ca661d280b6a3,Add documentation on how to view all running Spark jobs.
66,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,380471e3964c977c066132077e5e63fda077080c,ui is added to code cell when executed
67,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,207c0712a53aacb11b424a7dc28dc20a3111cbaa,add progress bar when executing cell
68,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,bae95c45b3932a4f7f034e7c63b6d207c3045205,"functions to update, remove progress bar"
69,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,e2a419a0ce9b289e0b6a9554af5000e40cc4ac2b,Merge auto update
70,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,583684e6878fb427c310088d0e9002bc9e0243ae,Merge master into progress_bar
71,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,f32b50ef1bcbcdcc0da2262da20879fa74d4aeb6,progress bar updates using info from cache
72,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,a88a84d2209a33d784125579e25543421fe1d55a,some refactoring of functions
73,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,51726dd967853ec3a683a7fba32d5cfa273bea8e,progress bar is removed after cell is executed
74,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,37a13bbc8c22aad83515ba72b747aa8a4043ff31,status class of progress bar now updates
75,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,a83cc196043b46a621adae39ea734833c7b1e3ad,progress bar starts off as 'loading'; is removed if nothing returned
76,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,9b37c40c9de57cc2d888118225194eeb6cf3e458,fixed bug with progress bar text
77,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,f9f9f4fff84b9ad065ee5127fdc6f58cf91e2e67,better implementation of remove progress bars
78,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,21f8575fe6a78bc7b0bdf81a5c6a712893576f93,jobs_so_far now based on length of jobs array
79,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,31d91cc5ff5aa8d0c7ee618312dc66bd0767c47d,cells to execute now stored in queue; progress bars removed when kernel is idle
80,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,7d3781a0214571d13395a5ef930a80c732376298,counter added to keep track of jobs completed per cell
81,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,4c33625dfedebd5767ba6bb47dc6b6e8511bd4f7,progress bars hide until new job detected
82,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,524859d412d1b4df190f03761d7202b169964598,cache updates more frequently when cell is executing
83,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,d1c4991ff82c89a793eec52c7de8867535fc97db,progress bars actually hide until new job detected
84,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,6394ccaf55e1618385cef225974d3227c3b36517,simplified process to remove progress bar
85,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,a88b7e07effe9b205838a96f261baa2cb3af642a,removed unnecessary functions
86,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,942a09500b5ea211ba8b4f4eb7617554113643b5,set spark_is_running=false if application has error
87,https://api.github.com/repos/mozilla/jupyter-spark/pulls/11,11,Progress bar,"There's still some iffy behaviour happening, but I thought it'd be good for people to look this over and give any feedback they may have.

How it works so far: 
- a progress bar gets added to all code cells when running
- a progress bar has class ""progress-bar-warning"" and text ""Loading Spark..."" when it is initialized
- cell_jobs keeps track of code cells currently running, as well as their corresponding Spark jobs
- as the cache gets automatically updated (currently once per second), a trigger event goes off to update progress bars for each cell in cell_jobs
- code cells not running Spark have progress bars that remain in class ""progress-bar-warning""
- when any cell is changed (eg. write something in input; something loads for output), all progress bars with class ""progress-bar-warning"" or ""progress-bar-success"" get removed

What is iffy:
- the first Spark cell that is executed can take a while to load, as the SparkContext gets up and running; since no jobs are being run at this point, I'm not sure how to monitor its progress (or if it needs monitoring at all)
- I upped the UPDATE_FREQUENCY of the cache so that progress bars can update more regularly... but this makes things very inefficient :/ 
- non-Spark cells ideally shouldn't have a progress bar... (idea just now - initially hide progress bar?)
- each SparkContext runs on a different port, so I'm not sure if you can have multiple Spark notebooks open at the same time (as we are only monitoring the 1st port)
",shaybeau731,7600728,2016-03-17T08:34:43Z,CONTRIBUTOR,True,270,19,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,50b9b26c1da233037e944e723a1d04fd581ee16b,style fixes
88,https://api.github.com/repos/mozilla/jupyter-spark/pulls/10,10,Proxy sparkui links,"This un-breaks the links on the Spark UI proxy (visit localhost:8888/spark/) and makes images and css/js work as expected.

I didn't check how it impacts the api calls used by the ""list running jobs"" - we might need to add an exception for `/spark/api`.

I'm open to less hackish suggestions too :)
Before:
<img width=""400"" alt=""unmodified content"" src=""https://cloud.githubusercontent.com/assets/969479/13812931/7c4db29e-eb5d-11e5-9cb7-d952d1d3bd91.png"">

After (clicking on the ""environment"" link):
<img width=""400"" alt=""hackishly modified content"" src=""https://cloud.githubusercontent.com/assets/969479/13812935/82b791fe-eb5d-11e5-9ba8-2d1685213cca.png"">

@yeah568 and @musicaljelly please review.
",mreid-moz,969479,2016-03-16T12:59:10Z,COLLABORATOR,True,32,5,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,89192a48d783c61775414f59e5a6f5e5f6d31e1f,"Insert the ""spark"" prefix into SparkUI responses.

Hackishly insert the spark link prefix (""/spark/"") into docs
returned from the Spark UI. This makes the resulting page work
as expected, including links, images, and css/js.

It may be too fragile, so if there is a better way to do this, I
am open to changing the approach."
89,https://api.github.com/repos/mozilla/jupyter-spark/pulls/10,10,Proxy sparkui links,"This un-breaks the links on the Spark UI proxy (visit localhost:8888/spark/) and makes images and css/js work as expected.

I didn't check how it impacts the api calls used by the ""list running jobs"" - we might need to add an exception for `/spark/api`.

I'm open to less hackish suggestions too :)
Before:
<img width=""400"" alt=""unmodified content"" src=""https://cloud.githubusercontent.com/assets/969479/13812931/7c4db29e-eb5d-11e5-9cb7-d952d1d3bd91.png"">

After (clicking on the ""environment"" link):
<img width=""400"" alt=""hackishly modified content"" src=""https://cloud.githubusercontent.com/assets/969479/13812935/82b791fe-eb5d-11e5-9ba8-2d1685213cca.png"">

@yeah568 and @musicaljelly please review.
",mreid-moz,969479,2016-03-16T12:59:10Z,COLLABORATOR,True,32,5,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,5e5388980b6fd568db234945a77c7235a323a88c,Avoid KeyError if spark_host is missing in config.
90,https://api.github.com/repos/mozilla/jupyter-spark/pulls/10,10,Proxy sparkui links,"This un-breaks the links on the Spark UI proxy (visit localhost:8888/spark/) and makes images and css/js work as expected.

I didn't check how it impacts the api calls used by the ""list running jobs"" - we might need to add an exception for `/spark/api`.

I'm open to less hackish suggestions too :)
Before:
<img width=""400"" alt=""unmodified content"" src=""https://cloud.githubusercontent.com/assets/969479/13812931/7c4db29e-eb5d-11e5-9cb7-d952d1d3bd91.png"">

After (clicking on the ""environment"" link):
<img width=""400"" alt=""hackishly modified content"" src=""https://cloud.githubusercontent.com/assets/969479/13812935/82b791fe-eb5d-11e5-9ba8-2d1685213cca.png"">

@yeah568 and @musicaljelly please review.
",mreid-moz,969479,2016-03-16T12:59:10Z,COLLABORATOR,True,32,5,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,7ef7890286bff9c216ed3ecf110a705d91728f47,Politeness++
91,https://api.github.com/repos/mozilla/jupyter-spark/pulls/10,10,Proxy sparkui links,"This un-breaks the links on the Spark UI proxy (visit localhost:8888/spark/) and makes images and css/js work as expected.

I didn't check how it impacts the api calls used by the ""list running jobs"" - we might need to add an exception for `/spark/api`.

I'm open to less hackish suggestions too :)
Before:
<img width=""400"" alt=""unmodified content"" src=""https://cloud.githubusercontent.com/assets/969479/13812931/7c4db29e-eb5d-11e5-9cb7-d952d1d3bd91.png"">

After (clicking on the ""environment"" link):
<img width=""400"" alt=""hackishly modified content"" src=""https://cloud.githubusercontent.com/assets/969479/13812935/82b791fe-eb5d-11e5-9ba8-2d1685213cca.png"">

@yeah568 and @musicaljelly please review.
",mreid-moz,969479,2016-03-16T12:59:10Z,COLLABORATOR,True,32,5,2,Jupyter Notebook extension for Apache Spark integration,JavaScript,ec180e8438ff1cc91d51d2ec127a94b453d114f9,"Use bs4 instead of bs3, drop superfluous prints."
92,https://api.github.com/repos/mozilla/jupyter-spark/pulls/7,7,Added uninstall instructions,"Added uninstall instructions
",musicaljelly,1386678,2016-03-01T23:29:09Z,CONTRIBUTOR,True,7,0,1,Jupyter Notebook extension for Apache Spark integration,JavaScript,3be4e5874f9e2f0010c66215112f21f5bad31728,Added uninstall instructions
93,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,e68e13c5510502733640e19c84148b845a457bd0,Added extension skeleton for testing
94,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,52a87ac9f21909208abfaaccd3007e2d8facc1aa,Implemented basic passthrough of the URL to the Spark API
95,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,134ac20b33d3d67c74cc7e1ecdd89d5a755e3914,"Do an XMLHttpRequest instead of opening a new window.
Added example notebook that spins up spark and runs a task on it. Works on my machine, but might need tweaks to some paths to work on others'. Only meant as an illustrative example, won't be in the final project."
96,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,0423a4405b5be5381f0fb7e4cd56a3bdd945afac,"Create modal that opens on keyboard shortcut ""Alt-S""."
97,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,1130deba86443530a27fb8e9acf9ab6a936d8736,Merge remote-tracking branch 'origin/spark-proxy-proof-of-concept' into keyboard_shortcut
98,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,ea557085326fb180ed432408b214188534906e08,"Sloppy, but working listing of running jobs"
99,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,cfd61ef8b3c7c66690e930abfa4e2505692a77c0,Add application name and ID to each table
100,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,22007c91c88dc63012e71337afc401d0784152b7,"Added missing statuses and fix linter errors

Status names are based on https://github.com/apache/spark/blob/d83c2f9f0b08d6d5d369d9fae04cdb15448e7f0d/core/src/main/java/org/apache/spark/JobExecutionStatus.java#L22"
101,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,c6795caa2acab40a21741ca24fffb801b5df7143,"Added auto-update code.

Also broke out some of the dialog generation code into more modular pieces."
102,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,2a43db9a841aa14244b557b8b67febfd076e92ea,"Added error handling to server extension
Setup.py now works with the new filename (spark.js)"
103,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,eeca6988c4ff797eb5c742d829fc95cd5a82efaf,"Merge Python extension changes
Fix JSHint errors
Remove test.js"
104,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,bc2d870dc47dbbffe118429d430ab971683f4e0c,Update API and update frequency constant naming
105,https://api.github.com/repos/mozilla/jupyter-spark/pulls/5,5,"Proxy server, dialog box, auto-update for progress, cache","Resolves #1, resolves #2
",yeah568,1504986,2016-02-24T19:40:58Z,COLLABORATOR,True,576,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,0d987341a7980f3b669877fdbe03d2c64695065d,Check to ensure all data for applications has been retrieved before firing callbacks
106,https://api.github.com/repos/mozilla/jupyter-spark/pulls/4,4,Spark proxy proof of concept,"I threw together a basic proof of concept for sending requests to the Spark UI API. The request currently being made is '/api/v1/applications' (ie get a list of all applications), and is hard coded in test.js, but can be changed to whatever is desired by changing just that line on the client.

There's still lots of obvious improvements needed (error handling, not showing the results in an alert box, etc), but it's a start, and should be useful for those developing other features that need requests to the Spark UI API.

I added a notebook I created to test out starting up spark from within a notebook. It's included purely as an example, which I created by stealing parts from a couple guides online. It works on my machine, but might need modification to work on others'.

I'm posting this pull request mostly to share progress and solicit feedback. It's up to you if you actually want to merge it, since it's not really an implemented feature yet  :D
",musicaljelly,1386678,2016-02-09T07:03:45Z,CONTRIBUTOR,False,465,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,e68e13c5510502733640e19c84148b845a457bd0,Added extension skeleton for testing
107,https://api.github.com/repos/mozilla/jupyter-spark/pulls/4,4,Spark proxy proof of concept,"I threw together a basic proof of concept for sending requests to the Spark UI API. The request currently being made is '/api/v1/applications' (ie get a list of all applications), and is hard coded in test.js, but can be changed to whatever is desired by changing just that line on the client.

There's still lots of obvious improvements needed (error handling, not showing the results in an alert box, etc), but it's a start, and should be useful for those developing other features that need requests to the Spark UI API.

I added a notebook I created to test out starting up spark from within a notebook. It's included purely as an example, which I created by stealing parts from a couple guides online. It works on my machine, but might need modification to work on others'.

I'm posting this pull request mostly to share progress and solicit feedback. It's up to you if you actually want to merge it, since it's not really an implemented feature yet  :D
",musicaljelly,1386678,2016-02-09T07:03:45Z,CONTRIBUTOR,False,465,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,52a87ac9f21909208abfaaccd3007e2d8facc1aa,Implemented basic passthrough of the URL to the Spark API
108,https://api.github.com/repos/mozilla/jupyter-spark/pulls/4,4,Spark proxy proof of concept,"I threw together a basic proof of concept for sending requests to the Spark UI API. The request currently being made is '/api/v1/applications' (ie get a list of all applications), and is hard coded in test.js, but can be changed to whatever is desired by changing just that line on the client.

There's still lots of obvious improvements needed (error handling, not showing the results in an alert box, etc), but it's a start, and should be useful for those developing other features that need requests to the Spark UI API.

I added a notebook I created to test out starting up spark from within a notebook. It's included purely as an example, which I created by stealing parts from a couple guides online. It works on my machine, but might need modification to work on others'.

I'm posting this pull request mostly to share progress and solicit feedback. It's up to you if you actually want to merge it, since it's not really an implemented feature yet  :D
",musicaljelly,1386678,2016-02-09T07:03:45Z,CONTRIBUTOR,False,465,0,5,Jupyter Notebook extension for Apache Spark integration,JavaScript,134ac20b33d3d67c74cc7e1ecdd89d5a755e3914,"Do an XMLHttpRequest instead of opening a new window.
Added example notebook that spins up spark and runs a task on it. Works on my machine, but might need tweaks to some paths to work on others'. Only meant as an illustrative example, won't be in the final project."
